{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from numpy import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load('training_arrays/merged_training_arrays/all_organs_training_array.npy')\n",
    "\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "# Find rows containing NaN values in X\n",
    "nan_rows_X = np.isnan(X).any(axis=1)\n",
    "\n",
    "# Drop rows with NaN values from X and y\n",
    "X = X[~nan_rows_X]\n",
    "y = y[~nan_rows_X]\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000] - Train Loss: 64.2554 - Val Loss: 23.6362\n",
      "Epoch [2/2000] - Train Loss: 23.7552 - Val Loss: 20.1528\n",
      "Epoch [3/2000] - Train Loss: 20.5354 - Val Loss: 17.6172\n",
      "Epoch [4/2000] - Train Loss: 18.3943 - Val Loss: 15.6992\n",
      "Epoch [5/2000] - Train Loss: 16.6086 - Val Loss: 14.0347\n",
      "Epoch [6/2000] - Train Loss: 15.3157 - Val Loss: 13.6682\n",
      "Epoch [7/2000] - Train Loss: 14.5249 - Val Loss: 12.4480\n",
      "Epoch [8/2000] - Train Loss: 14.0739 - Val Loss: 12.1462\n",
      "Epoch [9/2000] - Train Loss: 13.8283 - Val Loss: 11.8282\n",
      "Epoch [10/2000] - Train Loss: 13.2564 - Val Loss: 12.0092\n",
      "Epoch [11/2000] - Train Loss: 13.1464 - Val Loss: 11.3625\n",
      "Epoch [12/2000] - Train Loss: 13.0871 - Val Loss: 11.2317\n",
      "Epoch [13/2000] - Train Loss: 12.8458 - Val Loss: 11.2314\n",
      "Epoch [14/2000] - Train Loss: 12.6354 - Val Loss: 11.1814\n",
      "Epoch [15/2000] - Train Loss: 12.5125 - Val Loss: 10.8324\n",
      "Epoch [16/2000] - Train Loss: 12.3181 - Val Loss: 10.9539\n",
      "Epoch [17/2000] - Train Loss: 12.1518 - Val Loss: 10.7639\n",
      "Epoch [18/2000] - Train Loss: 12.1317 - Val Loss: 10.7006\n",
      "Epoch [19/2000] - Train Loss: 12.1075 - Val Loss: 10.8326\n",
      "Epoch [20/2000] - Train Loss: 12.1821 - Val Loss: 10.9779\n",
      "Epoch [21/2000] - Train Loss: 11.9407 - Val Loss: 10.3621\n",
      "Epoch [22/2000] - Train Loss: 11.8107 - Val Loss: 10.4457\n",
      "Epoch [23/2000] - Train Loss: 11.8567 - Val Loss: 10.1971\n",
      "Epoch [24/2000] - Train Loss: 11.7566 - Val Loss: 10.3374\n",
      "Epoch [25/2000] - Train Loss: 11.5454 - Val Loss: 10.3797\n",
      "Epoch [26/2000] - Train Loss: 11.6116 - Val Loss: 10.0674\n",
      "Epoch [27/2000] - Train Loss: 11.4360 - Val Loss: 10.3005\n",
      "Epoch [28/2000] - Train Loss: 11.6493 - Val Loss: 10.0505\n",
      "Epoch [29/2000] - Train Loss: 11.4596 - Val Loss: 10.4781\n",
      "Epoch [30/2000] - Train Loss: 11.3319 - Val Loss: 9.9886\n",
      "Epoch [31/2000] - Train Loss: 11.2481 - Val Loss: 10.0897\n",
      "Epoch [32/2000] - Train Loss: 11.3356 - Val Loss: 9.9367\n",
      "Epoch [33/2000] - Train Loss: 11.2858 - Val Loss: 9.9509\n",
      "Epoch [34/2000] - Train Loss: 11.0899 - Val Loss: 9.8381\n",
      "Epoch [35/2000] - Train Loss: 11.1327 - Val Loss: 9.7471\n",
      "Epoch [36/2000] - Train Loss: 11.2034 - Val Loss: 10.0322\n",
      "Epoch [37/2000] - Train Loss: 10.9890 - Val Loss: 9.6684\n",
      "Epoch [38/2000] - Train Loss: 10.9356 - Val Loss: 9.6932\n",
      "Epoch [39/2000] - Train Loss: 11.1356 - Val Loss: 9.6934\n",
      "Epoch [40/2000] - Train Loss: 10.9191 - Val Loss: 10.4162\n",
      "Epoch [41/2000] - Train Loss: 10.9425 - Val Loss: 9.6670\n",
      "Epoch [42/2000] - Train Loss: 10.8942 - Val Loss: 9.6309\n",
      "Epoch [43/2000] - Train Loss: 10.7705 - Val Loss: 9.5385\n",
      "Epoch [44/2000] - Train Loss: 10.8360 - Val Loss: 9.8068\n",
      "Epoch [45/2000] - Train Loss: 10.6171 - Val Loss: 9.5050\n",
      "Epoch [46/2000] - Train Loss: 10.6602 - Val Loss: 9.4987\n",
      "Epoch [47/2000] - Train Loss: 10.7623 - Val Loss: 9.5739\n",
      "Epoch [48/2000] - Train Loss: 10.6534 - Val Loss: 10.4153\n",
      "Epoch [49/2000] - Train Loss: 10.5922 - Val Loss: 9.8650\n",
      "Epoch [50/2000] - Train Loss: 10.6120 - Val Loss: 9.8156\n",
      "Epoch [51/2000] - Train Loss: 10.5029 - Val Loss: 9.6348\n",
      "Epoch [52/2000] - Train Loss: 10.4463 - Val Loss: 9.9345\n",
      "Epoch [53/2000] - Train Loss: 10.6075 - Val Loss: 9.3929\n",
      "Epoch [54/2000] - Train Loss: 10.3215 - Val Loss: 9.4629\n",
      "Epoch [55/2000] - Train Loss: 10.2915 - Val Loss: 9.3833\n",
      "Epoch [56/2000] - Train Loss: 10.3232 - Val Loss: 9.4502\n",
      "Epoch [57/2000] - Train Loss: 10.4757 - Val Loss: 9.2894\n",
      "Epoch [58/2000] - Train Loss: 10.3110 - Val Loss: 9.6399\n",
      "Epoch [59/2000] - Train Loss: 10.2843 - Val Loss: 9.4732\n",
      "Epoch [60/2000] - Train Loss: 10.3148 - Val Loss: 9.3414\n",
      "Epoch [61/2000] - Train Loss: 10.2315 - Val Loss: 9.4229\n",
      "Epoch [62/2000] - Train Loss: 10.3233 - Val Loss: 9.4674\n",
      "Epoch [63/2000] - Train Loss: 10.2499 - Val Loss: 9.0742\n",
      "Epoch [64/2000] - Train Loss: 10.2268 - Val Loss: 9.1061\n",
      "Epoch [65/2000] - Train Loss: 10.1339 - Val Loss: 9.1245\n",
      "Epoch [66/2000] - Train Loss: 10.0182 - Val Loss: 9.1562\n",
      "Epoch [67/2000] - Train Loss: 10.1265 - Val Loss: 9.0978\n",
      "Epoch [68/2000] - Train Loss: 10.1492 - Val Loss: 9.1681\n",
      "Epoch [69/2000] - Train Loss: 10.0916 - Val Loss: 9.2792\n",
      "Epoch [70/2000] - Train Loss: 10.0624 - Val Loss: 8.9892\n",
      "Epoch [71/2000] - Train Loss: 10.1097 - Val Loss: 9.0438\n",
      "Epoch [72/2000] - Train Loss: 9.9003 - Val Loss: 8.9977\n",
      "Epoch [73/2000] - Train Loss: 10.0026 - Val Loss: 9.0373\n",
      "Epoch [74/2000] - Train Loss: 9.9510 - Val Loss: 9.0105\n",
      "Epoch [75/2000] - Train Loss: 9.9297 - Val Loss: 8.9027\n",
      "Epoch [76/2000] - Train Loss: 9.9967 - Val Loss: 8.9262\n",
      "Epoch [77/2000] - Train Loss: 10.0905 - Val Loss: 9.3674\n",
      "Epoch [78/2000] - Train Loss: 9.8540 - Val Loss: 9.0418\n",
      "Epoch [79/2000] - Train Loss: 9.8728 - Val Loss: 8.9028\n",
      "Epoch [80/2000] - Train Loss: 9.7321 - Val Loss: 8.8538\n",
      "Epoch [81/2000] - Train Loss: 9.7832 - Val Loss: 8.7988\n",
      "Epoch [82/2000] - Train Loss: 9.7583 - Val Loss: 8.7614\n",
      "Epoch [83/2000] - Train Loss: 9.9026 - Val Loss: 8.9430\n",
      "Epoch [84/2000] - Train Loss: 9.6899 - Val Loss: 8.8686\n",
      "Epoch [85/2000] - Train Loss: 9.7195 - Val Loss: 8.8855\n",
      "Epoch [86/2000] - Train Loss: 9.8419 - Val Loss: 8.9143\n",
      "Epoch [87/2000] - Train Loss: 9.7476 - Val Loss: 9.1863\n",
      "Epoch [88/2000] - Train Loss: 9.6209 - Val Loss: 8.7147\n",
      "Epoch [89/2000] - Train Loss: 9.6491 - Val Loss: 9.0341\n",
      "Epoch [90/2000] - Train Loss: 9.6865 - Val Loss: 8.6440\n",
      "Epoch [91/2000] - Train Loss: 9.6333 - Val Loss: 8.7534\n",
      "Epoch [92/2000] - Train Loss: 9.4870 - Val Loss: 8.6637\n",
      "Epoch [93/2000] - Train Loss: 9.5748 - Val Loss: 8.6725\n",
      "Epoch [94/2000] - Train Loss: 9.6314 - Val Loss: 8.6457\n",
      "Epoch [95/2000] - Train Loss: 9.5599 - Val Loss: 8.6277\n",
      "Epoch [96/2000] - Train Loss: 9.5920 - Val Loss: 8.6382\n",
      "Epoch [97/2000] - Train Loss: 9.5165 - Val Loss: 8.5334\n",
      "Epoch [98/2000] - Train Loss: 9.5048 - Val Loss: 8.7879\n",
      "Epoch [99/2000] - Train Loss: 9.5189 - Val Loss: 8.5784\n",
      "Epoch [100/2000] - Train Loss: 9.5159 - Val Loss: 8.4635\n",
      "Epoch [101/2000] - Train Loss: 9.3875 - Val Loss: 8.4775\n",
      "Epoch [102/2000] - Train Loss: 9.4133 - Val Loss: 8.6085\n",
      "Epoch [103/2000] - Train Loss: 9.5031 - Val Loss: 8.4797\n",
      "Epoch [104/2000] - Train Loss: 9.3776 - Val Loss: 8.4398\n",
      "Epoch [105/2000] - Train Loss: 9.3545 - Val Loss: 8.3951\n",
      "Epoch [106/2000] - Train Loss: 9.4600 - Val Loss: 8.4476\n",
      "Epoch [107/2000] - Train Loss: 9.3253 - Val Loss: 8.4842\n",
      "Epoch [108/2000] - Train Loss: 9.2050 - Val Loss: 8.5923\n",
      "Epoch [109/2000] - Train Loss: 9.3034 - Val Loss: 8.4328\n",
      "Epoch [110/2000] - Train Loss: 9.3091 - Val Loss: 8.4062\n",
      "Epoch [111/2000] - Train Loss: 9.3554 - Val Loss: 8.6142\n",
      "Epoch [112/2000] - Train Loss: 9.2929 - Val Loss: 8.3664\n",
      "Epoch [113/2000] - Train Loss: 9.2563 - Val Loss: 8.8072\n",
      "Epoch [114/2000] - Train Loss: 9.4508 - Val Loss: 8.3267\n",
      "Epoch [115/2000] - Train Loss: 9.1409 - Val Loss: 8.2447\n",
      "Epoch [116/2000] - Train Loss: 9.2331 - Val Loss: 8.2643\n",
      "Epoch [117/2000] - Train Loss: 9.2255 - Val Loss: 8.2150\n",
      "Epoch [118/2000] - Train Loss: 9.1338 - Val Loss: 8.1993\n",
      "Epoch [119/2000] - Train Loss: 9.1751 - Val Loss: 8.3537\n",
      "Epoch [120/2000] - Train Loss: 9.1075 - Val Loss: 8.2115\n",
      "Epoch [121/2000] - Train Loss: 9.1742 - Val Loss: 8.3294\n",
      "Epoch [122/2000] - Train Loss: 9.1308 - Val Loss: 8.2038\n",
      "Epoch [123/2000] - Train Loss: 9.0191 - Val Loss: 8.1498\n",
      "Epoch [124/2000] - Train Loss: 9.1020 - Val Loss: 8.3409\n",
      "Epoch [125/2000] - Train Loss: 9.1727 - Val Loss: 8.2246\n",
      "Epoch [126/2000] - Train Loss: 9.0738 - Val Loss: 8.4697\n",
      "Epoch [127/2000] - Train Loss: 9.0302 - Val Loss: 8.3154\n",
      "Epoch [128/2000] - Train Loss: 9.1160 - Val Loss: 8.6219\n",
      "Epoch [129/2000] - Train Loss: 9.1393 - Val Loss: 8.0608\n",
      "Epoch [130/2000] - Train Loss: 8.9963 - Val Loss: 8.2040\n",
      "Epoch [131/2000] - Train Loss: 9.1162 - Val Loss: 8.1695\n",
      "Epoch [132/2000] - Train Loss: 9.0344 - Val Loss: 8.0394\n",
      "Epoch [133/2000] - Train Loss: 9.0468 - Val Loss: 8.2019\n",
      "Epoch [134/2000] - Train Loss: 8.9164 - Val Loss: 8.1228\n",
      "Epoch [135/2000] - Train Loss: 9.0257 - Val Loss: 7.9702\n",
      "Epoch [136/2000] - Train Loss: 8.9296 - Val Loss: 8.0627\n",
      "Epoch [137/2000] - Train Loss: 8.9801 - Val Loss: 7.9041\n",
      "Epoch [138/2000] - Train Loss: 8.8796 - Val Loss: 8.0209\n",
      "Epoch [139/2000] - Train Loss: 8.8851 - Val Loss: 7.9949\n",
      "Epoch [140/2000] - Train Loss: 8.7744 - Val Loss: 7.9641\n",
      "Epoch [141/2000] - Train Loss: 8.9059 - Val Loss: 7.9859\n",
      "Epoch [142/2000] - Train Loss: 8.8871 - Val Loss: 8.0277\n",
      "Epoch [143/2000] - Train Loss: 8.7630 - Val Loss: 7.9781\n",
      "Epoch [144/2000] - Train Loss: 8.7477 - Val Loss: 8.1051\n",
      "Epoch [145/2000] - Train Loss: 8.8204 - Val Loss: 7.8480\n",
      "Epoch [146/2000] - Train Loss: 8.8775 - Val Loss: 8.1073\n",
      "Epoch [147/2000] - Train Loss: 8.7022 - Val Loss: 7.8708\n",
      "Epoch [148/2000] - Train Loss: 8.7892 - Val Loss: 8.0131\n",
      "Epoch [149/2000] - Train Loss: 8.7966 - Val Loss: 7.8119\n",
      "Epoch [150/2000] - Train Loss: 8.6667 - Val Loss: 8.0426\n",
      "Epoch [151/2000] - Train Loss: 8.6396 - Val Loss: 8.0106\n",
      "Epoch [152/2000] - Train Loss: 8.6331 - Val Loss: 7.9146\n",
      "Epoch [153/2000] - Train Loss: 8.6783 - Val Loss: 8.1440\n",
      "Epoch [154/2000] - Train Loss: 8.7014 - Val Loss: 7.7581\n",
      "Epoch [155/2000] - Train Loss: 8.6832 - Val Loss: 7.9531\n",
      "Epoch [156/2000] - Train Loss: 8.5416 - Val Loss: 7.7690\n",
      "Epoch [157/2000] - Train Loss: 8.5652 - Val Loss: 7.9397\n",
      "Epoch [158/2000] - Train Loss: 8.6591 - Val Loss: 7.6887\n",
      "Epoch [159/2000] - Train Loss: 8.6655 - Val Loss: 7.7006\n",
      "Epoch [160/2000] - Train Loss: 8.6443 - Val Loss: 7.6867\n",
      "Epoch [161/2000] - Train Loss: 8.5986 - Val Loss: 7.8529\n",
      "Epoch [162/2000] - Train Loss: 8.5490 - Val Loss: 7.6921\n",
      "Epoch [163/2000] - Train Loss: 8.5077 - Val Loss: 7.6573\n",
      "Epoch [164/2000] - Train Loss: 8.5291 - Val Loss: 7.6447\n",
      "Epoch [165/2000] - Train Loss: 8.5612 - Val Loss: 7.5952\n",
      "Epoch [166/2000] - Train Loss: 8.4897 - Val Loss: 7.9056\n",
      "Epoch [167/2000] - Train Loss: 8.3808 - Val Loss: 7.5669\n",
      "Epoch [168/2000] - Train Loss: 8.5153 - Val Loss: 7.6416\n",
      "Epoch [169/2000] - Train Loss: 8.4936 - Val Loss: 7.5690\n",
      "Epoch [170/2000] - Train Loss: 8.3924 - Val Loss: 7.6222\n",
      "Epoch [171/2000] - Train Loss: 8.4733 - Val Loss: 7.6165\n",
      "Epoch [172/2000] - Train Loss: 8.5028 - Val Loss: 7.5077\n",
      "Epoch [173/2000] - Train Loss: 8.3732 - Val Loss: 7.5560\n",
      "Epoch [174/2000] - Train Loss: 8.3956 - Val Loss: 7.7792\n",
      "Epoch [175/2000] - Train Loss: 8.4773 - Val Loss: 7.6354\n",
      "Epoch [176/2000] - Train Loss: 8.5176 - Val Loss: 7.6433\n",
      "Epoch [177/2000] - Train Loss: 8.3245 - Val Loss: 7.4817\n",
      "Epoch [178/2000] - Train Loss: 8.3424 - Val Loss: 7.6434\n",
      "Epoch [179/2000] - Train Loss: 8.4254 - Val Loss: 7.4958\n",
      "Epoch [180/2000] - Train Loss: 8.3482 - Val Loss: 8.5318\n",
      "Epoch [181/2000] - Train Loss: 8.4812 - Val Loss: 7.5818\n",
      "Epoch [182/2000] - Train Loss: 8.3836 - Val Loss: 7.5487\n",
      "Epoch [183/2000] - Train Loss: 8.2322 - Val Loss: 7.4829\n",
      "Epoch [184/2000] - Train Loss: 8.2020 - Val Loss: 7.7446\n",
      "Epoch [185/2000] - Train Loss: 8.3066 - Val Loss: 7.7912\n",
      "Epoch [186/2000] - Train Loss: 8.2810 - Val Loss: 7.5551\n",
      "Epoch [187/2000] - Train Loss: 8.2726 - Val Loss: 7.8557\n",
      "Epoch [188/2000] - Train Loss: 8.2476 - Val Loss: 7.4989\n",
      "Epoch [189/2000] - Train Loss: 8.2147 - Val Loss: 7.2831\n",
      "Epoch [190/2000] - Train Loss: 8.2484 - Val Loss: 7.3999\n",
      "Epoch [191/2000] - Train Loss: 8.2197 - Val Loss: 7.3314\n",
      "Epoch [192/2000] - Train Loss: 8.1943 - Val Loss: 7.5776\n",
      "Epoch [193/2000] - Train Loss: 8.2530 - Val Loss: 7.4316\n",
      "Epoch [194/2000] - Train Loss: 8.2001 - Val Loss: 7.5027\n",
      "Epoch [195/2000] - Train Loss: 8.2188 - Val Loss: 7.5852\n",
      "Epoch [196/2000] - Train Loss: 8.1802 - Val Loss: 7.6633\n",
      "Epoch [197/2000] - Train Loss: 8.1841 - Val Loss: 7.2890\n",
      "Epoch [198/2000] - Train Loss: 8.2307 - Val Loss: 7.3406\n",
      "Epoch [199/2000] - Train Loss: 8.1876 - Val Loss: 7.3238\n",
      "Epoch [200/2000] - Train Loss: 8.1303 - Val Loss: 7.1770\n",
      "Epoch [201/2000] - Train Loss: 8.0598 - Val Loss: 7.3162\n",
      "Epoch [202/2000] - Train Loss: 8.2030 - Val Loss: 7.5446\n",
      "Epoch [203/2000] - Train Loss: 8.0976 - Val Loss: 8.0051\n",
      "Epoch [204/2000] - Train Loss: 8.2182 - Val Loss: 7.8478\n",
      "Epoch [205/2000] - Train Loss: 8.4029 - Val Loss: 7.6945\n",
      "Epoch [206/2000] - Train Loss: 8.1458 - Val Loss: 7.1467\n",
      "Epoch [207/2000] - Train Loss: 8.0757 - Val Loss: 7.4185\n",
      "Epoch [208/2000] - Train Loss: 8.0789 - Val Loss: 7.2618\n",
      "Epoch [209/2000] - Train Loss: 8.0528 - Val Loss: 7.1457\n",
      "Epoch [210/2000] - Train Loss: 7.9579 - Val Loss: 7.1784\n",
      "Epoch [211/2000] - Train Loss: 8.0277 - Val Loss: 7.3551\n",
      "Epoch [212/2000] - Train Loss: 8.0489 - Val Loss: 7.1310\n",
      "Epoch [213/2000] - Train Loss: 7.9445 - Val Loss: 7.2865\n",
      "Epoch [214/2000] - Train Loss: 7.9759 - Val Loss: 7.0394\n",
      "Epoch [215/2000] - Train Loss: 7.9003 - Val Loss: 7.4905\n",
      "Epoch [216/2000] - Train Loss: 8.1022 - Val Loss: 7.1330\n",
      "Epoch [217/2000] - Train Loss: 7.9618 - Val Loss: 7.1056\n",
      "Epoch [218/2000] - Train Loss: 7.9989 - Val Loss: 7.1407\n",
      "Epoch [219/2000] - Train Loss: 7.8590 - Val Loss: 7.0214\n",
      "Epoch [220/2000] - Train Loss: 7.9485 - Val Loss: 7.2922\n",
      "Epoch [221/2000] - Train Loss: 7.9200 - Val Loss: 7.0501\n",
      "Epoch [222/2000] - Train Loss: 8.0424 - Val Loss: 7.2031\n",
      "Epoch [223/2000] - Train Loss: 7.9199 - Val Loss: 7.5931\n",
      "Epoch [224/2000] - Train Loss: 7.9357 - Val Loss: 7.5494\n",
      "Epoch [225/2000] - Train Loss: 7.7911 - Val Loss: 6.9112\n",
      "Epoch [226/2000] - Train Loss: 7.8670 - Val Loss: 7.4389\n",
      "Epoch [227/2000] - Train Loss: 7.8065 - Val Loss: 7.9078\n",
      "Epoch [228/2000] - Train Loss: 7.8706 - Val Loss: 7.1107\n",
      "Epoch [229/2000] - Train Loss: 7.8318 - Val Loss: 7.1802\n",
      "Epoch [230/2000] - Train Loss: 7.8213 - Val Loss: 7.0148\n",
      "Epoch [231/2000] - Train Loss: 7.7357 - Val Loss: 7.1062\n",
      "Epoch [232/2000] - Train Loss: 7.7971 - Val Loss: 7.0776\n",
      "Epoch [233/2000] - Train Loss: 7.8448 - Val Loss: 7.2295\n",
      "Epoch [234/2000] - Train Loss: 7.8687 - Val Loss: 7.8329\n",
      "Epoch [235/2000] - Train Loss: 7.8156 - Val Loss: 6.9522\n",
      "Epoch [236/2000] - Train Loss: 7.7312 - Val Loss: 6.8969\n",
      "Epoch [237/2000] - Train Loss: 7.7659 - Val Loss: 6.9294\n",
      "Epoch [238/2000] - Train Loss: 7.7500 - Val Loss: 6.8986\n",
      "Epoch [239/2000] - Train Loss: 7.8675 - Val Loss: 7.0025\n",
      "Epoch [240/2000] - Train Loss: 7.8041 - Val Loss: 6.9126\n",
      "Epoch [241/2000] - Train Loss: 7.6975 - Val Loss: 6.8063\n",
      "Epoch [242/2000] - Train Loss: 7.8667 - Val Loss: 6.8866\n",
      "Epoch [243/2000] - Train Loss: 7.6170 - Val Loss: 7.0136\n",
      "Epoch [244/2000] - Train Loss: 7.7003 - Val Loss: 6.9426\n",
      "Epoch [245/2000] - Train Loss: 7.7074 - Val Loss: 7.4707\n",
      "Epoch [246/2000] - Train Loss: 7.7072 - Val Loss: 6.8807\n",
      "Epoch [247/2000] - Train Loss: 7.6599 - Val Loss: 6.8987\n",
      "Epoch [248/2000] - Train Loss: 7.6793 - Val Loss: 7.4493\n",
      "Epoch [249/2000] - Train Loss: 7.7916 - Val Loss: 6.9036\n",
      "Epoch [250/2000] - Train Loss: 7.6523 - Val Loss: 6.9726\n",
      "Epoch [251/2000] - Train Loss: 7.7320 - Val Loss: 6.8028\n",
      "Epoch [252/2000] - Train Loss: 7.6146 - Val Loss: 6.8892\n",
      "Epoch [253/2000] - Train Loss: 7.6796 - Val Loss: 7.1769\n",
      "Epoch [254/2000] - Train Loss: 7.6804 - Val Loss: 6.7732\n",
      "Epoch [255/2000] - Train Loss: 7.7178 - Val Loss: 6.8679\n",
      "Epoch [256/2000] - Train Loss: 7.5817 - Val Loss: 6.8786\n",
      "Epoch [257/2000] - Train Loss: 7.5278 - Val Loss: 6.7168\n",
      "Epoch [258/2000] - Train Loss: 7.5968 - Val Loss: 6.7524\n",
      "Epoch [259/2000] - Train Loss: 7.6321 - Val Loss: 6.8374\n",
      "Epoch [260/2000] - Train Loss: 7.5508 - Val Loss: 6.8287\n",
      "Epoch [261/2000] - Train Loss: 7.6479 - Val Loss: 6.6896\n",
      "Epoch [262/2000] - Train Loss: 7.5948 - Val Loss: 6.7088\n",
      "Epoch [263/2000] - Train Loss: 7.5905 - Val Loss: 6.7531\n",
      "Epoch [264/2000] - Train Loss: 7.5807 - Val Loss: 6.8685\n",
      "Epoch [265/2000] - Train Loss: 7.5066 - Val Loss: 6.7870\n",
      "Epoch [266/2000] - Train Loss: 7.5323 - Val Loss: 6.7353\n",
      "Epoch [267/2000] - Train Loss: 7.4395 - Val Loss: 7.1114\n",
      "Epoch [268/2000] - Train Loss: 7.5205 - Val Loss: 6.8242\n",
      "Epoch [269/2000] - Train Loss: 7.5695 - Val Loss: 6.8845\n",
      "Epoch [270/2000] - Train Loss: 7.6004 - Val Loss: 6.9056\n",
      "Epoch [271/2000] - Train Loss: 7.4970 - Val Loss: 6.6250\n",
      "Epoch [272/2000] - Train Loss: 7.4610 - Val Loss: 6.7783\n",
      "Epoch [273/2000] - Train Loss: 7.4759 - Val Loss: 6.6159\n",
      "Epoch [274/2000] - Train Loss: 7.4449 - Val Loss: 6.5513\n",
      "Epoch [275/2000] - Train Loss: 7.3434 - Val Loss: 6.8165\n",
      "Epoch [276/2000] - Train Loss: 7.4325 - Val Loss: 6.6670\n",
      "Epoch [277/2000] - Train Loss: 7.4716 - Val Loss: 6.5613\n",
      "Epoch [278/2000] - Train Loss: 7.4981 - Val Loss: 6.8659\n",
      "Epoch [279/2000] - Train Loss: 7.4101 - Val Loss: 6.7338\n",
      "Epoch [280/2000] - Train Loss: 7.4310 - Val Loss: 7.3750\n",
      "Epoch [281/2000] - Train Loss: 7.5297 - Val Loss: 6.6404\n",
      "Epoch [282/2000] - Train Loss: 7.4067 - Val Loss: 6.7257\n",
      "Epoch [283/2000] - Train Loss: 7.3323 - Val Loss: 7.1795\n",
      "Epoch [284/2000] - Train Loss: 7.4582 - Val Loss: 6.7522\n",
      "Epoch [285/2000] - Train Loss: 7.4072 - Val Loss: 6.6775\n",
      "Epoch [286/2000] - Train Loss: 7.2640 - Val Loss: 6.5097\n",
      "Epoch [287/2000] - Train Loss: 7.3460 - Val Loss: 6.9258\n",
      "Epoch [288/2000] - Train Loss: 7.4215 - Val Loss: 6.6241\n",
      "Epoch [289/2000] - Train Loss: 7.2981 - Val Loss: 7.4194\n",
      "Epoch [290/2000] - Train Loss: 7.3953 - Val Loss: 6.6237\n",
      "Epoch [291/2000] - Train Loss: 7.3538 - Val Loss: 6.6889\n",
      "Epoch [292/2000] - Train Loss: 7.2959 - Val Loss: 6.5720\n",
      "Epoch [293/2000] - Train Loss: 7.3204 - Val Loss: 7.6639\n",
      "Epoch [294/2000] - Train Loss: 7.3140 - Val Loss: 6.5064\n",
      "Epoch [295/2000] - Train Loss: 7.3239 - Val Loss: 6.4059\n",
      "Epoch [296/2000] - Train Loss: 7.3669 - Val Loss: 6.5004\n",
      "Epoch [297/2000] - Train Loss: 7.2640 - Val Loss: 6.6590\n",
      "Epoch [298/2000] - Train Loss: 7.3100 - Val Loss: 6.5431\n",
      "Epoch [299/2000] - Train Loss: 7.3062 - Val Loss: 6.5628\n",
      "Epoch [300/2000] - Train Loss: 7.2325 - Val Loss: 6.5590\n",
      "Epoch [301/2000] - Train Loss: 7.3039 - Val Loss: 6.8563\n",
      "Epoch [302/2000] - Train Loss: 7.2364 - Val Loss: 6.5087\n",
      "Epoch [303/2000] - Train Loss: 7.2685 - Val Loss: 6.6022\n",
      "Epoch [304/2000] - Train Loss: 7.3262 - Val Loss: 6.6327\n",
      "Epoch [305/2000] - Train Loss: 7.3207 - Val Loss: 6.5219\n",
      "Epoch [306/2000] - Train Loss: 7.1378 - Val Loss: 6.3893\n",
      "Epoch [307/2000] - Train Loss: 7.2599 - Val Loss: 6.4778\n",
      "Epoch [308/2000] - Train Loss: 7.2053 - Val Loss: 6.8142\n",
      "Epoch [309/2000] - Train Loss: 7.1568 - Val Loss: 6.4613\n",
      "Epoch [310/2000] - Train Loss: 7.2150 - Val Loss: 6.4057\n",
      "Epoch [311/2000] - Train Loss: 7.0689 - Val Loss: 6.4248\n",
      "Epoch [312/2000] - Train Loss: 7.0773 - Val Loss: 6.4612\n",
      "Epoch [313/2000] - Train Loss: 7.1995 - Val Loss: 6.4261\n",
      "Epoch [314/2000] - Train Loss: 7.1402 - Val Loss: 6.5505\n",
      "Epoch [315/2000] - Train Loss: 7.0750 - Val Loss: 6.3517\n",
      "Epoch [316/2000] - Train Loss: 7.1786 - Val Loss: 6.6150\n",
      "Epoch [317/2000] - Train Loss: 7.1393 - Val Loss: 6.3785\n",
      "Epoch [318/2000] - Train Loss: 7.1542 - Val Loss: 6.2971\n",
      "Epoch [319/2000] - Train Loss: 7.0572 - Val Loss: 6.3623\n",
      "Epoch [320/2000] - Train Loss: 7.0649 - Val Loss: 6.5482\n",
      "Epoch [321/2000] - Train Loss: 7.1317 - Val Loss: 6.5069\n",
      "Epoch [322/2000] - Train Loss: 7.1271 - Val Loss: 6.3049\n",
      "Epoch [323/2000] - Train Loss: 7.1567 - Val Loss: 6.4349\n",
      "Epoch [324/2000] - Train Loss: 7.1648 - Val Loss: 6.3839\n",
      "Epoch [325/2000] - Train Loss: 7.1087 - Val Loss: 6.3600\n",
      "Epoch [326/2000] - Train Loss: 7.1418 - Val Loss: 6.7704\n",
      "Epoch [327/2000] - Train Loss: 7.1228 - Val Loss: 6.6477\n",
      "Epoch [328/2000] - Train Loss: 7.1272 - Val Loss: 6.3282\n",
      "Epoch [329/2000] - Train Loss: 7.0684 - Val Loss: 6.3391\n",
      "Epoch [330/2000] - Train Loss: 7.0480 - Val Loss: 6.2909\n",
      "Epoch [331/2000] - Train Loss: 7.0497 - Val Loss: 6.5951\n",
      "Epoch [332/2000] - Train Loss: 7.1386 - Val Loss: 6.3918\n",
      "Epoch [333/2000] - Train Loss: 6.9083 - Val Loss: 6.6285\n",
      "Epoch [334/2000] - Train Loss: 7.0163 - Val Loss: 6.4498\n",
      "Epoch [335/2000] - Train Loss: 7.0635 - Val Loss: 6.2270\n",
      "Epoch [336/2000] - Train Loss: 7.0385 - Val Loss: 6.3679\n",
      "Epoch [337/2000] - Train Loss: 6.9733 - Val Loss: 6.3818\n",
      "Epoch [338/2000] - Train Loss: 7.0941 - Val Loss: 6.2885\n",
      "Epoch [339/2000] - Train Loss: 6.9718 - Val Loss: 6.7596\n",
      "Epoch [340/2000] - Train Loss: 7.1104 - Val Loss: 6.3732\n",
      "Epoch [341/2000] - Train Loss: 7.0201 - Val Loss: 6.2773\n",
      "Epoch [342/2000] - Train Loss: 6.9455 - Val Loss: 6.3469\n",
      "Epoch [343/2000] - Train Loss: 6.9319 - Val Loss: 6.1704\n",
      "Epoch [344/2000] - Train Loss: 7.0030 - Val Loss: 6.3523\n",
      "Epoch [345/2000] - Train Loss: 7.0409 - Val Loss: 6.1590\n",
      "Epoch [346/2000] - Train Loss: 7.0309 - Val Loss: 6.2634\n",
      "Epoch [347/2000] - Train Loss: 7.0532 - Val Loss: 6.2961\n",
      "Epoch [348/2000] - Train Loss: 7.0532 - Val Loss: 6.2597\n",
      "Epoch [349/2000] - Train Loss: 6.9168 - Val Loss: 6.4586\n",
      "Epoch [350/2000] - Train Loss: 7.0438 - Val Loss: 6.8192\n",
      "Epoch [351/2000] - Train Loss: 6.8915 - Val Loss: 6.2092\n",
      "Epoch [352/2000] - Train Loss: 6.9151 - Val Loss: 6.4965\n",
      "Epoch [353/2000] - Train Loss: 6.9097 - Val Loss: 6.4574\n",
      "Epoch [354/2000] - Train Loss: 6.9883 - Val Loss: 6.1633\n",
      "Epoch [355/2000] - Train Loss: 6.8974 - Val Loss: 6.1257\n",
      "Epoch [356/2000] - Train Loss: 6.8080 - Val Loss: 6.2459\n",
      "Epoch [357/2000] - Train Loss: 7.0191 - Val Loss: 6.2008\n",
      "Epoch [358/2000] - Train Loss: 6.8888 - Val Loss: 6.2384\n",
      "Epoch [359/2000] - Train Loss: 6.8295 - Val Loss: 6.5313\n",
      "Epoch [360/2000] - Train Loss: 6.8009 - Val Loss: 6.2813\n",
      "Epoch [361/2000] - Train Loss: 6.8492 - Val Loss: 6.3085\n",
      "Epoch [362/2000] - Train Loss: 6.8773 - Val Loss: 6.3480\n",
      "Epoch [363/2000] - Train Loss: 7.0214 - Val Loss: 6.1208\n",
      "Epoch [364/2000] - Train Loss: 6.8567 - Val Loss: 6.1037\n",
      "Epoch [365/2000] - Train Loss: 6.8963 - Val Loss: 6.2438\n",
      "Epoch [366/2000] - Train Loss: 6.7849 - Val Loss: 6.1097\n",
      "Epoch [367/2000] - Train Loss: 6.8145 - Val Loss: 6.4196\n",
      "Epoch [368/2000] - Train Loss: 6.8348 - Val Loss: 6.1780\n",
      "Epoch [369/2000] - Train Loss: 6.7744 - Val Loss: 6.3014\n",
      "Epoch [370/2000] - Train Loss: 6.8865 - Val Loss: 6.5242\n",
      "Epoch [371/2000] - Train Loss: 6.9462 - Val Loss: 6.6511\n",
      "Epoch [372/2000] - Train Loss: 6.8495 - Val Loss: 6.8903\n",
      "Epoch [373/2000] - Train Loss: 6.8135 - Val Loss: 6.0614\n",
      "Epoch [374/2000] - Train Loss: 6.7536 - Val Loss: 6.0433\n",
      "Epoch [375/2000] - Train Loss: 6.7801 - Val Loss: 6.1149\n",
      "Epoch [376/2000] - Train Loss: 6.7433 - Val Loss: 6.4436\n",
      "Epoch [377/2000] - Train Loss: 6.7606 - Val Loss: 6.0970\n",
      "Epoch [378/2000] - Train Loss: 6.7623 - Val Loss: 6.0649\n",
      "Epoch [379/2000] - Train Loss: 6.7965 - Val Loss: 6.9370\n",
      "Epoch [380/2000] - Train Loss: 6.8946 - Val Loss: 6.0137\n",
      "Epoch [381/2000] - Train Loss: 6.7551 - Val Loss: 6.0155\n",
      "Epoch [382/2000] - Train Loss: 6.7505 - Val Loss: 6.0253\n",
      "Epoch [383/2000] - Train Loss: 6.6750 - Val Loss: 6.1966\n",
      "Epoch [384/2000] - Train Loss: 6.6237 - Val Loss: 6.1514\n",
      "Epoch [385/2000] - Train Loss: 6.6723 - Val Loss: 6.0284\n",
      "Epoch [386/2000] - Train Loss: 6.6848 - Val Loss: 6.3655\n",
      "Epoch [387/2000] - Train Loss: 6.8857 - Val Loss: 6.3031\n",
      "Epoch [388/2000] - Train Loss: 6.6658 - Val Loss: 6.1572\n",
      "Epoch [389/2000] - Train Loss: 6.6401 - Val Loss: 6.1474\n",
      "Epoch [390/2000] - Train Loss: 6.6521 - Val Loss: 5.9693\n",
      "Epoch [391/2000] - Train Loss: 6.6884 - Val Loss: 6.2032\n",
      "Epoch [392/2000] - Train Loss: 6.7994 - Val Loss: 5.9829\n",
      "Epoch [393/2000] - Train Loss: 6.6638 - Val Loss: 6.4081\n",
      "Epoch [394/2000] - Train Loss: 6.7558 - Val Loss: 6.2365\n",
      "Epoch [395/2000] - Train Loss: 6.7392 - Val Loss: 6.2667\n",
      "Epoch [396/2000] - Train Loss: 6.7485 - Val Loss: 5.9740\n",
      "Epoch [397/2000] - Train Loss: 6.5614 - Val Loss: 5.9936\n",
      "Epoch [398/2000] - Train Loss: 6.5985 - Val Loss: 5.9961\n",
      "Epoch [399/2000] - Train Loss: 6.6058 - Val Loss: 5.9911\n",
      "Epoch [400/2000] - Train Loss: 6.5270 - Val Loss: 6.2023\n",
      "Epoch [401/2000] - Train Loss: 6.6568 - Val Loss: 6.1055\n",
      "Epoch [402/2000] - Train Loss: 6.6361 - Val Loss: 6.1568\n",
      "Epoch [403/2000] - Train Loss: 6.6336 - Val Loss: 6.5540\n",
      "Epoch [404/2000] - Train Loss: 6.5711 - Val Loss: 6.1043\n",
      "Epoch [405/2000] - Train Loss: 6.5849 - Val Loss: 6.0399\n",
      "Epoch [406/2000] - Train Loss: 6.5416 - Val Loss: 6.1393\n",
      "Epoch [407/2000] - Train Loss: 6.6145 - Val Loss: 6.1161\n",
      "Epoch [408/2000] - Train Loss: 6.5771 - Val Loss: 6.0236\n",
      "Epoch [409/2000] - Train Loss: 6.6335 - Val Loss: 5.9191\n",
      "Epoch [410/2000] - Train Loss: 6.5597 - Val Loss: 5.8099\n",
      "Epoch [411/2000] - Train Loss: 6.5280 - Val Loss: 5.8343\n",
      "Epoch [412/2000] - Train Loss: 6.5309 - Val Loss: 6.0275\n",
      "Epoch [413/2000] - Train Loss: 6.5460 - Val Loss: 5.9552\n",
      "Epoch [414/2000] - Train Loss: 6.5378 - Val Loss: 5.8913\n",
      "Epoch [415/2000] - Train Loss: 6.5949 - Val Loss: 5.9323\n",
      "Epoch [416/2000] - Train Loss: 6.5085 - Val Loss: 6.2622\n",
      "Epoch [417/2000] - Train Loss: 6.5171 - Val Loss: 5.8607\n",
      "Epoch [418/2000] - Train Loss: 6.5822 - Val Loss: 6.0684\n",
      "Epoch [419/2000] - Train Loss: 6.5126 - Val Loss: 6.0999\n",
      "Epoch [420/2000] - Train Loss: 6.5835 - Val Loss: 5.9036\n",
      "Epoch [421/2000] - Train Loss: 6.5378 - Val Loss: 5.8791\n",
      "Epoch [422/2000] - Train Loss: 6.4880 - Val Loss: 5.9928\n",
      "Epoch [423/2000] - Train Loss: 6.4670 - Val Loss: 6.1591\n",
      "Epoch [424/2000] - Train Loss: 6.5538 - Val Loss: 5.9154\n",
      "Epoch [425/2000] - Train Loss: 6.5291 - Val Loss: 5.8550\n",
      "Epoch [426/2000] - Train Loss: 6.5716 - Val Loss: 5.8009\n",
      "Epoch [427/2000] - Train Loss: 6.4498 - Val Loss: 5.8744\n",
      "Epoch [428/2000] - Train Loss: 6.4812 - Val Loss: 5.9846\n",
      "Epoch [429/2000] - Train Loss: 6.4801 - Val Loss: 5.8701\n",
      "Epoch [430/2000] - Train Loss: 6.5395 - Val Loss: 6.6185\n",
      "Epoch [431/2000] - Train Loss: 6.5597 - Val Loss: 5.6800\n",
      "Epoch [432/2000] - Train Loss: 6.4922 - Val Loss: 5.8458\n",
      "Epoch [433/2000] - Train Loss: 6.5704 - Val Loss: 5.8849\n",
      "Epoch [434/2000] - Train Loss: 6.4394 - Val Loss: 5.7524\n",
      "Epoch [435/2000] - Train Loss: 6.4439 - Val Loss: 5.8527\n",
      "Epoch [436/2000] - Train Loss: 6.4611 - Val Loss: 5.9193\n",
      "Epoch [437/2000] - Train Loss: 6.3869 - Val Loss: 5.7651\n",
      "Epoch [438/2000] - Train Loss: 6.3263 - Val Loss: 6.0815\n",
      "Epoch [439/2000] - Train Loss: 6.5229 - Val Loss: 5.8271\n",
      "Epoch [440/2000] - Train Loss: 6.5165 - Val Loss: 5.8200\n",
      "Epoch [441/2000] - Train Loss: 6.4502 - Val Loss: 5.8150\n",
      "Epoch [442/2000] - Train Loss: 6.3324 - Val Loss: 5.8055\n",
      "Epoch [443/2000] - Train Loss: 6.3529 - Val Loss: 6.0135\n",
      "Epoch [444/2000] - Train Loss: 6.4443 - Val Loss: 5.8283\n",
      "Epoch [445/2000] - Train Loss: 6.4502 - Val Loss: 5.8583\n",
      "Epoch [446/2000] - Train Loss: 6.3920 - Val Loss: 6.2351\n",
      "Epoch [447/2000] - Train Loss: 6.3982 - Val Loss: 5.9547\n",
      "Epoch [448/2000] - Train Loss: 6.4475 - Val Loss: 5.8005\n",
      "Epoch [449/2000] - Train Loss: 6.3569 - Val Loss: 5.8977\n",
      "Epoch [450/2000] - Train Loss: 6.4104 - Val Loss: 6.3068\n",
      "Epoch [451/2000] - Train Loss: 6.3053 - Val Loss: 5.6272\n",
      "Epoch [452/2000] - Train Loss: 6.4065 - Val Loss: 5.8384\n",
      "Epoch [453/2000] - Train Loss: 6.3653 - Val Loss: 5.8688\n",
      "Epoch [454/2000] - Train Loss: 6.3936 - Val Loss: 5.8203\n",
      "Epoch [455/2000] - Train Loss: 6.3715 - Val Loss: 5.6999\n",
      "Epoch [456/2000] - Train Loss: 6.4180 - Val Loss: 5.6580\n",
      "Epoch [457/2000] - Train Loss: 6.1738 - Val Loss: 5.7273\n",
      "Epoch [458/2000] - Train Loss: 6.3150 - Val Loss: 5.6495\n",
      "Epoch [459/2000] - Train Loss: 6.3234 - Val Loss: 5.6683\n",
      "Epoch [460/2000] - Train Loss: 6.4755 - Val Loss: 5.6231\n",
      "Epoch [461/2000] - Train Loss: 6.3112 - Val Loss: 5.8776\n",
      "Epoch [462/2000] - Train Loss: 6.3317 - Val Loss: 5.7157\n",
      "Epoch [463/2000] - Train Loss: 6.2419 - Val Loss: 5.7258\n",
      "Epoch [464/2000] - Train Loss: 6.2825 - Val Loss: 5.6467\n",
      "Epoch [465/2000] - Train Loss: 6.2337 - Val Loss: 5.7865\n",
      "Epoch [466/2000] - Train Loss: 6.2566 - Val Loss: 6.0483\n",
      "Epoch [467/2000] - Train Loss: 6.2514 - Val Loss: 5.8281\n",
      "Epoch [468/2000] - Train Loss: 6.2530 - Val Loss: 5.9759\n",
      "Epoch [469/2000] - Train Loss: 6.3009 - Val Loss: 5.5893\n",
      "Epoch [470/2000] - Train Loss: 6.3005 - Val Loss: 5.8573\n",
      "Epoch [471/2000] - Train Loss: 6.4132 - Val Loss: 5.7176\n",
      "Epoch [472/2000] - Train Loss: 6.2698 - Val Loss: 5.8327\n",
      "Epoch [473/2000] - Train Loss: 6.3582 - Val Loss: 5.8260\n",
      "Epoch [474/2000] - Train Loss: 6.2459 - Val Loss: 5.6590\n",
      "Epoch [475/2000] - Train Loss: 6.2220 - Val Loss: 5.6523\n",
      "Epoch [476/2000] - Train Loss: 6.2940 - Val Loss: 5.8181\n",
      "Epoch [477/2000] - Train Loss: 6.2337 - Val Loss: 5.5822\n",
      "Epoch [478/2000] - Train Loss: 6.2547 - Val Loss: 5.5672\n",
      "Epoch [479/2000] - Train Loss: 6.1858 - Val Loss: 5.5936\n",
      "Epoch [480/2000] - Train Loss: 6.3332 - Val Loss: 5.8296\n",
      "Epoch [481/2000] - Train Loss: 6.2466 - Val Loss: 5.5851\n",
      "Epoch [482/2000] - Train Loss: 6.2665 - Val Loss: 6.3612\n",
      "Epoch [483/2000] - Train Loss: 6.4275 - Val Loss: 5.5696\n",
      "Epoch [484/2000] - Train Loss: 6.1881 - Val Loss: 5.6752\n",
      "Epoch [485/2000] - Train Loss: 6.2176 - Val Loss: 5.6423\n",
      "Epoch [486/2000] - Train Loss: 6.3263 - Val Loss: 5.9489\n",
      "Epoch [487/2000] - Train Loss: 6.2009 - Val Loss: 5.6511\n",
      "Epoch [488/2000] - Train Loss: 6.3077 - Val Loss: 5.9765\n",
      "Epoch [489/2000] - Train Loss: 6.2486 - Val Loss: 5.7639\n",
      "Epoch [490/2000] - Train Loss: 6.2121 - Val Loss: 5.6020\n",
      "Epoch [491/2000] - Train Loss: 6.1764 - Val Loss: 5.7852\n",
      "Epoch [492/2000] - Train Loss: 6.1724 - Val Loss: 5.6347\n",
      "Epoch [493/2000] - Train Loss: 6.1980 - Val Loss: 6.2146\n",
      "Epoch [494/2000] - Train Loss: 6.0929 - Val Loss: 5.5044\n",
      "Epoch [495/2000] - Train Loss: 6.1552 - Val Loss: 5.6368\n",
      "Epoch [496/2000] - Train Loss: 6.2209 - Val Loss: 5.5695\n",
      "Epoch [497/2000] - Train Loss: 6.1044 - Val Loss: 5.7283\n",
      "Epoch [498/2000] - Train Loss: 6.2510 - Val Loss: 5.7058\n",
      "Epoch [499/2000] - Train Loss: 6.1346 - Val Loss: 5.5971\n",
      "Epoch [500/2000] - Train Loss: 6.0801 - Val Loss: 5.7203\n",
      "Epoch [501/2000] - Train Loss: 6.1197 - Val Loss: 6.2478\n",
      "Epoch [502/2000] - Train Loss: 6.2717 - Val Loss: 5.9033\n",
      "Epoch [503/2000] - Train Loss: 6.1647 - Val Loss: 5.5756\n",
      "Epoch [504/2000] - Train Loss: 6.1865 - Val Loss: 5.7575\n",
      "Epoch [505/2000] - Train Loss: 6.0776 - Val Loss: 5.6506\n",
      "Epoch [506/2000] - Train Loss: 6.0361 - Val Loss: 5.7830\n",
      "Epoch [507/2000] - Train Loss: 6.1357 - Val Loss: 5.5492\n",
      "Epoch [508/2000] - Train Loss: 6.0819 - Val Loss: 5.7673\n",
      "Epoch [509/2000] - Train Loss: 6.1536 - Val Loss: 5.5351\n",
      "Epoch [510/2000] - Train Loss: 6.0210 - Val Loss: 5.6515\n",
      "Epoch [511/2000] - Train Loss: 6.1491 - Val Loss: 5.7119\n",
      "Epoch [512/2000] - Train Loss: 6.0983 - Val Loss: 5.6377\n",
      "Epoch [513/2000] - Train Loss: 6.1754 - Val Loss: 5.4878\n",
      "Epoch [514/2000] - Train Loss: 6.1668 - Val Loss: 6.5876\n",
      "Epoch [515/2000] - Train Loss: 6.2539 - Val Loss: 5.5641\n",
      "Epoch [516/2000] - Train Loss: 6.1904 - Val Loss: 5.8982\n",
      "Epoch [517/2000] - Train Loss: 6.0838 - Val Loss: 5.5200\n",
      "Epoch [518/2000] - Train Loss: 6.0838 - Val Loss: 5.4992\n",
      "Epoch [519/2000] - Train Loss: 6.0708 - Val Loss: 5.5668\n",
      "Epoch [520/2000] - Train Loss: 6.0443 - Val Loss: 5.6639\n",
      "Epoch [521/2000] - Train Loss: 6.0371 - Val Loss: 5.4694\n",
      "Epoch [522/2000] - Train Loss: 6.0319 - Val Loss: 5.8433\n",
      "Epoch [523/2000] - Train Loss: 6.0796 - Val Loss: 5.4368\n",
      "Epoch [524/2000] - Train Loss: 6.0402 - Val Loss: 5.5445\n",
      "Epoch [525/2000] - Train Loss: 6.1362 - Val Loss: 5.6584\n",
      "Epoch [526/2000] - Train Loss: 6.0050 - Val Loss: 5.4339\n",
      "Epoch [527/2000] - Train Loss: 5.9490 - Val Loss: 5.4123\n",
      "Epoch [528/2000] - Train Loss: 6.0689 - Val Loss: 5.4633\n",
      "Epoch [529/2000] - Train Loss: 6.1279 - Val Loss: 5.6481\n",
      "Epoch [530/2000] - Train Loss: 6.0479 - Val Loss: 5.5390\n",
      "Epoch [531/2000] - Train Loss: 5.8613 - Val Loss: 5.4410\n",
      "Epoch [532/2000] - Train Loss: 5.9801 - Val Loss: 5.5156\n",
      "Epoch [533/2000] - Train Loss: 6.0562 - Val Loss: 5.5370\n",
      "Epoch [534/2000] - Train Loss: 6.0300 - Val Loss: 5.5249\n",
      "Epoch [535/2000] - Train Loss: 5.9918 - Val Loss: 5.5508\n",
      "Epoch [536/2000] - Train Loss: 5.9560 - Val Loss: 5.5382\n",
      "Epoch [537/2000] - Train Loss: 6.0810 - Val Loss: 5.5911\n",
      "Epoch [538/2000] - Train Loss: 6.0819 - Val Loss: 5.5787\n",
      "Epoch [539/2000] - Train Loss: 5.9655 - Val Loss: 5.7200\n",
      "Epoch [540/2000] - Train Loss: 6.0395 - Val Loss: 5.5146\n",
      "Epoch [541/2000] - Train Loss: 6.0165 - Val Loss: 5.4003\n",
      "Epoch [542/2000] - Train Loss: 5.9596 - Val Loss: 5.5113\n",
      "Epoch [543/2000] - Train Loss: 6.0340 - Val Loss: 5.7323\n",
      "Epoch [544/2000] - Train Loss: 5.9417 - Val Loss: 5.5272\n",
      "Epoch [545/2000] - Train Loss: 6.0222 - Val Loss: 5.6645\n",
      "Epoch [546/2000] - Train Loss: 5.9960 - Val Loss: 5.4244\n",
      "Epoch [547/2000] - Train Loss: 6.0196 - Val Loss: 5.7457\n",
      "Epoch [548/2000] - Train Loss: 6.0047 - Val Loss: 5.4731\n",
      "Epoch [549/2000] - Train Loss: 5.9679 - Val Loss: 5.4172\n",
      "Epoch [550/2000] - Train Loss: 5.9658 - Val Loss: 5.5652\n",
      "Epoch [551/2000] - Train Loss: 5.9577 - Val Loss: 5.6363\n",
      "Epoch [552/2000] - Train Loss: 5.9572 - Val Loss: 5.5062\n",
      "Epoch [553/2000] - Train Loss: 5.9472 - Val Loss: 5.5427\n",
      "Epoch [554/2000] - Train Loss: 6.0679 - Val Loss: 5.5223\n",
      "Epoch [555/2000] - Train Loss: 5.9787 - Val Loss: 5.6166\n",
      "Epoch [556/2000] - Train Loss: 5.9512 - Val Loss: 5.7055\n",
      "Epoch [557/2000] - Train Loss: 5.9655 - Val Loss: 5.3598\n",
      "Epoch [558/2000] - Train Loss: 5.9262 - Val Loss: 5.5169\n",
      "Epoch [559/2000] - Train Loss: 5.9865 - Val Loss: 5.5309\n",
      "Epoch [560/2000] - Train Loss: 5.8739 - Val Loss: 5.4645\n",
      "Epoch [561/2000] - Train Loss: 5.9126 - Val Loss: 5.6730\n",
      "Epoch [562/2000] - Train Loss: 6.0001 - Val Loss: 5.4278\n",
      "Epoch [563/2000] - Train Loss: 5.9648 - Val Loss: 5.4382\n",
      "Epoch [564/2000] - Train Loss: 5.9438 - Val Loss: 5.3694\n",
      "Epoch [565/2000] - Train Loss: 5.9688 - Val Loss: 5.5526\n",
      "Epoch [566/2000] - Train Loss: 5.9528 - Val Loss: 5.4540\n",
      "Epoch [567/2000] - Train Loss: 5.9319 - Val Loss: 5.6001\n",
      "Epoch [568/2000] - Train Loss: 5.8970 - Val Loss: 5.3395\n",
      "Epoch [569/2000] - Train Loss: 5.8317 - Val Loss: 5.2722\n",
      "Epoch [570/2000] - Train Loss: 5.9683 - Val Loss: 5.3110\n",
      "Epoch [571/2000] - Train Loss: 5.9381 - Val Loss: 5.4295\n",
      "Epoch [572/2000] - Train Loss: 5.9147 - Val Loss: 5.3208\n",
      "Epoch [573/2000] - Train Loss: 5.8661 - Val Loss: 5.4076\n",
      "Epoch [574/2000] - Train Loss: 5.9052 - Val Loss: 5.4436\n",
      "Epoch [575/2000] - Train Loss: 5.9666 - Val Loss: 5.3395\n",
      "Epoch [576/2000] - Train Loss: 5.8252 - Val Loss: 5.4358\n",
      "Epoch [577/2000] - Train Loss: 5.9696 - Val Loss: 5.5109\n",
      "Epoch [578/2000] - Train Loss: 5.9063 - Val Loss: 5.3133\n",
      "Epoch [579/2000] - Train Loss: 5.9748 - Val Loss: 5.5235\n",
      "Epoch [580/2000] - Train Loss: 5.8771 - Val Loss: 5.6631\n",
      "Epoch [581/2000] - Train Loss: 5.8838 - Val Loss: 5.6110\n",
      "Epoch [582/2000] - Train Loss: 5.8448 - Val Loss: 5.4548\n",
      "Epoch [583/2000] - Train Loss: 5.8753 - Val Loss: 5.4201\n",
      "Epoch [584/2000] - Train Loss: 5.8284 - Val Loss: 5.4928\n",
      "Epoch [585/2000] - Train Loss: 5.8400 - Val Loss: 5.4954\n",
      "Epoch [586/2000] - Train Loss: 5.7274 - Val Loss: 5.3131\n",
      "Epoch [587/2000] - Train Loss: 5.8737 - Val Loss: 5.2503\n",
      "Epoch [588/2000] - Train Loss: 5.9021 - Val Loss: 5.3727\n",
      "Epoch [589/2000] - Train Loss: 5.7486 - Val Loss: 5.3118\n",
      "Epoch [590/2000] - Train Loss: 5.8396 - Val Loss: 5.5361\n",
      "Epoch [591/2000] - Train Loss: 5.8320 - Val Loss: 5.6284\n",
      "Epoch [592/2000] - Train Loss: 5.7993 - Val Loss: 5.5724\n",
      "Epoch [593/2000] - Train Loss: 5.7975 - Val Loss: 5.3509\n",
      "Epoch [594/2000] - Train Loss: 5.9618 - Val Loss: 5.6023\n",
      "Epoch [595/2000] - Train Loss: 5.8026 - Val Loss: 5.9698\n",
      "Epoch [596/2000] - Train Loss: 5.8816 - Val Loss: 5.2709\n",
      "Epoch [597/2000] - Train Loss: 5.8212 - Val Loss: 5.6093\n",
      "Epoch [598/2000] - Train Loss: 5.7376 - Val Loss: 5.7990\n",
      "Epoch [599/2000] - Train Loss: 5.8747 - Val Loss: 5.4469\n",
      "Epoch [600/2000] - Train Loss: 5.9000 - Val Loss: 5.3388\n",
      "Epoch [601/2000] - Train Loss: 5.8779 - Val Loss: 5.3509\n",
      "Epoch [602/2000] - Train Loss: 5.8461 - Val Loss: 5.5581\n",
      "Epoch [603/2000] - Train Loss: 5.7854 - Val Loss: 5.3277\n",
      "Epoch [604/2000] - Train Loss: 5.8236 - Val Loss: 5.4229\n",
      "Epoch [605/2000] - Train Loss: 5.9209 - Val Loss: 5.3475\n",
      "Epoch [606/2000] - Train Loss: 5.7923 - Val Loss: 5.9544\n",
      "Epoch [607/2000] - Train Loss: 5.8317 - Val Loss: 5.2773\n",
      "Epoch [608/2000] - Train Loss: 5.7558 - Val Loss: 5.2985\n",
      "Epoch [609/2000] - Train Loss: 5.7810 - Val Loss: 5.5556\n",
      "Epoch [610/2000] - Train Loss: 5.7469 - Val Loss: 5.3202\n",
      "Epoch [611/2000] - Train Loss: 5.8744 - Val Loss: 5.5076\n",
      "Epoch [612/2000] - Train Loss: 5.8287 - Val Loss: 5.3679\n",
      "Epoch [613/2000] - Train Loss: 5.8955 - Val Loss: 5.5851\n",
      "Epoch [614/2000] - Train Loss: 5.8730 - Val Loss: 5.3335\n",
      "Epoch [615/2000] - Train Loss: 5.7736 - Val Loss: 5.3726\n",
      "Epoch [616/2000] - Train Loss: 5.7748 - Val Loss: 5.5581\n",
      "Epoch [617/2000] - Train Loss: 5.7825 - Val Loss: 5.5728\n",
      "Epoch [618/2000] - Train Loss: 5.7746 - Val Loss: 5.2387\n",
      "Epoch [619/2000] - Train Loss: 5.8135 - Val Loss: 5.3672\n",
      "Epoch [620/2000] - Train Loss: 5.7326 - Val Loss: 5.2307\n",
      "Epoch [621/2000] - Train Loss: 5.7885 - Val Loss: 5.2821\n",
      "Epoch [622/2000] - Train Loss: 5.7828 - Val Loss: 5.7125\n",
      "Epoch [623/2000] - Train Loss: 5.7920 - Val Loss: 5.4626\n",
      "Epoch [624/2000] - Train Loss: 5.7732 - Val Loss: 5.4304\n",
      "Epoch [625/2000] - Train Loss: 5.7079 - Val Loss: 5.3209\n",
      "Epoch [626/2000] - Train Loss: 5.7365 - Val Loss: 5.2501\n",
      "Epoch [627/2000] - Train Loss: 5.6954 - Val Loss: 5.1857\n",
      "Epoch [628/2000] - Train Loss: 5.7121 - Val Loss: 5.3219\n",
      "Epoch [629/2000] - Train Loss: 5.7402 - Val Loss: 5.3200\n",
      "Epoch [630/2000] - Train Loss: 5.7619 - Val Loss: 5.4565\n",
      "Epoch [631/2000] - Train Loss: 5.7772 - Val Loss: 5.4915\n",
      "Epoch [632/2000] - Train Loss: 5.7308 - Val Loss: 5.3528\n",
      "Epoch [633/2000] - Train Loss: 5.7519 - Val Loss: 5.2953\n",
      "Epoch [634/2000] - Train Loss: 5.8018 - Val Loss: 5.2960\n",
      "Epoch [635/2000] - Train Loss: 5.7083 - Val Loss: 5.4623\n",
      "Epoch [636/2000] - Train Loss: 5.6813 - Val Loss: 5.6270\n",
      "Epoch [637/2000] - Train Loss: 5.6791 - Val Loss: 5.3253\n",
      "Epoch [638/2000] - Train Loss: 5.6930 - Val Loss: 5.3022\n",
      "Epoch [639/2000] - Train Loss: 5.5847 - Val Loss: 5.3228\n",
      "Epoch [640/2000] - Train Loss: 5.7243 - Val Loss: 5.1588\n",
      "Epoch [641/2000] - Train Loss: 5.7455 - Val Loss: 5.4317\n",
      "Epoch [642/2000] - Train Loss: 5.6802 - Val Loss: 5.2206\n",
      "Epoch [643/2000] - Train Loss: 5.7296 - Val Loss: 5.1964\n",
      "Epoch [644/2000] - Train Loss: 5.6125 - Val Loss: 5.3252\n",
      "Epoch [645/2000] - Train Loss: 5.6522 - Val Loss: 5.3926\n",
      "Epoch [646/2000] - Train Loss: 5.7184 - Val Loss: 5.3188\n",
      "Epoch [647/2000] - Train Loss: 5.6902 - Val Loss: 5.1625\n",
      "Epoch [648/2000] - Train Loss: 5.6978 - Val Loss: 5.2599\n",
      "Epoch [649/2000] - Train Loss: 5.6760 - Val Loss: 5.1256\n",
      "Epoch [650/2000] - Train Loss: 5.7344 - Val Loss: 5.4182\n",
      "Epoch [651/2000] - Train Loss: 5.7106 - Val Loss: 5.2130\n",
      "Epoch [652/2000] - Train Loss: 5.6855 - Val Loss: 5.2445\n",
      "Epoch [653/2000] - Train Loss: 5.6988 - Val Loss: 5.3944\n",
      "Epoch [654/2000] - Train Loss: 5.7061 - Val Loss: 5.1218\n",
      "Epoch [655/2000] - Train Loss: 5.7065 - Val Loss: 5.3178\n",
      "Epoch [656/2000] - Train Loss: 5.6830 - Val Loss: 5.3648\n",
      "Epoch [657/2000] - Train Loss: 5.7765 - Val Loss: 5.1770\n",
      "Epoch [658/2000] - Train Loss: 5.6476 - Val Loss: 5.2808\n",
      "Epoch [659/2000] - Train Loss: 5.6766 - Val Loss: 5.1543\n",
      "Epoch [660/2000] - Train Loss: 5.5843 - Val Loss: 5.3512\n",
      "Epoch [661/2000] - Train Loss: 5.7075 - Val Loss: 5.4018\n",
      "Epoch [662/2000] - Train Loss: 5.6142 - Val Loss: 5.1614\n",
      "Epoch [663/2000] - Train Loss: 5.5900 - Val Loss: 5.1080\n",
      "Epoch [664/2000] - Train Loss: 5.5498 - Val Loss: 5.1930\n",
      "Epoch [665/2000] - Train Loss: 5.6009 - Val Loss: 5.1968\n",
      "Epoch [666/2000] - Train Loss: 5.5630 - Val Loss: 5.0653\n",
      "Epoch [667/2000] - Train Loss: 5.6384 - Val Loss: 5.1626\n",
      "Epoch [668/2000] - Train Loss: 5.6298 - Val Loss: 5.2761\n",
      "Epoch [669/2000] - Train Loss: 5.6237 - Val Loss: 5.4069\n",
      "Epoch [670/2000] - Train Loss: 5.6208 - Val Loss: 5.5474\n",
      "Epoch [671/2000] - Train Loss: 5.5714 - Val Loss: 5.4378\n",
      "Epoch [672/2000] - Train Loss: 5.6179 - Val Loss: 5.3985\n",
      "Epoch [673/2000] - Train Loss: 5.6064 - Val Loss: 5.4138\n",
      "Epoch [674/2000] - Train Loss: 5.5429 - Val Loss: 5.1491\n",
      "Epoch [675/2000] - Train Loss: 5.6050 - Val Loss: 5.1682\n",
      "Epoch [676/2000] - Train Loss: 5.5793 - Val Loss: 5.1922\n",
      "Epoch [677/2000] - Train Loss: 5.5500 - Val Loss: 5.3890\n",
      "Epoch [678/2000] - Train Loss: 5.6807 - Val Loss: 5.3434\n",
      "Epoch [679/2000] - Train Loss: 5.5287 - Val Loss: 5.2415\n",
      "Epoch [680/2000] - Train Loss: 5.5920 - Val Loss: 5.5290\n",
      "Epoch [681/2000] - Train Loss: 5.5538 - Val Loss: 5.1797\n",
      "Epoch [682/2000] - Train Loss: 5.6027 - Val Loss: 5.4482\n",
      "Epoch [683/2000] - Train Loss: 5.5663 - Val Loss: 5.1518\n",
      "Epoch [684/2000] - Train Loss: 5.6518 - Val Loss: 5.1252\n",
      "Epoch [685/2000] - Train Loss: 5.6485 - Val Loss: 5.2203\n",
      "Epoch [686/2000] - Train Loss: 5.6105 - Val Loss: 5.2258\n",
      "Epoch [687/2000] - Train Loss: 5.6416 - Val Loss: 5.4045\n",
      "Epoch [688/2000] - Train Loss: 5.5567 - Val Loss: 5.2223\n",
      "Epoch [689/2000] - Train Loss: 5.5354 - Val Loss: 5.0940\n",
      "Epoch [690/2000] - Train Loss: 5.5920 - Val Loss: 5.2424\n",
      "Epoch [691/2000] - Train Loss: 5.6074 - Val Loss: 5.1697\n",
      "Epoch [692/2000] - Train Loss: 5.4759 - Val Loss: 5.1768\n",
      "Epoch [693/2000] - Train Loss: 5.5220 - Val Loss: 5.3726\n",
      "Epoch [694/2000] - Train Loss: 5.5464 - Val Loss: 5.2027\n",
      "Epoch [695/2000] - Train Loss: 5.5269 - Val Loss: 5.3198\n",
      "Epoch [696/2000] - Train Loss: 5.5955 - Val Loss: 5.1265\n",
      "Epoch [697/2000] - Train Loss: 5.5138 - Val Loss: 5.0829\n",
      "Epoch [698/2000] - Train Loss: 5.5426 - Val Loss: 5.1802\n",
      "Epoch [699/2000] - Train Loss: 5.7057 - Val Loss: 5.1741\n",
      "Epoch [700/2000] - Train Loss: 5.4730 - Val Loss: 5.2791\n",
      "Epoch [701/2000] - Train Loss: 5.5415 - Val Loss: 5.2086\n",
      "Epoch [702/2000] - Train Loss: 5.4964 - Val Loss: 5.1370\n",
      "Epoch [703/2000] - Train Loss: 5.4771 - Val Loss: 5.1482\n",
      "Epoch [704/2000] - Train Loss: 5.5088 - Val Loss: 5.2530\n",
      "Epoch [705/2000] - Train Loss: 5.5175 - Val Loss: 5.2922\n",
      "Epoch [706/2000] - Train Loss: 5.5589 - Val Loss: 5.1710\n",
      "Epoch [707/2000] - Train Loss: 5.4322 - Val Loss: 5.1831\n",
      "Epoch [708/2000] - Train Loss: 5.5291 - Val Loss: 5.2385\n",
      "Epoch [709/2000] - Train Loss: 5.5118 - Val Loss: 5.0906\n",
      "Epoch [710/2000] - Train Loss: 5.6244 - Val Loss: 5.0636\n",
      "Epoch [711/2000] - Train Loss: 5.5202 - Val Loss: 5.1669\n",
      "Epoch [712/2000] - Train Loss: 5.5525 - Val Loss: 5.1814\n",
      "Epoch [713/2000] - Train Loss: 5.4827 - Val Loss: 5.1835\n",
      "Epoch [714/2000] - Train Loss: 5.5561 - Val Loss: 5.1242\n",
      "Epoch [715/2000] - Train Loss: 5.5106 - Val Loss: 5.1075\n",
      "Epoch [716/2000] - Train Loss: 5.4594 - Val Loss: 5.0535\n",
      "Epoch [717/2000] - Train Loss: 5.4113 - Val Loss: 5.1552\n",
      "Epoch [718/2000] - Train Loss: 5.5223 - Val Loss: 5.8533\n",
      "Epoch [719/2000] - Train Loss: 5.6892 - Val Loss: 5.0185\n",
      "Epoch [720/2000] - Train Loss: 5.4689 - Val Loss: 5.1568\n",
      "Epoch [721/2000] - Train Loss: 5.4516 - Val Loss: 5.2528\n",
      "Epoch [722/2000] - Train Loss: 5.4487 - Val Loss: 5.2633\n",
      "Epoch [723/2000] - Train Loss: 5.4643 - Val Loss: 5.2395\n",
      "Epoch [724/2000] - Train Loss: 5.4664 - Val Loss: 5.2769\n",
      "Epoch [725/2000] - Train Loss: 5.4606 - Val Loss: 5.3512\n",
      "Epoch [726/2000] - Train Loss: 5.4547 - Val Loss: 5.1813\n",
      "Epoch [727/2000] - Train Loss: 5.4949 - Val Loss: 5.0566\n",
      "Epoch [728/2000] - Train Loss: 5.5376 - Val Loss: 5.3782\n",
      "Epoch [729/2000] - Train Loss: 5.4474 - Val Loss: 5.3667\n",
      "Epoch [730/2000] - Train Loss: 5.4690 - Val Loss: 4.9481\n",
      "Epoch [731/2000] - Train Loss: 5.3950 - Val Loss: 5.0647\n",
      "Epoch [732/2000] - Train Loss: 5.5255 - Val Loss: 5.1225\n",
      "Epoch [733/2000] - Train Loss: 5.4918 - Val Loss: 5.0699\n",
      "Epoch [734/2000] - Train Loss: 5.4018 - Val Loss: 5.3171\n",
      "Epoch [735/2000] - Train Loss: 5.4315 - Val Loss: 5.0460\n",
      "Epoch [736/2000] - Train Loss: 5.4018 - Val Loss: 5.1155\n",
      "Epoch [737/2000] - Train Loss: 5.3629 - Val Loss: 5.1047\n",
      "Epoch [738/2000] - Train Loss: 5.4501 - Val Loss: 5.0918\n",
      "Epoch [739/2000] - Train Loss: 5.3788 - Val Loss: 5.1352\n",
      "Epoch [740/2000] - Train Loss: 5.4824 - Val Loss: 5.3727\n",
      "Epoch [741/2000] - Train Loss: 5.5081 - Val Loss: 5.2592\n",
      "Epoch [742/2000] - Train Loss: 5.4533 - Val Loss: 5.1627\n",
      "Epoch [743/2000] - Train Loss: 5.4415 - Val Loss: 5.0841\n",
      "Epoch [744/2000] - Train Loss: 5.5260 - Val Loss: 5.0594\n",
      "Epoch [745/2000] - Train Loss: 5.4330 - Val Loss: 5.1714\n",
      "Epoch [746/2000] - Train Loss: 5.4716 - Val Loss: 5.1917\n",
      "Epoch [747/2000] - Train Loss: 5.3872 - Val Loss: 5.0741\n",
      "Epoch [748/2000] - Train Loss: 5.4462 - Val Loss: 5.1836\n",
      "Epoch [749/2000] - Train Loss: 5.4839 - Val Loss: 5.2331\n",
      "Epoch [750/2000] - Train Loss: 5.3941 - Val Loss: 5.2729\n",
      "Epoch [751/2000] - Train Loss: 5.3678 - Val Loss: 5.1984\n",
      "Epoch [752/2000] - Train Loss: 5.3517 - Val Loss: 4.9660\n",
      "Epoch [753/2000] - Train Loss: 5.4171 - Val Loss: 5.2164\n",
      "Epoch [754/2000] - Train Loss: 5.4275 - Val Loss: 5.2092\n",
      "Epoch [755/2000] - Train Loss: 5.3487 - Val Loss: 4.9879\n",
      "Epoch [756/2000] - Train Loss: 5.2867 - Val Loss: 5.0989\n",
      "Epoch [757/2000] - Train Loss: 5.4973 - Val Loss: 5.2345\n",
      "Epoch [758/2000] - Train Loss: 5.3790 - Val Loss: 5.0121\n",
      "Epoch [759/2000] - Train Loss: 5.3901 - Val Loss: 5.4067\n",
      "Epoch [760/2000] - Train Loss: 5.4299 - Val Loss: 5.1055\n",
      "Epoch [761/2000] - Train Loss: 5.4471 - Val Loss: 4.9250\n",
      "Epoch [762/2000] - Train Loss: 5.3440 - Val Loss: 5.1766\n",
      "Epoch [763/2000] - Train Loss: 5.3691 - Val Loss: 4.9681\n",
      "Epoch [764/2000] - Train Loss: 5.3360 - Val Loss: 5.0068\n",
      "Epoch [765/2000] - Train Loss: 5.3398 - Val Loss: 4.9401\n",
      "Epoch [766/2000] - Train Loss: 5.4172 - Val Loss: 5.6513\n",
      "Epoch [767/2000] - Train Loss: 5.3547 - Val Loss: 5.1693\n",
      "Epoch [768/2000] - Train Loss: 5.5150 - Val Loss: 5.1766\n",
      "Epoch [769/2000] - Train Loss: 5.3361 - Val Loss: 5.1543\n",
      "Epoch [770/2000] - Train Loss: 5.3952 - Val Loss: 5.1405\n",
      "Epoch [771/2000] - Train Loss: 5.3448 - Val Loss: 5.0262\n",
      "Epoch [772/2000] - Train Loss: 5.3256 - Val Loss: 5.1986\n",
      "Epoch [773/2000] - Train Loss: 5.3018 - Val Loss: 5.1791\n",
      "Epoch [774/2000] - Train Loss: 5.2705 - Val Loss: 5.0805\n",
      "Epoch [775/2000] - Train Loss: 5.3645 - Val Loss: 5.0328\n",
      "Epoch [776/2000] - Train Loss: 5.1995 - Val Loss: 5.0132\n",
      "Epoch [777/2000] - Train Loss: 5.3056 - Val Loss: 5.0110\n",
      "Epoch [778/2000] - Train Loss: 5.4048 - Val Loss: 5.0892\n",
      "Epoch [779/2000] - Train Loss: 5.2469 - Val Loss: 5.1595\n",
      "Epoch [780/2000] - Train Loss: 5.3182 - Val Loss: 5.0327\n",
      "Epoch [781/2000] - Train Loss: 5.3884 - Val Loss: 5.7113\n",
      "Epoch [782/2000] - Train Loss: 5.3683 - Val Loss: 5.1461\n",
      "Epoch [783/2000] - Train Loss: 5.2671 - Val Loss: 5.0138\n",
      "Epoch [784/2000] - Train Loss: 5.2553 - Val Loss: 5.0385\n",
      "Epoch [785/2000] - Train Loss: 5.4057 - Val Loss: 5.0704\n",
      "Epoch [786/2000] - Train Loss: 5.4275 - Val Loss: 5.2684\n",
      "Epoch [787/2000] - Train Loss: 5.2640 - Val Loss: 4.9560\n",
      "Epoch [788/2000] - Train Loss: 5.3281 - Val Loss: 5.1528\n",
      "Epoch [789/2000] - Train Loss: 5.3907 - Val Loss: 5.1304\n",
      "Epoch [790/2000] - Train Loss: 5.3499 - Val Loss: 5.1241\n",
      "Epoch [791/2000] - Train Loss: 5.3062 - Val Loss: 5.1809\n",
      "Epoch [792/2000] - Train Loss: 5.3059 - Val Loss: 5.3569\n",
      "Epoch [793/2000] - Train Loss: 5.3483 - Val Loss: 5.1603\n",
      "Epoch [794/2000] - Train Loss: 5.3611 - Val Loss: 5.0614\n",
      "Epoch [795/2000] - Train Loss: 5.2560 - Val Loss: 5.1141\n",
      "Epoch [796/2000] - Train Loss: 5.2838 - Val Loss: 4.9737\n",
      "Epoch [797/2000] - Train Loss: 5.3924 - Val Loss: 4.9553\n",
      "Epoch [798/2000] - Train Loss: 5.2817 - Val Loss: 5.0157\n",
      "Epoch [799/2000] - Train Loss: 5.2718 - Val Loss: 5.6444\n",
      "Epoch [800/2000] - Train Loss: 5.3345 - Val Loss: 4.9921\n",
      "Epoch [801/2000] - Train Loss: 5.3052 - Val Loss: 5.5529\n",
      "Epoch [802/2000] - Train Loss: 5.3649 - Val Loss: 5.1350\n",
      "Epoch [803/2000] - Train Loss: 5.3915 - Val Loss: 5.0465\n",
      "Epoch [804/2000] - Train Loss: 5.2970 - Val Loss: 4.9923\n",
      "Epoch [805/2000] - Train Loss: 5.2486 - Val Loss: 4.9306\n",
      "Epoch [806/2000] - Train Loss: 5.3102 - Val Loss: 5.1373\n",
      "Epoch [807/2000] - Train Loss: 5.2880 - Val Loss: 5.1776\n",
      "Epoch [808/2000] - Train Loss: 5.3409 - Val Loss: 5.0041\n",
      "Epoch [809/2000] - Train Loss: 5.2747 - Val Loss: 4.9657\n",
      "Epoch [810/2000] - Train Loss: 5.3166 - Val Loss: 5.1400\n",
      "Epoch [811/2000] - Train Loss: 5.2803 - Val Loss: 5.1113\n",
      "Epoch [812/2000] - Train Loss: 5.2270 - Val Loss: 5.0618\n",
      "Epoch [813/2000] - Train Loss: 5.2405 - Val Loss: 5.0183\n",
      "Epoch [814/2000] - Train Loss: 5.2406 - Val Loss: 5.0209\n",
      "Epoch [815/2000] - Train Loss: 5.2747 - Val Loss: 5.1230\n",
      "Epoch [816/2000] - Train Loss: 5.1971 - Val Loss: 5.1027\n",
      "Epoch [817/2000] - Train Loss: 5.2691 - Val Loss: 5.1586\n",
      "Epoch [818/2000] - Train Loss: 5.3586 - Val Loss: 5.1860\n",
      "Epoch [819/2000] - Train Loss: 5.2900 - Val Loss: 5.2187\n",
      "Epoch [820/2000] - Train Loss: 5.3185 - Val Loss: 5.1869\n",
      "Epoch [821/2000] - Train Loss: 5.2322 - Val Loss: 5.1148\n",
      "Epoch [822/2000] - Train Loss: 5.2997 - Val Loss: 4.9567\n",
      "Epoch [823/2000] - Train Loss: 5.3687 - Val Loss: 4.9358\n",
      "Epoch [824/2000] - Train Loss: 5.1931 - Val Loss: 4.9869\n",
      "Epoch [825/2000] - Train Loss: 5.2233 - Val Loss: 4.8762\n",
      "Epoch [826/2000] - Train Loss: 5.2820 - Val Loss: 4.9825\n",
      "Epoch [827/2000] - Train Loss: 5.2086 - Val Loss: 5.1285\n",
      "Epoch [828/2000] - Train Loss: 5.2169 - Val Loss: 5.1792\n",
      "Epoch [829/2000] - Train Loss: 5.2714 - Val Loss: 4.9505\n",
      "Epoch [830/2000] - Train Loss: 5.2229 - Val Loss: 5.2755\n",
      "Epoch [831/2000] - Train Loss: 5.2910 - Val Loss: 5.2245\n",
      "Epoch [832/2000] - Train Loss: 5.2422 - Val Loss: 5.0402\n",
      "Epoch [833/2000] - Train Loss: 5.1878 - Val Loss: 5.0635\n",
      "Epoch [834/2000] - Train Loss: 5.2739 - Val Loss: 5.0278\n",
      "Epoch [835/2000] - Train Loss: 5.1441 - Val Loss: 4.8769\n",
      "Epoch [836/2000] - Train Loss: 5.1541 - Val Loss: 4.9714\n",
      "Epoch [837/2000] - Train Loss: 5.2637 - Val Loss: 4.8946\n",
      "Epoch [838/2000] - Train Loss: 5.2122 - Val Loss: 4.9655\n",
      "Epoch [839/2000] - Train Loss: 5.2335 - Val Loss: 4.9301\n",
      "Epoch [840/2000] - Train Loss: 5.2385 - Val Loss: 4.9041\n",
      "Epoch [841/2000] - Train Loss: 5.2600 - Val Loss: 5.0055\n",
      "Epoch [842/2000] - Train Loss: 5.1936 - Val Loss: 4.9472\n",
      "Epoch [843/2000] - Train Loss: 5.2833 - Val Loss: 5.0601\n",
      "Epoch [844/2000] - Train Loss: 5.2742 - Val Loss: 5.0464\n",
      "Epoch [845/2000] - Train Loss: 5.2090 - Val Loss: 5.1556\n",
      "Epoch [846/2000] - Train Loss: 5.5096 - Val Loss: 5.1068\n",
      "Epoch [847/2000] - Train Loss: 5.1681 - Val Loss: 5.0137\n",
      "Epoch [848/2000] - Train Loss: 5.2460 - Val Loss: 4.9950\n",
      "Epoch [849/2000] - Train Loss: 5.1714 - Val Loss: 4.8998\n",
      "Epoch [850/2000] - Train Loss: 5.1986 - Val Loss: 5.0283\n",
      "Epoch [851/2000] - Train Loss: 5.1313 - Val Loss: 4.9030\n",
      "Epoch [852/2000] - Train Loss: 5.1361 - Val Loss: 5.1413\n",
      "Epoch [853/2000] - Train Loss: 5.1577 - Val Loss: 5.0981\n",
      "Epoch [854/2000] - Train Loss: 5.2411 - Val Loss: 4.9200\n",
      "Epoch [855/2000] - Train Loss: 5.1825 - Val Loss: 5.0816\n",
      "Epoch [856/2000] - Train Loss: 5.1835 - Val Loss: 5.0734\n",
      "Epoch [857/2000] - Train Loss: 5.2258 - Val Loss: 4.9718\n",
      "Epoch [858/2000] - Train Loss: 5.1255 - Val Loss: 4.9603\n",
      "Epoch [859/2000] - Train Loss: 5.1710 - Val Loss: 4.9710\n",
      "Epoch [860/2000] - Train Loss: 5.2156 - Val Loss: 5.2114\n",
      "Epoch [861/2000] - Train Loss: 5.2424 - Val Loss: 5.1090\n",
      "Epoch [862/2000] - Train Loss: 5.1790 - Val Loss: 5.0347\n",
      "Epoch [863/2000] - Train Loss: 5.1727 - Val Loss: 4.8582\n",
      "Epoch [864/2000] - Train Loss: 5.1724 - Val Loss: 5.0824\n",
      "Epoch [865/2000] - Train Loss: 5.1839 - Val Loss: 4.9795\n",
      "Epoch [866/2000] - Train Loss: 5.1698 - Val Loss: 5.0147\n",
      "Epoch [867/2000] - Train Loss: 5.2489 - Val Loss: 4.8724\n",
      "Epoch [868/2000] - Train Loss: 5.1502 - Val Loss: 5.1088\n",
      "Epoch [869/2000] - Train Loss: 5.1850 - Val Loss: 5.1805\n",
      "Epoch [870/2000] - Train Loss: 5.1665 - Val Loss: 4.9552\n",
      "Epoch [871/2000] - Train Loss: 5.1788 - Val Loss: 4.9023\n",
      "Epoch [872/2000] - Train Loss: 5.1047 - Val Loss: 4.9941\n",
      "Epoch [873/2000] - Train Loss: 5.2586 - Val Loss: 5.0961\n",
      "Epoch [874/2000] - Train Loss: 5.2448 - Val Loss: 5.0736\n",
      "Epoch [875/2000] - Train Loss: 5.1466 - Val Loss: 4.9044\n",
      "Epoch [876/2000] - Train Loss: 5.2014 - Val Loss: 4.7630\n",
      "Epoch [877/2000] - Train Loss: 5.0614 - Val Loss: 5.0874\n",
      "Epoch [878/2000] - Train Loss: 5.2318 - Val Loss: 4.8067\n",
      "Epoch [879/2000] - Train Loss: 5.1813 - Val Loss: 5.0507\n",
      "Epoch [880/2000] - Train Loss: 5.0713 - Val Loss: 4.9059\n",
      "Epoch [881/2000] - Train Loss: 5.1616 - Val Loss: 4.9003\n",
      "Epoch [882/2000] - Train Loss: 5.0874 - Val Loss: 5.0392\n",
      "Epoch [883/2000] - Train Loss: 5.1612 - Val Loss: 4.9129\n",
      "Epoch [884/2000] - Train Loss: 5.0980 - Val Loss: 4.9069\n",
      "Epoch [885/2000] - Train Loss: 5.1913 - Val Loss: 4.9155\n",
      "Epoch [886/2000] - Train Loss: 5.1225 - Val Loss: 4.8255\n",
      "Epoch [887/2000] - Train Loss: 5.1391 - Val Loss: 5.0515\n",
      "Epoch [888/2000] - Train Loss: 5.1272 - Val Loss: 5.1299\n",
      "Epoch [889/2000] - Train Loss: 5.0558 - Val Loss: 4.8811\n",
      "Epoch [890/2000] - Train Loss: 5.1174 - Val Loss: 4.8965\n",
      "Epoch [891/2000] - Train Loss: 5.1696 - Val Loss: 5.0073\n",
      "Epoch [892/2000] - Train Loss: 5.0907 - Val Loss: 4.8863\n",
      "Epoch [893/2000] - Train Loss: 5.1465 - Val Loss: 5.4297\n",
      "Epoch [894/2000] - Train Loss: 5.1993 - Val Loss: 4.8998\n",
      "Epoch [895/2000] - Train Loss: 5.1338 - Val Loss: 4.8747\n",
      "Epoch [896/2000] - Train Loss: 5.1317 - Val Loss: 5.9020\n",
      "Epoch [897/2000] - Train Loss: 5.1143 - Val Loss: 4.9963\n",
      "Epoch [898/2000] - Train Loss: 5.2183 - Val Loss: 4.9035\n",
      "Epoch [899/2000] - Train Loss: 5.1162 - Val Loss: 4.8698\n",
      "Epoch [900/2000] - Train Loss: 5.0811 - Val Loss: 5.1026\n",
      "Epoch [901/2000] - Train Loss: 5.0925 - Val Loss: 5.0262\n",
      "Epoch [902/2000] - Train Loss: 5.0232 - Val Loss: 4.9033\n",
      "Epoch [903/2000] - Train Loss: 5.1686 - Val Loss: 4.8948\n",
      "Epoch [904/2000] - Train Loss: 5.1212 - Val Loss: 5.1913\n",
      "Epoch [905/2000] - Train Loss: 5.0111 - Val Loss: 4.9937\n",
      "Epoch [906/2000] - Train Loss: 5.1365 - Val Loss: 4.9458\n",
      "Epoch [907/2000] - Train Loss: 5.0780 - Val Loss: 4.8898\n",
      "Epoch [908/2000] - Train Loss: 5.1104 - Val Loss: 5.0731\n",
      "Epoch [909/2000] - Train Loss: 5.0542 - Val Loss: 4.9735\n",
      "Epoch [910/2000] - Train Loss: 5.0861 - Val Loss: 4.8973\n",
      "Epoch [911/2000] - Train Loss: 5.1845 - Val Loss: 4.8518\n",
      "Epoch [912/2000] - Train Loss: 5.0726 - Val Loss: 4.8294\n",
      "Epoch [913/2000] - Train Loss: 5.0218 - Val Loss: 5.0652\n",
      "Epoch [914/2000] - Train Loss: 5.0948 - Val Loss: 5.1072\n",
      "Epoch [915/2000] - Train Loss: 5.1142 - Val Loss: 4.9816\n",
      "Epoch [916/2000] - Train Loss: 5.0231 - Val Loss: 4.9005\n",
      "Epoch [917/2000] - Train Loss: 5.0224 - Val Loss: 4.8687\n",
      "Epoch [918/2000] - Train Loss: 4.9894 - Val Loss: 4.9194\n",
      "Epoch [919/2000] - Train Loss: 5.1086 - Val Loss: 4.9811\n",
      "Epoch [920/2000] - Train Loss: 5.1513 - Val Loss: 5.0180\n",
      "Epoch [921/2000] - Train Loss: 5.1535 - Val Loss: 4.8956\n",
      "Epoch [922/2000] - Train Loss: 5.0585 - Val Loss: 4.9123\n",
      "Epoch [923/2000] - Train Loss: 5.0573 - Val Loss: 4.9107\n",
      "Epoch [924/2000] - Train Loss: 5.1163 - Val Loss: 4.8496\n",
      "Epoch [925/2000] - Train Loss: 5.0452 - Val Loss: 5.1210\n",
      "Epoch [926/2000] - Train Loss: 5.0450 - Val Loss: 4.8037\n",
      "Epoch [927/2000] - Train Loss: 5.0999 - Val Loss: 5.1243\n",
      "Epoch [928/2000] - Train Loss: 5.0685 - Val Loss: 4.8672\n",
      "Epoch [929/2000] - Train Loss: 5.1335 - Val Loss: 4.8712\n",
      "Epoch [930/2000] - Train Loss: 5.0597 - Val Loss: 4.7974\n",
      "Epoch [931/2000] - Train Loss: 5.0863 - Val Loss: 5.0322\n",
      "Epoch [932/2000] - Train Loss: 5.1003 - Val Loss: 5.1781\n",
      "Epoch [933/2000] - Train Loss: 5.0050 - Val Loss: 4.9893\n",
      "Epoch [934/2000] - Train Loss: 5.0165 - Val Loss: 5.0354\n",
      "Epoch [935/2000] - Train Loss: 4.9738 - Val Loss: 4.7930\n",
      "Epoch [936/2000] - Train Loss: 5.0923 - Val Loss: 4.9025\n",
      "Epoch [937/2000] - Train Loss: 4.9997 - Val Loss: 4.9335\n",
      "Epoch [938/2000] - Train Loss: 5.0221 - Val Loss: 5.0328\n",
      "Epoch [939/2000] - Train Loss: 5.1468 - Val Loss: 4.9647\n",
      "Epoch [940/2000] - Train Loss: 5.0200 - Val Loss: 4.8122\n",
      "Epoch [941/2000] - Train Loss: 5.0183 - Val Loss: 4.9171\n",
      "Epoch [942/2000] - Train Loss: 5.1199 - Val Loss: 4.9508\n",
      "Epoch [943/2000] - Train Loss: 5.0330 - Val Loss: 5.1001\n",
      "Epoch [944/2000] - Train Loss: 5.1452 - Val Loss: 4.8828\n",
      "Epoch [945/2000] - Train Loss: 5.0312 - Val Loss: 5.3897\n",
      "Epoch [946/2000] - Train Loss: 5.0974 - Val Loss: 4.8921\n",
      "Epoch [947/2000] - Train Loss: 5.0257 - Val Loss: 4.9386\n",
      "Epoch [948/2000] - Train Loss: 5.0056 - Val Loss: 4.8475\n",
      "Epoch [949/2000] - Train Loss: 5.0365 - Val Loss: 4.9162\n",
      "Epoch [950/2000] - Train Loss: 5.0966 - Val Loss: 4.8612\n",
      "Epoch [951/2000] - Train Loss: 5.0032 - Val Loss: 4.9224\n",
      "Epoch [952/2000] - Train Loss: 5.0654 - Val Loss: 4.8424\n",
      "Epoch [953/2000] - Train Loss: 5.0122 - Val Loss: 5.2063\n",
      "Epoch [954/2000] - Train Loss: 5.0418 - Val Loss: 4.9319\n",
      "Epoch [955/2000] - Train Loss: 5.0943 - Val Loss: 5.1195\n",
      "Epoch [956/2000] - Train Loss: 5.1402 - Val Loss: 4.9911\n",
      "Epoch [957/2000] - Train Loss: 5.0220 - Val Loss: 4.8811\n",
      "Epoch [958/2000] - Train Loss: 4.9842 - Val Loss: 4.9955\n",
      "Epoch [959/2000] - Train Loss: 5.0386 - Val Loss: 4.7979\n",
      "Epoch [960/2000] - Train Loss: 5.0440 - Val Loss: 4.9567\n",
      "Epoch [961/2000] - Train Loss: 4.9792 - Val Loss: 4.9562\n",
      "Epoch [962/2000] - Train Loss: 5.0890 - Val Loss: 4.8750\n",
      "Epoch [963/2000] - Train Loss: 5.0480 - Val Loss: 4.8008\n",
      "Epoch [964/2000] - Train Loss: 5.0203 - Val Loss: 4.8533\n",
      "Epoch [965/2000] - Train Loss: 5.0732 - Val Loss: 5.2888\n",
      "Epoch [966/2000] - Train Loss: 5.0213 - Val Loss: 5.0126\n",
      "Epoch [967/2000] - Train Loss: 5.0279 - Val Loss: 4.7155\n",
      "Epoch [968/2000] - Train Loss: 5.0021 - Val Loss: 4.9184\n",
      "Epoch [969/2000] - Train Loss: 4.9706 - Val Loss: 4.8794\n",
      "Epoch [970/2000] - Train Loss: 4.9499 - Val Loss: 5.1203\n",
      "Epoch [971/2000] - Train Loss: 4.9880 - Val Loss: 4.6886\n",
      "Epoch [972/2000] - Train Loss: 4.9821 - Val Loss: 4.7817\n",
      "Epoch [973/2000] - Train Loss: 4.9597 - Val Loss: 4.9491\n",
      "Epoch [974/2000] - Train Loss: 5.0823 - Val Loss: 4.8264\n",
      "Epoch [975/2000] - Train Loss: 4.9239 - Val Loss: 4.7472\n",
      "Epoch [976/2000] - Train Loss: 4.9483 - Val Loss: 4.7076\n",
      "Epoch [977/2000] - Train Loss: 4.9899 - Val Loss: 4.8540\n",
      "Epoch [978/2000] - Train Loss: 4.9920 - Val Loss: 4.9747\n",
      "Epoch [979/2000] - Train Loss: 4.9743 - Val Loss: 4.8770\n",
      "Epoch [980/2000] - Train Loss: 4.9209 - Val Loss: 4.9520\n",
      "Epoch [981/2000] - Train Loss: 4.9409 - Val Loss: 4.8301\n",
      "Epoch [982/2000] - Train Loss: 4.9839 - Val Loss: 4.7729\n",
      "Epoch [983/2000] - Train Loss: 5.0157 - Val Loss: 4.8312\n",
      "Epoch [984/2000] - Train Loss: 4.9938 - Val Loss: 4.8918\n",
      "Epoch [985/2000] - Train Loss: 5.0684 - Val Loss: 4.8772\n",
      "Epoch [986/2000] - Train Loss: 4.9104 - Val Loss: 4.9534\n",
      "Epoch [987/2000] - Train Loss: 4.9400 - Val Loss: 5.0786\n",
      "Epoch [988/2000] - Train Loss: 5.0895 - Val Loss: 5.0105\n",
      "Epoch [989/2000] - Train Loss: 4.9326 - Val Loss: 4.8261\n",
      "Epoch [990/2000] - Train Loss: 4.9719 - Val Loss: 4.7477\n",
      "Epoch [991/2000] - Train Loss: 5.0028 - Val Loss: 4.9149\n",
      "Epoch [992/2000] - Train Loss: 5.0448 - Val Loss: 5.2102\n",
      "Epoch [993/2000] - Train Loss: 4.9882 - Val Loss: 4.7578\n",
      "Epoch [994/2000] - Train Loss: 5.0436 - Val Loss: 4.8832\n",
      "Epoch [995/2000] - Train Loss: 4.9356 - Val Loss: 4.9428\n",
      "Epoch [996/2000] - Train Loss: 5.0226 - Val Loss: 4.8619\n",
      "Epoch [997/2000] - Train Loss: 4.9930 - Val Loss: 4.7606\n",
      "Epoch [998/2000] - Train Loss: 5.0288 - Val Loss: 4.7828\n",
      "Epoch [999/2000] - Train Loss: 4.9881 - Val Loss: 4.9361\n",
      "Epoch [1000/2000] - Train Loss: 5.0154 - Val Loss: 4.9946\n",
      "Epoch [1001/2000] - Train Loss: 5.0153 - Val Loss: 4.8433\n",
      "Epoch [1002/2000] - Train Loss: 4.9440 - Val Loss: 4.7278\n",
      "Epoch [1003/2000] - Train Loss: 4.9060 - Val Loss: 4.7646\n",
      "Epoch [1004/2000] - Train Loss: 4.9419 - Val Loss: 4.8152\n",
      "Epoch [1005/2000] - Train Loss: 4.9568 - Val Loss: 4.7908\n",
      "Epoch [1006/2000] - Train Loss: 4.9661 - Val Loss: 4.8291\n",
      "Epoch [1007/2000] - Train Loss: 4.9655 - Val Loss: 4.9020\n",
      "Epoch [1008/2000] - Train Loss: 5.0514 - Val Loss: 4.8340\n",
      "Epoch [1009/2000] - Train Loss: 4.9466 - Val Loss: 4.9248\n",
      "Epoch [1010/2000] - Train Loss: 4.9264 - Val Loss: 4.7614\n",
      "Epoch [1011/2000] - Train Loss: 4.9666 - Val Loss: 5.1095\n",
      "Epoch [1012/2000] - Train Loss: 5.0198 - Val Loss: 5.1717\n",
      "Epoch [1013/2000] - Train Loss: 4.9917 - Val Loss: 4.7676\n",
      "Epoch [1014/2000] - Train Loss: 4.9278 - Val Loss: 4.8275\n",
      "Epoch [1015/2000] - Train Loss: 4.9445 - Val Loss: 4.8996\n",
      "Epoch [1016/2000] - Train Loss: 4.9699 - Val Loss: 4.8724\n",
      "Epoch [1017/2000] - Train Loss: 4.8842 - Val Loss: 4.8721\n",
      "Epoch [1018/2000] - Train Loss: 4.9053 - Val Loss: 5.0599\n",
      "Epoch [1019/2000] - Train Loss: 5.0360 - Val Loss: 4.8251\n",
      "Epoch [1020/2000] - Train Loss: 4.9495 - Val Loss: 4.8976\n",
      "Epoch [1021/2000] - Train Loss: 4.9953 - Val Loss: 4.9050\n",
      "Epoch [1022/2000] - Train Loss: 4.9632 - Val Loss: 4.7988\n",
      "Epoch [1023/2000] - Train Loss: 4.9967 - Val Loss: 4.7451\n",
      "Epoch [1024/2000] - Train Loss: 4.8741 - Val Loss: 4.8286\n",
      "Epoch [1025/2000] - Train Loss: 4.7963 - Val Loss: 4.8432\n",
      "Epoch [1026/2000] - Train Loss: 5.0642 - Val Loss: 4.9050\n",
      "Epoch [1027/2000] - Train Loss: 4.9257 - Val Loss: 4.8992\n",
      "Epoch [1028/2000] - Train Loss: 4.8743 - Val Loss: 4.7770\n",
      "Epoch [1029/2000] - Train Loss: 4.9896 - Val Loss: 4.7491\n",
      "Epoch [1030/2000] - Train Loss: 4.9413 - Val Loss: 4.8577\n",
      "Epoch [1031/2000] - Train Loss: 4.9931 - Val Loss: 4.8612\n",
      "Epoch [1032/2000] - Train Loss: 5.0229 - Val Loss: 4.8222\n",
      "Epoch [1033/2000] - Train Loss: 4.8790 - Val Loss: 4.7967\n",
      "Epoch [1034/2000] - Train Loss: 4.9281 - Val Loss: 5.2113\n",
      "Epoch [1035/2000] - Train Loss: 4.9258 - Val Loss: 4.7355\n",
      "Epoch [1036/2000] - Train Loss: 4.8359 - Val Loss: 5.2486\n",
      "Epoch [1037/2000] - Train Loss: 4.9425 - Val Loss: 4.7608\n",
      "Epoch [1038/2000] - Train Loss: 4.9160 - Val Loss: 5.3399\n",
      "Epoch [1039/2000] - Train Loss: 4.8780 - Val Loss: 4.7469\n",
      "Epoch [1040/2000] - Train Loss: 4.8958 - Val Loss: 4.7642\n",
      "Epoch [1041/2000] - Train Loss: 4.8683 - Val Loss: 4.9239\n",
      "Epoch [1042/2000] - Train Loss: 5.0093 - Val Loss: 4.8184\n",
      "Epoch [1043/2000] - Train Loss: 4.9274 - Val Loss: 4.7133\n",
      "Epoch [1044/2000] - Train Loss: 4.8444 - Val Loss: 4.8344\n",
      "Epoch [1045/2000] - Train Loss: 4.9403 - Val Loss: 5.0558\n",
      "Epoch [1046/2000] - Train Loss: 4.9148 - Val Loss: 4.6556\n",
      "Epoch [1047/2000] - Train Loss: 4.9100 - Val Loss: 4.8138\n",
      "Epoch [1048/2000] - Train Loss: 4.8927 - Val Loss: 4.9227\n",
      "Epoch [1049/2000] - Train Loss: 4.8889 - Val Loss: 4.8036\n",
      "Epoch [1050/2000] - Train Loss: 4.9100 - Val Loss: 4.7118\n",
      "Epoch [1051/2000] - Train Loss: 4.9407 - Val Loss: 4.9111\n",
      "Epoch [1052/2000] - Train Loss: 4.9693 - Val Loss: 4.7825\n",
      "Epoch [1053/2000] - Train Loss: 4.8605 - Val Loss: 4.8181\n",
      "Epoch [1054/2000] - Train Loss: 4.9051 - Val Loss: 4.7965\n",
      "Epoch [1055/2000] - Train Loss: 4.8721 - Val Loss: 4.6563\n",
      "Epoch [1056/2000] - Train Loss: 4.8144 - Val Loss: 4.7780\n",
      "Epoch [1057/2000] - Train Loss: 4.8904 - Val Loss: 4.7889\n",
      "Epoch [1058/2000] - Train Loss: 4.7702 - Val Loss: 4.6736\n",
      "Epoch [1059/2000] - Train Loss: 4.8313 - Val Loss: 4.6963\n",
      "Epoch [1060/2000] - Train Loss: 4.8802 - Val Loss: 4.9297\n",
      "Epoch [1061/2000] - Train Loss: 4.9114 - Val Loss: 4.7127\n",
      "Epoch [1062/2000] - Train Loss: 4.8954 - Val Loss: 4.8777\n",
      "Epoch [1063/2000] - Train Loss: 4.9144 - Val Loss: 5.2175\n",
      "Epoch [1064/2000] - Train Loss: 4.8900 - Val Loss: 4.8152\n",
      "Epoch [1065/2000] - Train Loss: 4.8624 - Val Loss: 5.0675\n",
      "Epoch [1066/2000] - Train Loss: 4.9171 - Val Loss: 4.7637\n",
      "Epoch [1067/2000] - Train Loss: 4.8953 - Val Loss: 4.8502\n",
      "Epoch [1068/2000] - Train Loss: 4.8218 - Val Loss: 4.7895\n",
      "Epoch [1069/2000] - Train Loss: 4.9444 - Val Loss: 4.8324\n",
      "Epoch [1070/2000] - Train Loss: 4.8432 - Val Loss: 4.8054\n",
      "Epoch [1071/2000] - Train Loss: 4.8875 - Val Loss: 4.7384\n",
      "Epoch [1072/2000] - Train Loss: 4.8132 - Val Loss: 4.6881\n",
      "Epoch [1073/2000] - Train Loss: 4.8463 - Val Loss: 4.7274\n",
      "Epoch [1074/2000] - Train Loss: 4.8542 - Val Loss: 4.7725\n",
      "Epoch [1075/2000] - Train Loss: 4.9139 - Val Loss: 4.9836\n",
      "Epoch [1076/2000] - Train Loss: 4.8971 - Val Loss: 5.0747\n",
      "Epoch [1077/2000] - Train Loss: 4.8985 - Val Loss: 4.7153\n",
      "Epoch [1078/2000] - Train Loss: 4.8526 - Val Loss: 4.7434\n",
      "Epoch [1079/2000] - Train Loss: 4.8164 - Val Loss: 4.7511\n",
      "Epoch [1080/2000] - Train Loss: 4.7574 - Val Loss: 4.7182\n",
      "Epoch [1081/2000] - Train Loss: 4.8462 - Val Loss: 5.0933\n",
      "Epoch [1082/2000] - Train Loss: 4.8208 - Val Loss: 5.0088\n",
      "Epoch [1083/2000] - Train Loss: 4.8804 - Val Loss: 4.7710\n",
      "Epoch [1084/2000] - Train Loss: 4.7907 - Val Loss: 4.7455\n",
      "Epoch [1085/2000] - Train Loss: 4.8525 - Val Loss: 4.9037\n",
      "Epoch [1086/2000] - Train Loss: 4.7829 - Val Loss: 4.7716\n",
      "Epoch [1087/2000] - Train Loss: 4.8625 - Val Loss: 5.0443\n",
      "Epoch [1088/2000] - Train Loss: 4.8194 - Val Loss: 4.6892\n",
      "Epoch [1089/2000] - Train Loss: 4.8773 - Val Loss: 4.7982\n",
      "Epoch [1090/2000] - Train Loss: 4.8440 - Val Loss: 4.8910\n",
      "Epoch [1091/2000] - Train Loss: 4.8550 - Val Loss: 4.8042\n",
      "Epoch [1092/2000] - Train Loss: 4.8818 - Val Loss: 4.7263\n",
      "Epoch [1093/2000] - Train Loss: 4.8574 - Val Loss: 4.7193\n",
      "Epoch [1094/2000] - Train Loss: 4.7723 - Val Loss: 4.6311\n",
      "Epoch [1095/2000] - Train Loss: 4.7809 - Val Loss: 4.8748\n",
      "Epoch [1096/2000] - Train Loss: 4.8221 - Val Loss: 4.7061\n",
      "Epoch [1097/2000] - Train Loss: 4.7829 - Val Loss: 4.8276\n",
      "Epoch [1098/2000] - Train Loss: 4.9254 - Val Loss: 4.7735\n",
      "Epoch [1099/2000] - Train Loss: 4.8134 - Val Loss: 4.6896\n",
      "Epoch [1100/2000] - Train Loss: 4.7943 - Val Loss: 4.9987\n",
      "Epoch [1101/2000] - Train Loss: 4.7790 - Val Loss: 4.7588\n",
      "Epoch [1102/2000] - Train Loss: 4.7621 - Val Loss: 4.8184\n",
      "Epoch [1103/2000] - Train Loss: 4.7812 - Val Loss: 4.6792\n",
      "Epoch [1104/2000] - Train Loss: 4.7844 - Val Loss: 4.7296\n",
      "Epoch [1105/2000] - Train Loss: 4.8481 - Val Loss: 5.0546\n",
      "Epoch [1106/2000] - Train Loss: 4.8646 - Val Loss: 4.7391\n",
      "Epoch [1107/2000] - Train Loss: 4.8301 - Val Loss: 4.8573\n",
      "Epoch [1108/2000] - Train Loss: 4.7787 - Val Loss: 5.0093\n",
      "Epoch [1109/2000] - Train Loss: 4.7966 - Val Loss: 4.6504\n",
      "Epoch [1110/2000] - Train Loss: 4.8475 - Val Loss: 4.6573\n",
      "Epoch [1111/2000] - Train Loss: 4.8518 - Val Loss: 5.0710\n",
      "Epoch [1112/2000] - Train Loss: 4.8742 - Val Loss: 4.7509\n",
      "Epoch [1113/2000] - Train Loss: 4.8041 - Val Loss: 4.7649\n",
      "Epoch [1114/2000] - Train Loss: 4.8227 - Val Loss: 4.6692\n",
      "Epoch [1115/2000] - Train Loss: 4.8448 - Val Loss: 4.8531\n",
      "Epoch [1116/2000] - Train Loss: 4.7040 - Val Loss: 4.7382\n",
      "Epoch [1117/2000] - Train Loss: 4.7514 - Val Loss: 4.7203\n",
      "Epoch [1118/2000] - Train Loss: 4.8689 - Val Loss: 4.9151\n",
      "Epoch [1119/2000] - Train Loss: 4.8446 - Val Loss: 4.8173\n",
      "Epoch [1120/2000] - Train Loss: 4.7746 - Val Loss: 5.1657\n",
      "Epoch [1121/2000] - Train Loss: 4.9125 - Val Loss: 4.6810\n",
      "Epoch [1122/2000] - Train Loss: 4.8617 - Val Loss: 4.9514\n",
      "Epoch [1123/2000] - Train Loss: 4.7955 - Val Loss: 5.1570\n",
      "Epoch [1124/2000] - Train Loss: 4.8128 - Val Loss: 4.7874\n",
      "Epoch [1125/2000] - Train Loss: 4.9994 - Val Loss: 5.1877\n",
      "Epoch [1126/2000] - Train Loss: 4.8192 - Val Loss: 4.7106\n",
      "Epoch [1127/2000] - Train Loss: 4.7505 - Val Loss: 4.7312\n",
      "Epoch [1128/2000] - Train Loss: 4.6577 - Val Loss: 4.9284\n",
      "Epoch [1129/2000] - Train Loss: 4.7722 - Val Loss: 4.8027\n",
      "Epoch [1130/2000] - Train Loss: 4.7888 - Val Loss: 4.9262\n",
      "Epoch [1131/2000] - Train Loss: 4.7412 - Val Loss: 4.8144\n",
      "Epoch [1132/2000] - Train Loss: 4.7545 - Val Loss: 4.7372\n",
      "Epoch [1133/2000] - Train Loss: 4.7565 - Val Loss: 4.6040\n",
      "Epoch [1134/2000] - Train Loss: 4.7095 - Val Loss: 5.0032\n",
      "Epoch [1135/2000] - Train Loss: 4.8260 - Val Loss: 4.7684\n",
      "Epoch [1136/2000] - Train Loss: 4.8197 - Val Loss: 5.1472\n",
      "Epoch [1137/2000] - Train Loss: 4.8509 - Val Loss: 4.7254\n",
      "Epoch [1138/2000] - Train Loss: 4.8050 - Val Loss: 4.9773\n",
      "Epoch [1139/2000] - Train Loss: 4.6945 - Val Loss: 4.5634\n",
      "Epoch [1140/2000] - Train Loss: 4.7685 - Val Loss: 4.7499\n",
      "Epoch [1141/2000] - Train Loss: 4.7754 - Val Loss: 4.7929\n",
      "Epoch [1142/2000] - Train Loss: 4.7932 - Val Loss: 4.6965\n",
      "Epoch [1143/2000] - Train Loss: 4.7017 - Val Loss: 4.7738\n",
      "Epoch [1144/2000] - Train Loss: 4.7104 - Val Loss: 4.7963\n",
      "Epoch [1145/2000] - Train Loss: 4.7665 - Val Loss: 4.8810\n",
      "Epoch [1146/2000] - Train Loss: 4.8430 - Val Loss: 4.5917\n",
      "Epoch [1147/2000] - Train Loss: 4.6717 - Val Loss: 4.7311\n",
      "Epoch [1148/2000] - Train Loss: 4.6961 - Val Loss: 4.6461\n",
      "Epoch [1149/2000] - Train Loss: 4.8040 - Val Loss: 4.6808\n",
      "Epoch [1150/2000] - Train Loss: 4.7147 - Val Loss: 4.9948\n",
      "Epoch [1151/2000] - Train Loss: 4.7677 - Val Loss: 4.7483\n",
      "Epoch [1152/2000] - Train Loss: 4.8721 - Val Loss: 4.7788\n",
      "Epoch [1153/2000] - Train Loss: 4.7937 - Val Loss: 4.7623\n",
      "Epoch [1154/2000] - Train Loss: 4.7158 - Val Loss: 4.9147\n",
      "Epoch [1155/2000] - Train Loss: 4.7308 - Val Loss: 4.7111\n",
      "Epoch [1156/2000] - Train Loss: 4.8195 - Val Loss: 4.6884\n",
      "Epoch [1157/2000] - Train Loss: 4.7475 - Val Loss: 4.8803\n",
      "Epoch [1158/2000] - Train Loss: 4.7871 - Val Loss: 4.7275\n",
      "Epoch [1159/2000] - Train Loss: 4.7786 - Val Loss: 4.5626\n",
      "Epoch [1160/2000] - Train Loss: 4.6944 - Val Loss: 4.6325\n",
      "Epoch [1161/2000] - Train Loss: 4.7554 - Val Loss: 4.7457\n",
      "Epoch [1162/2000] - Train Loss: 4.7566 - Val Loss: 4.7802\n",
      "Epoch [1163/2000] - Train Loss: 4.7124 - Val Loss: 4.7231\n",
      "Epoch [1164/2000] - Train Loss: 4.6957 - Val Loss: 4.7474\n",
      "Epoch [1165/2000] - Train Loss: 4.7524 - Val Loss: 4.7334\n",
      "Epoch [1166/2000] - Train Loss: 4.7565 - Val Loss: 4.5994\n",
      "Epoch [1167/2000] - Train Loss: 4.7890 - Val Loss: 4.7319\n",
      "Epoch [1168/2000] - Train Loss: 4.6689 - Val Loss: 4.6852\n",
      "Epoch [1169/2000] - Train Loss: 4.6530 - Val Loss: 4.9483\n",
      "Epoch [1170/2000] - Train Loss: 4.7535 - Val Loss: 4.8044\n",
      "Epoch [1171/2000] - Train Loss: 4.7360 - Val Loss: 4.7102\n",
      "Epoch [1172/2000] - Train Loss: 4.7479 - Val Loss: 4.7242\n",
      "Epoch [1173/2000] - Train Loss: 4.6988 - Val Loss: 4.6133\n",
      "Epoch [1174/2000] - Train Loss: 4.6363 - Val Loss: 4.8229\n",
      "Epoch [1175/2000] - Train Loss: 4.6878 - Val Loss: 4.6698\n",
      "Epoch [1176/2000] - Train Loss: 4.6828 - Val Loss: 4.7425\n",
      "Epoch [1177/2000] - Train Loss: 4.7406 - Val Loss: 4.6692\n",
      "Epoch [1178/2000] - Train Loss: 4.7534 - Val Loss: 4.8945\n",
      "Epoch [1179/2000] - Train Loss: 4.6878 - Val Loss: 4.5954\n",
      "Epoch [1180/2000] - Train Loss: 4.7027 - Val Loss: 4.6136\n",
      "Epoch [1181/2000] - Train Loss: 4.7359 - Val Loss: 4.6132\n",
      "Epoch [1182/2000] - Train Loss: 4.7370 - Val Loss: 4.6290\n",
      "Epoch [1183/2000] - Train Loss: 4.7700 - Val Loss: 4.8830\n",
      "Epoch [1184/2000] - Train Loss: 4.7905 - Val Loss: 4.6883\n",
      "Epoch [1185/2000] - Train Loss: 4.7420 - Val Loss: 4.8156\n",
      "Epoch [1186/2000] - Train Loss: 4.6920 - Val Loss: 4.5480\n",
      "Epoch [1187/2000] - Train Loss: 4.6030 - Val Loss: 4.8582\n",
      "Epoch [1188/2000] - Train Loss: 4.6821 - Val Loss: 4.8043\n",
      "Epoch [1189/2000] - Train Loss: 4.6995 - Val Loss: 4.7840\n",
      "Epoch [1190/2000] - Train Loss: 4.7252 - Val Loss: 4.7139\n",
      "Epoch [1191/2000] - Train Loss: 4.7383 - Val Loss: 4.7232\n",
      "Epoch [1192/2000] - Train Loss: 4.7784 - Val Loss: 4.6714\n",
      "Epoch [1193/2000] - Train Loss: 4.8175 - Val Loss: 4.7029\n",
      "Epoch [1194/2000] - Train Loss: 4.6667 - Val Loss: 5.2239\n",
      "Epoch [1195/2000] - Train Loss: 4.7573 - Val Loss: 4.8348\n",
      "Epoch [1196/2000] - Train Loss: 4.9624 - Val Loss: 4.7545\n",
      "Epoch [1197/2000] - Train Loss: 4.7746 - Val Loss: 4.7490\n",
      "Epoch [1198/2000] - Train Loss: 4.6917 - Val Loss: 4.7932\n",
      "Epoch [1199/2000] - Train Loss: 4.6781 - Val Loss: 4.7583\n",
      "Epoch [1200/2000] - Train Loss: 4.7735 - Val Loss: 4.8684\n",
      "Epoch [1201/2000] - Train Loss: 4.7260 - Val Loss: 4.8068\n",
      "Epoch [1202/2000] - Train Loss: 4.7570 - Val Loss: 4.7286\n",
      "Epoch [1203/2000] - Train Loss: 4.6675 - Val Loss: 4.5863\n",
      "Epoch [1204/2000] - Train Loss: 4.8012 - Val Loss: 4.7031\n",
      "Epoch [1205/2000] - Train Loss: 4.7747 - Val Loss: 4.7204\n",
      "Epoch [1206/2000] - Train Loss: 4.6111 - Val Loss: 4.7243\n",
      "Epoch [1207/2000] - Train Loss: 4.7279 - Val Loss: 4.8874\n",
      "Epoch [1208/2000] - Train Loss: 4.6943 - Val Loss: 4.6111\n",
      "Epoch [1209/2000] - Train Loss: 4.7669 - Val Loss: 4.7168\n",
      "Epoch [1210/2000] - Train Loss: 4.6151 - Val Loss: 4.6466\n",
      "Epoch [1211/2000] - Train Loss: 4.6802 - Val Loss: 4.6184\n",
      "Epoch [1212/2000] - Train Loss: 4.7045 - Val Loss: 4.6621\n",
      "Epoch [1213/2000] - Train Loss: 4.6362 - Val Loss: 4.7819\n",
      "Epoch [1214/2000] - Train Loss: 4.7249 - Val Loss: 4.5763\n",
      "Epoch [1215/2000] - Train Loss: 4.7796 - Val Loss: 4.7245\n",
      "Epoch [1216/2000] - Train Loss: 4.7353 - Val Loss: 4.6413\n",
      "Epoch [1217/2000] - Train Loss: 4.6908 - Val Loss: 4.5859\n",
      "Epoch [1218/2000] - Train Loss: 4.6646 - Val Loss: 4.8708\n",
      "Epoch [1219/2000] - Train Loss: 4.7006 - Val Loss: 4.6571\n",
      "Epoch [1220/2000] - Train Loss: 4.6486 - Val Loss: 4.7007\n",
      "Epoch [1221/2000] - Train Loss: 4.7027 - Val Loss: 4.6167\n",
      "Epoch [1222/2000] - Train Loss: 4.7339 - Val Loss: 4.8059\n",
      "Epoch [1223/2000] - Train Loss: 4.6849 - Val Loss: 4.7211\n",
      "Epoch [1224/2000] - Train Loss: 4.7444 - Val Loss: 4.6511\n",
      "Epoch [1225/2000] - Train Loss: 4.6773 - Val Loss: 4.6634\n",
      "Epoch [1226/2000] - Train Loss: 4.6560 - Val Loss: 4.7489\n",
      "Epoch [1227/2000] - Train Loss: 4.6764 - Val Loss: 4.6987\n",
      "Epoch [1228/2000] - Train Loss: 4.8342 - Val Loss: 4.7765\n",
      "Epoch [1229/2000] - Train Loss: 4.6795 - Val Loss: 4.5657\n",
      "Epoch [1230/2000] - Train Loss: 4.7192 - Val Loss: 4.5773\n",
      "Epoch [1231/2000] - Train Loss: 4.6155 - Val Loss: 4.7466\n",
      "Epoch [1232/2000] - Train Loss: 4.7092 - Val Loss: 5.1706\n",
      "Epoch [1233/2000] - Train Loss: 4.7153 - Val Loss: 4.8637\n",
      "Epoch [1234/2000] - Train Loss: 4.5996 - Val Loss: 4.7198\n",
      "Epoch [1235/2000] - Train Loss: 4.7317 - Val Loss: 4.7602\n",
      "Epoch [1236/2000] - Train Loss: 4.7232 - Val Loss: 4.5359\n",
      "Epoch [1237/2000] - Train Loss: 4.5894 - Val Loss: 4.7123\n",
      "Epoch [1238/2000] - Train Loss: 4.7358 - Val Loss: 4.6270\n",
      "Epoch [1239/2000] - Train Loss: 4.6776 - Val Loss: 4.7475\n",
      "Epoch [1240/2000] - Train Loss: 4.6071 - Val Loss: 4.7163\n",
      "Epoch [1241/2000] - Train Loss: 4.5879 - Val Loss: 4.4640\n",
      "Epoch [1242/2000] - Train Loss: 4.6656 - Val Loss: 4.5688\n",
      "Epoch [1243/2000] - Train Loss: 4.5937 - Val Loss: 4.5808\n",
      "Epoch [1244/2000] - Train Loss: 4.6550 - Val Loss: 4.6904\n",
      "Epoch [1245/2000] - Train Loss: 4.6716 - Val Loss: 4.6276\n",
      "Epoch [1246/2000] - Train Loss: 4.6779 - Val Loss: 4.7677\n",
      "Epoch [1247/2000] - Train Loss: 4.6636 - Val Loss: 4.7428\n",
      "Epoch [1248/2000] - Train Loss: 4.7140 - Val Loss: 4.6260\n",
      "Epoch [1249/2000] - Train Loss: 4.6125 - Val Loss: 4.9316\n",
      "Epoch [1250/2000] - Train Loss: 4.6844 - Val Loss: 4.7476\n",
      "Epoch [1251/2000] - Train Loss: 4.6346 - Val Loss: 4.6658\n",
      "Epoch [1252/2000] - Train Loss: 4.6690 - Val Loss: 4.7385\n",
      "Epoch [1253/2000] - Train Loss: 4.6733 - Val Loss: 4.6908\n",
      "Epoch [1254/2000] - Train Loss: 4.7110 - Val Loss: 4.7832\n",
      "Epoch [1255/2000] - Train Loss: 4.6625 - Val Loss: 4.7313\n",
      "Epoch [1256/2000] - Train Loss: 4.5905 - Val Loss: 4.7347\n",
      "Epoch [1257/2000] - Train Loss: 4.7353 - Val Loss: 4.7222\n",
      "Epoch [1258/2000] - Train Loss: 4.6175 - Val Loss: 4.6619\n",
      "Epoch [1259/2000] - Train Loss: 4.6868 - Val Loss: 4.6503\n",
      "Epoch [1260/2000] - Train Loss: 4.6513 - Val Loss: 4.6143\n",
      "Epoch [1261/2000] - Train Loss: 4.6075 - Val Loss: 4.6250\n",
      "Epoch [1262/2000] - Train Loss: 4.6276 - Val Loss: 4.6411\n",
      "Epoch [1263/2000] - Train Loss: 4.5891 - Val Loss: 4.6014\n",
      "Epoch [1264/2000] - Train Loss: 4.7689 - Val Loss: 4.7137\n",
      "Epoch [1265/2000] - Train Loss: 4.6849 - Val Loss: 4.8624\n",
      "Epoch [1266/2000] - Train Loss: 4.7316 - Val Loss: 4.6745\n",
      "Epoch [1267/2000] - Train Loss: 4.6455 - Val Loss: 4.6737\n",
      "Epoch [1268/2000] - Train Loss: 4.6790 - Val Loss: 4.7857\n",
      "Epoch [1269/2000] - Train Loss: 4.6981 - Val Loss: 4.7349\n",
      "Epoch [1270/2000] - Train Loss: 4.5845 - Val Loss: 4.7268\n",
      "Epoch [1271/2000] - Train Loss: 4.6425 - Val Loss: 4.5248\n",
      "Epoch [1272/2000] - Train Loss: 4.6107 - Val Loss: 4.6794\n",
      "Epoch [1273/2000] - Train Loss: 4.5963 - Val Loss: 4.6628\n",
      "Epoch [1274/2000] - Train Loss: 4.6318 - Val Loss: 4.6314\n",
      "Epoch [1275/2000] - Train Loss: 4.6552 - Val Loss: 4.6359\n",
      "Epoch [1276/2000] - Train Loss: 4.6347 - Val Loss: 4.6131\n",
      "Epoch [1277/2000] - Train Loss: 4.6689 - Val Loss: 4.5445\n",
      "Epoch [1278/2000] - Train Loss: 4.6284 - Val Loss: 4.6255\n",
      "Epoch [1279/2000] - Train Loss: 4.6638 - Val Loss: 4.7566\n",
      "Epoch [1280/2000] - Train Loss: 4.6671 - Val Loss: 4.5733\n",
      "Epoch [1281/2000] - Train Loss: 4.6527 - Val Loss: 4.8069\n",
      "Epoch [1282/2000] - Train Loss: 4.6197 - Val Loss: 4.7247\n",
      "Epoch [1283/2000] - Train Loss: 4.6386 - Val Loss: 5.2871\n",
      "Epoch [1284/2000] - Train Loss: 4.6158 - Val Loss: 4.6583\n",
      "Epoch [1285/2000] - Train Loss: 4.6802 - Val Loss: 4.5800\n",
      "Epoch [1286/2000] - Train Loss: 4.5257 - Val Loss: 4.6640\n",
      "Epoch [1287/2000] - Train Loss: 4.6286 - Val Loss: 4.7599\n",
      "Epoch [1288/2000] - Train Loss: 4.5788 - Val Loss: 4.6063\n",
      "Epoch [1289/2000] - Train Loss: 4.6790 - Val Loss: 4.6564\n",
      "Epoch [1290/2000] - Train Loss: 4.6188 - Val Loss: 4.6575\n",
      "Epoch [1291/2000] - Train Loss: 4.6609 - Val Loss: 4.6644\n",
      "Epoch [1292/2000] - Train Loss: 4.6472 - Val Loss: 4.7028\n",
      "Epoch [1293/2000] - Train Loss: 4.6062 - Val Loss: 4.7550\n",
      "Epoch [1294/2000] - Train Loss: 4.5724 - Val Loss: 4.6133\n",
      "Epoch [1295/2000] - Train Loss: 4.7004 - Val Loss: 4.6905\n",
      "Epoch [1296/2000] - Train Loss: 4.6026 - Val Loss: 4.6984\n",
      "Epoch [1297/2000] - Train Loss: 4.5613 - Val Loss: 4.6530\n",
      "Epoch [1298/2000] - Train Loss: 4.6235 - Val Loss: 4.6464\n",
      "Epoch [1299/2000] - Train Loss: 4.6236 - Val Loss: 4.5709\n",
      "Epoch [1300/2000] - Train Loss: 4.5874 - Val Loss: 4.8969\n",
      "Epoch [1301/2000] - Train Loss: 4.6318 - Val Loss: 4.6769\n",
      "Epoch [1302/2000] - Train Loss: 4.7074 - Val Loss: 4.6107\n",
      "Epoch [1303/2000] - Train Loss: 4.5518 - Val Loss: 4.6675\n",
      "Epoch [1304/2000] - Train Loss: 4.6190 - Val Loss: 4.6086\n",
      "Epoch [1305/2000] - Train Loss: 4.5907 - Val Loss: 4.6268\n",
      "Epoch [1306/2000] - Train Loss: 4.5696 - Val Loss: 4.6679\n",
      "Epoch [1307/2000] - Train Loss: 4.6529 - Val Loss: 4.6807\n",
      "Epoch [1308/2000] - Train Loss: 4.6523 - Val Loss: 4.7651\n",
      "Epoch [1309/2000] - Train Loss: 4.5531 - Val Loss: 4.5716\n",
      "Epoch [1310/2000] - Train Loss: 4.6040 - Val Loss: 4.9459\n",
      "Epoch [1311/2000] - Train Loss: 4.7189 - Val Loss: 4.6829\n",
      "Epoch [1312/2000] - Train Loss: 4.6125 - Val Loss: 4.7296\n",
      "Epoch [1313/2000] - Train Loss: 4.7107 - Val Loss: 4.6740\n",
      "Epoch [1314/2000] - Train Loss: 4.6359 - Val Loss: 4.6936\n",
      "Epoch [1315/2000] - Train Loss: 4.6222 - Val Loss: 4.6251\n",
      "Epoch [1316/2000] - Train Loss: 4.6398 - Val Loss: 4.6662\n",
      "Epoch [1317/2000] - Train Loss: 4.6474 - Val Loss: 4.5765\n",
      "Epoch [1318/2000] - Train Loss: 4.5622 - Val Loss: 4.6342\n",
      "Epoch [1319/2000] - Train Loss: 4.6166 - Val Loss: 4.6464\n",
      "Epoch [1320/2000] - Train Loss: 4.5912 - Val Loss: 4.8905\n",
      "Epoch [1321/2000] - Train Loss: 4.5572 - Val Loss: 4.6377\n",
      "Epoch [1322/2000] - Train Loss: 4.7096 - Val Loss: 4.6997\n",
      "Epoch [1323/2000] - Train Loss: 4.5958 - Val Loss: 4.8401\n",
      "Epoch [1324/2000] - Train Loss: 4.7624 - Val Loss: 4.6947\n",
      "Epoch [1325/2000] - Train Loss: 4.5609 - Val Loss: 4.5514\n",
      "Epoch [1326/2000] - Train Loss: 4.5584 - Val Loss: 4.5885\n",
      "Epoch [1327/2000] - Train Loss: 4.5288 - Val Loss: 4.7694\n",
      "Epoch [1328/2000] - Train Loss: 4.5480 - Val Loss: 4.5143\n",
      "Epoch [1329/2000] - Train Loss: 4.5885 - Val Loss: 4.5918\n",
      "Epoch [1330/2000] - Train Loss: 4.5638 - Val Loss: 4.8057\n",
      "Epoch [1331/2000] - Train Loss: 4.5422 - Val Loss: 4.5594\n",
      "Epoch [1332/2000] - Train Loss: 4.6668 - Val Loss: 4.7433\n",
      "Epoch [1333/2000] - Train Loss: 4.6070 - Val Loss: 4.5364\n",
      "Epoch [1334/2000] - Train Loss: 4.5752 - Val Loss: 4.6411\n",
      "Epoch [1335/2000] - Train Loss: 4.5303 - Val Loss: 4.5754\n",
      "Epoch [1336/2000] - Train Loss: 4.5474 - Val Loss: 4.6697\n",
      "Epoch [1337/2000] - Train Loss: 4.6020 - Val Loss: 4.6834\n",
      "Epoch [1338/2000] - Train Loss: 4.6093 - Val Loss: 4.6535\n",
      "Epoch [1339/2000] - Train Loss: 4.6013 - Val Loss: 4.6890\n",
      "Epoch [1340/2000] - Train Loss: 4.5966 - Val Loss: 4.5169\n",
      "Epoch [1341/2000] - Train Loss: 4.5621 - Val Loss: 4.6250\n",
      "Epoch [1342/2000] - Train Loss: 4.5632 - Val Loss: 4.6975\n",
      "Epoch [1343/2000] - Train Loss: 4.5237 - Val Loss: 4.5022\n",
      "Epoch [1344/2000] - Train Loss: 4.6675 - Val Loss: 4.7843\n",
      "Epoch [1345/2000] - Train Loss: 4.5702 - Val Loss: 4.7394\n",
      "Epoch [1346/2000] - Train Loss: 4.5706 - Val Loss: 4.6495\n",
      "Epoch [1347/2000] - Train Loss: 4.5445 - Val Loss: 4.7076\n",
      "Epoch [1348/2000] - Train Loss: 4.7991 - Val Loss: 4.6491\n",
      "Epoch [1349/2000] - Train Loss: 4.6468 - Val Loss: 4.5511\n",
      "Epoch [1350/2000] - Train Loss: 4.5455 - Val Loss: 4.6366\n",
      "Epoch [1351/2000] - Train Loss: 4.5699 - Val Loss: 4.5605\n",
      "Epoch [1352/2000] - Train Loss: 4.6567 - Val Loss: 4.6432\n",
      "Epoch [1353/2000] - Train Loss: 4.6001 - Val Loss: 4.5008\n",
      "Epoch [1354/2000] - Train Loss: 4.6237 - Val Loss: 4.5451\n",
      "Epoch [1355/2000] - Train Loss: 4.5404 - Val Loss: 4.7585\n",
      "Epoch [1356/2000] - Train Loss: 4.4909 - Val Loss: 4.6078\n",
      "Epoch [1357/2000] - Train Loss: 4.6032 - Val Loss: 4.9438\n",
      "Epoch [1358/2000] - Train Loss: 4.5270 - Val Loss: 4.5729\n",
      "Epoch [1359/2000] - Train Loss: 4.6365 - Val Loss: 4.8247\n",
      "Epoch [1360/2000] - Train Loss: 4.5217 - Val Loss: 4.5585\n",
      "Epoch [1361/2000] - Train Loss: 4.5952 - Val Loss: 4.6556\n",
      "Epoch [1362/2000] - Train Loss: 4.5156 - Val Loss: 4.6764\n",
      "Epoch [1363/2000] - Train Loss: 4.5217 - Val Loss: 4.3918\n",
      "Epoch [1364/2000] - Train Loss: 4.5604 - Val Loss: 4.5498\n",
      "Epoch [1365/2000] - Train Loss: 4.5581 - Val Loss: 4.6299\n",
      "Epoch [1366/2000] - Train Loss: 4.5505 - Val Loss: 4.7485\n",
      "Epoch [1367/2000] - Train Loss: 4.6035 - Val Loss: 4.9379\n",
      "Epoch [1368/2000] - Train Loss: 4.5505 - Val Loss: 4.9507\n",
      "Epoch [1369/2000] - Train Loss: 4.5956 - Val Loss: 4.7529\n",
      "Epoch [1370/2000] - Train Loss: 4.5852 - Val Loss: 4.6853\n",
      "Epoch [1371/2000] - Train Loss: 4.5673 - Val Loss: 4.6426\n",
      "Epoch [1372/2000] - Train Loss: 4.5551 - Val Loss: 4.6552\n",
      "Epoch [1373/2000] - Train Loss: 4.5673 - Val Loss: 4.6876\n",
      "Epoch [1374/2000] - Train Loss: 4.5213 - Val Loss: 4.5395\n",
      "Epoch [1375/2000] - Train Loss: 4.5894 - Val Loss: 4.7069\n",
      "Epoch [1376/2000] - Train Loss: 4.5958 - Val Loss: 4.6253\n",
      "Epoch [1377/2000] - Train Loss: 4.5205 - Val Loss: 4.8408\n",
      "Epoch [1378/2000] - Train Loss: 4.6771 - Val Loss: 4.7993\n",
      "Epoch [1379/2000] - Train Loss: 4.5430 - Val Loss: 4.5152\n",
      "Epoch [1380/2000] - Train Loss: 4.5026 - Val Loss: 4.5971\n",
      "Epoch [1381/2000] - Train Loss: 4.4844 - Val Loss: 4.9118\n",
      "Epoch [1382/2000] - Train Loss: 4.6257 - Val Loss: 4.8314\n",
      "Epoch [1383/2000] - Train Loss: 4.5468 - Val Loss: 4.5707\n",
      "Epoch [1384/2000] - Train Loss: 4.5148 - Val Loss: 4.6280\n",
      "Epoch [1385/2000] - Train Loss: 4.5202 - Val Loss: 4.7347\n",
      "Epoch [1386/2000] - Train Loss: 4.4542 - Val Loss: 4.6175\n",
      "Epoch [1387/2000] - Train Loss: 4.5281 - Val Loss: 4.6381\n",
      "Epoch [1388/2000] - Train Loss: 4.5983 - Val Loss: 4.5577\n",
      "Epoch [1389/2000] - Train Loss: 4.4799 - Val Loss: 4.6551\n",
      "Epoch [1390/2000] - Train Loss: 4.5726 - Val Loss: 4.9447\n",
      "Epoch [1391/2000] - Train Loss: 4.5793 - Val Loss: 4.6676\n",
      "Epoch [1392/2000] - Train Loss: 4.5138 - Val Loss: 4.6649\n",
      "Epoch [1393/2000] - Train Loss: 4.4831 - Val Loss: 4.6668\n",
      "Epoch [1394/2000] - Train Loss: 4.5764 - Val Loss: 4.7088\n",
      "Epoch [1395/2000] - Train Loss: 4.6712 - Val Loss: 4.5668\n",
      "Epoch [1396/2000] - Train Loss: 4.5694 - Val Loss: 4.5811\n",
      "Epoch [1397/2000] - Train Loss: 4.5024 - Val Loss: 4.5956\n",
      "Epoch [1398/2000] - Train Loss: 4.5103 - Val Loss: 4.5698\n",
      "Epoch [1399/2000] - Train Loss: 4.4859 - Val Loss: 4.5942\n",
      "Epoch [1400/2000] - Train Loss: 4.5327 - Val Loss: 4.7858\n",
      "Epoch [1401/2000] - Train Loss: 4.5218 - Val Loss: 4.5904\n",
      "Epoch [1402/2000] - Train Loss: 4.5203 - Val Loss: 4.8988\n",
      "Epoch [1403/2000] - Train Loss: 4.5791 - Val Loss: 4.5475\n",
      "Epoch [1404/2000] - Train Loss: 4.5182 - Val Loss: 4.4879\n",
      "Epoch [1405/2000] - Train Loss: 4.5674 - Val Loss: 4.9020\n",
      "Epoch [1406/2000] - Train Loss: 4.5702 - Val Loss: 4.5105\n",
      "Epoch [1407/2000] - Train Loss: 4.5004 - Val Loss: 4.7919\n",
      "Epoch [1408/2000] - Train Loss: 4.5583 - Val Loss: 4.6580\n",
      "Epoch [1409/2000] - Train Loss: 4.6048 - Val Loss: 4.6018\n",
      "Epoch [1410/2000] - Train Loss: 4.5469 - Val Loss: 4.6437\n",
      "Epoch [1411/2000] - Train Loss: 4.5502 - Val Loss: 4.6403\n",
      "Epoch [1412/2000] - Train Loss: 4.5674 - Val Loss: 4.6723\n",
      "Epoch [1413/2000] - Train Loss: 4.5771 - Val Loss: 4.7231\n",
      "Epoch [1414/2000] - Train Loss: 4.5373 - Val Loss: 4.6266\n",
      "Epoch [1415/2000] - Train Loss: 4.4841 - Val Loss: 4.4771\n",
      "Epoch [1416/2000] - Train Loss: 4.4993 - Val Loss: 4.6764\n",
      "Epoch [1417/2000] - Train Loss: 4.6288 - Val Loss: 4.5518\n",
      "Epoch [1418/2000] - Train Loss: 4.5363 - Val Loss: 4.7236\n",
      "Epoch [1419/2000] - Train Loss: 4.4929 - Val Loss: 4.9032\n",
      "Epoch [1420/2000] - Train Loss: 4.5716 - Val Loss: 4.9812\n",
      "Epoch [1421/2000] - Train Loss: 4.5394 - Val Loss: 4.5967\n",
      "Epoch [1422/2000] - Train Loss: 4.5773 - Val Loss: 4.8122\n",
      "Epoch [1423/2000] - Train Loss: 4.5909 - Val Loss: 4.6128\n",
      "Epoch [1424/2000] - Train Loss: 4.4647 - Val Loss: 4.6221\n",
      "Epoch [1425/2000] - Train Loss: 4.4922 - Val Loss: 4.6245\n",
      "Epoch [1426/2000] - Train Loss: 4.5361 - Val Loss: 4.6291\n",
      "Epoch [1427/2000] - Train Loss: 4.5215 - Val Loss: 4.4820\n",
      "Epoch [1428/2000] - Train Loss: 4.5179 - Val Loss: 4.5628\n",
      "Epoch [1429/2000] - Train Loss: 4.4556 - Val Loss: 4.7235\n",
      "Epoch [1430/2000] - Train Loss: 4.5102 - Val Loss: 4.5276\n",
      "Epoch [1431/2000] - Train Loss: 4.5387 - Val Loss: 4.4517\n",
      "Epoch [1432/2000] - Train Loss: 4.4048 - Val Loss: 4.5911\n",
      "Epoch [1433/2000] - Train Loss: 4.4380 - Val Loss: 4.4981\n",
      "Epoch [1434/2000] - Train Loss: 4.5825 - Val Loss: 4.5705\n",
      "Epoch [1435/2000] - Train Loss: 4.4979 - Val Loss: 4.4993\n",
      "Epoch [1436/2000] - Train Loss: 4.4983 - Val Loss: 4.6901\n",
      "Epoch [1437/2000] - Train Loss: 4.4997 - Val Loss: 4.4747\n",
      "Epoch [1438/2000] - Train Loss: 4.5370 - Val Loss: 4.6885\n",
      "Epoch [1439/2000] - Train Loss: 4.4483 - Val Loss: 4.4899\n",
      "Epoch [1440/2000] - Train Loss: 4.4311 - Val Loss: 4.5345\n",
      "Epoch [1441/2000] - Train Loss: 4.4991 - Val Loss: 4.7869\n",
      "Epoch [1442/2000] - Train Loss: 4.6006 - Val Loss: 4.6276\n",
      "Epoch [1443/2000] - Train Loss: 4.5588 - Val Loss: 4.6366\n",
      "Epoch [1444/2000] - Train Loss: 4.5394 - Val Loss: 4.6432\n",
      "Epoch [1445/2000] - Train Loss: 4.5798 - Val Loss: 4.5406\n",
      "Epoch [1446/2000] - Train Loss: 4.4977 - Val Loss: 4.6572\n",
      "Epoch [1447/2000] - Train Loss: 4.5205 - Val Loss: 4.5095\n",
      "Epoch [1448/2000] - Train Loss: 4.5044 - Val Loss: 4.5846\n",
      "Epoch [1449/2000] - Train Loss: 4.4935 - Val Loss: 4.4700\n",
      "Epoch [1450/2000] - Train Loss: 4.4582 - Val Loss: 4.5685\n",
      "Epoch [1451/2000] - Train Loss: 4.5369 - Val Loss: 4.7608\n",
      "Epoch [1452/2000] - Train Loss: 4.4842 - Val Loss: 4.6171\n",
      "Epoch [1453/2000] - Train Loss: 4.5153 - Val Loss: 4.7176\n",
      "Epoch [1454/2000] - Train Loss: 4.4339 - Val Loss: 4.5726\n",
      "Epoch [1455/2000] - Train Loss: 4.5299 - Val Loss: 4.6783\n",
      "Epoch [1456/2000] - Train Loss: 4.4995 - Val Loss: 4.6561\n",
      "Epoch [1457/2000] - Train Loss: 4.4823 - Val Loss: 4.7766\n",
      "Epoch [1458/2000] - Train Loss: 4.5352 - Val Loss: 4.7155\n",
      "Epoch [1459/2000] - Train Loss: 4.5824 - Val Loss: 4.5065\n",
      "Epoch [1460/2000] - Train Loss: 4.5134 - Val Loss: 4.6423\n",
      "Epoch [1461/2000] - Train Loss: 4.4659 - Val Loss: 4.5853\n",
      "Epoch [1462/2000] - Train Loss: 4.5304 - Val Loss: 4.4544\n",
      "Epoch [1463/2000] - Train Loss: 4.4897 - Val Loss: 4.6731\n",
      "Epoch [1464/2000] - Train Loss: 4.5475 - Val Loss: 4.5902\n",
      "Epoch [1465/2000] - Train Loss: 4.3841 - Val Loss: 4.5984\n",
      "Epoch [1466/2000] - Train Loss: 4.5059 - Val Loss: 4.6213\n",
      "Epoch [1467/2000] - Train Loss: 4.4921 - Val Loss: 4.6612\n",
      "Epoch [1468/2000] - Train Loss: 4.6259 - Val Loss: 4.7094\n",
      "Epoch [1469/2000] - Train Loss: 4.4801 - Val Loss: 4.6763\n",
      "Epoch [1470/2000] - Train Loss: 4.4643 - Val Loss: 4.7281\n",
      "Epoch [1471/2000] - Train Loss: 4.4803 - Val Loss: 4.4912\n",
      "Epoch [1472/2000] - Train Loss: 4.4700 - Val Loss: 4.5565\n",
      "Epoch [1473/2000] - Train Loss: 4.4283 - Val Loss: 4.5935\n",
      "Epoch [1474/2000] - Train Loss: 4.4568 - Val Loss: 4.5824\n",
      "Epoch [1475/2000] - Train Loss: 4.5424 - Val Loss: 4.5756\n",
      "Epoch [1476/2000] - Train Loss: 4.4533 - Val Loss: 4.5555\n",
      "Epoch [1477/2000] - Train Loss: 4.4683 - Val Loss: 4.5490\n",
      "Epoch [1478/2000] - Train Loss: 4.4955 - Val Loss: 4.8194\n",
      "Epoch [1479/2000] - Train Loss: 4.5568 - Val Loss: 4.6136\n",
      "Epoch [1480/2000] - Train Loss: 4.5940 - Val Loss: 4.4667\n",
      "Epoch [1481/2000] - Train Loss: 4.4485 - Val Loss: 4.5249\n",
      "Epoch [1482/2000] - Train Loss: 4.4478 - Val Loss: 4.5465\n",
      "Epoch [1483/2000] - Train Loss: 4.3963 - Val Loss: 4.7925\n",
      "Epoch [1484/2000] - Train Loss: 4.4510 - Val Loss: 4.5902\n",
      "Epoch [1485/2000] - Train Loss: 4.5005 - Val Loss: 4.5064\n",
      "Epoch [1486/2000] - Train Loss: 4.5225 - Val Loss: 4.4637\n",
      "Epoch [1487/2000] - Train Loss: 4.4425 - Val Loss: 4.4687\n",
      "Epoch [1488/2000] - Train Loss: 4.5029 - Val Loss: 4.7426\n",
      "Epoch [1489/2000] - Train Loss: 4.4744 - Val Loss: 4.6078\n",
      "Epoch [1490/2000] - Train Loss: 4.5083 - Val Loss: 4.4946\n",
      "Epoch [1491/2000] - Train Loss: 4.4394 - Val Loss: 4.6592\n",
      "Epoch [1492/2000] - Train Loss: 4.4875 - Val Loss: 4.8291\n",
      "Epoch [1493/2000] - Train Loss: 4.5330 - Val Loss: 4.6564\n",
      "Epoch [1494/2000] - Train Loss: 4.4607 - Val Loss: 4.5540\n",
      "Epoch [1495/2000] - Train Loss: 4.4223 - Val Loss: 4.4766\n",
      "Epoch [1496/2000] - Train Loss: 4.5290 - Val Loss: 4.5870\n",
      "Epoch [1497/2000] - Train Loss: 4.5359 - Val Loss: 4.6563\n",
      "Epoch [1498/2000] - Train Loss: 4.4704 - Val Loss: 4.7314\n",
      "Epoch [1499/2000] - Train Loss: 4.4644 - Val Loss: 4.6436\n",
      "Epoch [1500/2000] - Train Loss: 4.4404 - Val Loss: 4.8132\n",
      "Epoch [1501/2000] - Train Loss: 4.4915 - Val Loss: 4.5219\n",
      "Epoch [1502/2000] - Train Loss: 4.4096 - Val Loss: 4.6388\n",
      "Epoch [1503/2000] - Train Loss: 4.3442 - Val Loss: 4.4199\n",
      "Epoch [1504/2000] - Train Loss: 4.3687 - Val Loss: 4.5624\n",
      "Epoch [1505/2000] - Train Loss: 4.5242 - Val Loss: 4.4493\n",
      "Epoch [1506/2000] - Train Loss: 4.4482 - Val Loss: 4.6221\n",
      "Epoch [1507/2000] - Train Loss: 4.4716 - Val Loss: 4.5800\n",
      "Epoch [1508/2000] - Train Loss: 4.4412 - Val Loss: 4.4047\n",
      "Epoch [1509/2000] - Train Loss: 4.4657 - Val Loss: 4.5098\n",
      "Epoch [1510/2000] - Train Loss: 4.5094 - Val Loss: 4.5230\n",
      "Epoch [1511/2000] - Train Loss: 4.5228 - Val Loss: 4.6271\n",
      "Epoch [1512/2000] - Train Loss: 4.3955 - Val Loss: 4.6426\n",
      "Epoch [1513/2000] - Train Loss: 4.3897 - Val Loss: 4.4547\n",
      "Epoch [1514/2000] - Train Loss: 4.3909 - Val Loss: 4.4947\n",
      "Epoch [1515/2000] - Train Loss: 4.5495 - Val Loss: 4.5547\n",
      "Epoch [1516/2000] - Train Loss: 4.4636 - Val Loss: 4.8419\n",
      "Epoch [1517/2000] - Train Loss: 4.3388 - Val Loss: 4.6189\n",
      "Epoch [1518/2000] - Train Loss: 4.4864 - Val Loss: 4.4830\n",
      "Epoch [1519/2000] - Train Loss: 4.5265 - Val Loss: 4.6202\n",
      "Epoch [1520/2000] - Train Loss: 4.4443 - Val Loss: 4.5701\n",
      "Epoch [1521/2000] - Train Loss: 4.4956 - Val Loss: 4.5549\n",
      "Epoch [1522/2000] - Train Loss: 4.4460 - Val Loss: 4.4968\n",
      "Epoch [1523/2000] - Train Loss: 4.3925 - Val Loss: 4.6315\n",
      "Epoch [1524/2000] - Train Loss: 4.3979 - Val Loss: 4.5482\n",
      "Epoch [1525/2000] - Train Loss: 4.5151 - Val Loss: 4.4840\n",
      "Epoch [1526/2000] - Train Loss: 4.4798 - Val Loss: 4.6886\n",
      "Epoch [1527/2000] - Train Loss: 4.4425 - Val Loss: 4.7717\n",
      "Epoch [1528/2000] - Train Loss: 4.4780 - Val Loss: 4.6773\n",
      "Epoch [1529/2000] - Train Loss: 4.4212 - Val Loss: 4.4544\n",
      "Epoch [1530/2000] - Train Loss: 4.5622 - Val Loss: 4.7725\n",
      "Epoch [1531/2000] - Train Loss: 4.4676 - Val Loss: 4.6128\n",
      "Epoch [1532/2000] - Train Loss: 4.5208 - Val Loss: 4.6208\n",
      "Epoch [1533/2000] - Train Loss: 4.4605 - Val Loss: 4.9553\n",
      "Epoch [1534/2000] - Train Loss: 4.4947 - Val Loss: 4.5389\n",
      "Epoch [1535/2000] - Train Loss: 4.4309 - Val Loss: 4.5727\n",
      "Epoch [1536/2000] - Train Loss: 4.5145 - Val Loss: 4.7452\n",
      "Epoch [1537/2000] - Train Loss: 4.5100 - Val Loss: 4.6190\n",
      "Epoch [1538/2000] - Train Loss: 4.4490 - Val Loss: 4.4298\n",
      "Epoch [1539/2000] - Train Loss: 4.4542 - Val Loss: 4.6465\n",
      "Epoch [1540/2000] - Train Loss: 4.4605 - Val Loss: 4.5368\n",
      "Epoch [1541/2000] - Train Loss: 4.4615 - Val Loss: 4.4806\n",
      "Epoch [1542/2000] - Train Loss: 4.4635 - Val Loss: 4.5862\n",
      "Epoch [1543/2000] - Train Loss: 4.4470 - Val Loss: 4.5736\n",
      "Epoch [1544/2000] - Train Loss: 4.4784 - Val Loss: 4.5333\n",
      "Epoch [1545/2000] - Train Loss: 4.4560 - Val Loss: 4.7023\n",
      "Epoch [1546/2000] - Train Loss: 4.4377 - Val Loss: 4.7469\n",
      "Epoch [1547/2000] - Train Loss: 4.3919 - Val Loss: 4.5591\n",
      "Epoch [1548/2000] - Train Loss: 4.4765 - Val Loss: 4.6301\n",
      "Epoch [1549/2000] - Train Loss: 4.4313 - Val Loss: 4.5030\n",
      "Epoch [1550/2000] - Train Loss: 4.4163 - Val Loss: 4.6651\n",
      "Epoch [1551/2000] - Train Loss: 4.5605 - Val Loss: 5.1484\n",
      "Epoch [1552/2000] - Train Loss: 4.5375 - Val Loss: 4.6935\n",
      "Epoch [1553/2000] - Train Loss: 4.4636 - Val Loss: 4.6513\n",
      "Epoch [1554/2000] - Train Loss: 4.4831 - Val Loss: 4.4732\n",
      "Epoch [1555/2000] - Train Loss: 4.4242 - Val Loss: 4.6980\n",
      "Epoch [1556/2000] - Train Loss: 4.4512 - Val Loss: 4.8523\n",
      "Epoch [1557/2000] - Train Loss: 4.3736 - Val Loss: 4.8017\n",
      "Epoch [1558/2000] - Train Loss: 4.3783 - Val Loss: 4.5472\n",
      "Epoch [1559/2000] - Train Loss: 4.3723 - Val Loss: 4.5597\n",
      "Epoch [1560/2000] - Train Loss: 4.3872 - Val Loss: 4.5830\n",
      "Epoch [1561/2000] - Train Loss: 4.4532 - Val Loss: 4.5656\n",
      "Epoch [1562/2000] - Train Loss: 4.4262 - Val Loss: 4.6064\n",
      "Epoch [1563/2000] - Train Loss: 4.3155 - Val Loss: 4.4355\n",
      "Epoch [1564/2000] - Train Loss: 4.4662 - Val Loss: 4.6319\n",
      "Epoch [1565/2000] - Train Loss: 4.4017 - Val Loss: 4.4374\n",
      "Epoch [1566/2000] - Train Loss: 4.3955 - Val Loss: 4.6412\n",
      "Epoch [1567/2000] - Train Loss: 4.4610 - Val Loss: 4.5536\n",
      "Epoch [1568/2000] - Train Loss: 4.3727 - Val Loss: 4.7649\n",
      "Epoch [1569/2000] - Train Loss: 4.4210 - Val Loss: 4.4528\n",
      "Epoch [1570/2000] - Train Loss: 4.4013 - Val Loss: 4.7409\n",
      "Epoch [1571/2000] - Train Loss: 4.4627 - Val Loss: 4.6339\n",
      "Epoch [1572/2000] - Train Loss: 4.4650 - Val Loss: 4.5284\n",
      "Epoch [1573/2000] - Train Loss: 4.3886 - Val Loss: 4.4300\n",
      "Epoch [1574/2000] - Train Loss: 4.3744 - Val Loss: 4.5455\n",
      "Epoch [1575/2000] - Train Loss: 4.4193 - Val Loss: 4.4222\n",
      "Epoch [1576/2000] - Train Loss: 4.4497 - Val Loss: 4.6399\n",
      "Epoch [1577/2000] - Train Loss: 4.4635 - Val Loss: 4.4038\n",
      "Epoch [1578/2000] - Train Loss: 4.4326 - Val Loss: 4.6616\n",
      "Epoch [1579/2000] - Train Loss: 4.4445 - Val Loss: 4.4967\n",
      "Epoch [1580/2000] - Train Loss: 4.4138 - Val Loss: 4.5442\n",
      "Epoch [1581/2000] - Train Loss: 4.4267 - Val Loss: 4.6702\n",
      "Epoch [1582/2000] - Train Loss: 4.3856 - Val Loss: 4.6037\n",
      "Epoch [1583/2000] - Train Loss: 4.4794 - Val Loss: 4.7789\n",
      "Epoch [1584/2000] - Train Loss: 4.4090 - Val Loss: 4.7389\n",
      "Epoch [1585/2000] - Train Loss: 4.4262 - Val Loss: 4.5120\n",
      "Epoch [1586/2000] - Train Loss: 4.4065 - Val Loss: 4.5156\n",
      "Epoch [1587/2000] - Train Loss: 4.5123 - Val Loss: 4.4605\n",
      "Epoch [1588/2000] - Train Loss: 4.3312 - Val Loss: 4.4954\n",
      "Epoch [1589/2000] - Train Loss: 4.4290 - Val Loss: 4.5280\n",
      "Epoch [1590/2000] - Train Loss: 4.4333 - Val Loss: 4.4380\n",
      "Epoch [1591/2000] - Train Loss: 4.3209 - Val Loss: 4.3786\n",
      "Epoch [1592/2000] - Train Loss: 4.3533 - Val Loss: 4.4888\n",
      "Epoch [1593/2000] - Train Loss: 4.3482 - Val Loss: 4.6668\n",
      "Epoch [1594/2000] - Train Loss: 4.4523 - Val Loss: 4.4418\n",
      "Epoch [1595/2000] - Train Loss: 4.3395 - Val Loss: 4.5308\n",
      "Epoch [1596/2000] - Train Loss: 4.4832 - Val Loss: 4.5198\n",
      "Epoch [1597/2000] - Train Loss: 4.3909 - Val Loss: 4.5267\n",
      "Epoch [1598/2000] - Train Loss: 4.4030 - Val Loss: 4.4700\n",
      "Epoch [1599/2000] - Train Loss: 4.3622 - Val Loss: 4.4943\n",
      "Epoch [1600/2000] - Train Loss: 4.4156 - Val Loss: 4.4899\n",
      "Epoch [1601/2000] - Train Loss: 4.4828 - Val Loss: 4.3435\n",
      "Epoch [1602/2000] - Train Loss: 4.3307 - Val Loss: 4.5611\n",
      "Epoch [1603/2000] - Train Loss: 4.3939 - Val Loss: 4.5870\n",
      "Epoch [1604/2000] - Train Loss: 4.4123 - Val Loss: 4.5622\n",
      "Epoch [1605/2000] - Train Loss: 4.3263 - Val Loss: 4.4727\n",
      "Epoch [1606/2000] - Train Loss: 4.3792 - Val Loss: 4.4827\n",
      "Epoch [1607/2000] - Train Loss: 4.4082 - Val Loss: 4.5169\n",
      "Epoch [1608/2000] - Train Loss: 4.3575 - Val Loss: 4.6686\n",
      "Epoch [1609/2000] - Train Loss: 4.3897 - Val Loss: 4.5235\n",
      "Epoch [1610/2000] - Train Loss: 4.4259 - Val Loss: 4.7817\n",
      "Epoch [1611/2000] - Train Loss: 4.3614 - Val Loss: 4.4969\n",
      "Epoch [1612/2000] - Train Loss: 4.4352 - Val Loss: 4.4410\n",
      "Epoch [1613/2000] - Train Loss: 4.3648 - Val Loss: 4.8106\n",
      "Epoch [1614/2000] - Train Loss: 4.4060 - Val Loss: 4.4454\n",
      "Epoch [1615/2000] - Train Loss: 4.4180 - Val Loss: 4.6804\n",
      "Epoch [1616/2000] - Train Loss: 4.4314 - Val Loss: 4.6627\n",
      "Epoch [1617/2000] - Train Loss: 4.3893 - Val Loss: 4.5929\n",
      "Epoch [1618/2000] - Train Loss: 4.3994 - Val Loss: 4.5653\n",
      "Epoch [1619/2000] - Train Loss: 4.4534 - Val Loss: 4.4179\n",
      "Epoch [1620/2000] - Train Loss: 4.3827 - Val Loss: 4.5962\n",
      "Epoch [1621/2000] - Train Loss: 4.3912 - Val Loss: 4.4270\n",
      "Epoch [1622/2000] - Train Loss: 4.3743 - Val Loss: 4.3966\n",
      "Epoch [1623/2000] - Train Loss: 4.3784 - Val Loss: 4.6002\n",
      "Epoch [1624/2000] - Train Loss: 4.4055 - Val Loss: 4.5107\n",
      "Epoch [1625/2000] - Train Loss: 4.3676 - Val Loss: 4.6287\n",
      "Epoch [1626/2000] - Train Loss: 4.3756 - Val Loss: 4.4771\n",
      "Epoch [1627/2000] - Train Loss: 4.3967 - Val Loss: 4.4955\n",
      "Epoch [1628/2000] - Train Loss: 4.3405 - Val Loss: 4.4400\n",
      "Epoch [1629/2000] - Train Loss: 4.3797 - Val Loss: 4.5043\n",
      "Epoch [1630/2000] - Train Loss: 4.3236 - Val Loss: 4.4918\n",
      "Epoch [1631/2000] - Train Loss: 4.4059 - Val Loss: 4.9293\n",
      "Epoch [1632/2000] - Train Loss: 4.4476 - Val Loss: 4.4922\n",
      "Epoch [1633/2000] - Train Loss: 4.3943 - Val Loss: 4.5815\n",
      "Epoch [1634/2000] - Train Loss: 4.4185 - Val Loss: 4.5138\n",
      "Epoch [1635/2000] - Train Loss: 4.4471 - Val Loss: 4.4944\n",
      "Epoch [1636/2000] - Train Loss: 4.3479 - Val Loss: 4.6155\n",
      "Epoch [1637/2000] - Train Loss: 4.3527 - Val Loss: 4.4394\n",
      "Epoch [1638/2000] - Train Loss: 4.3496 - Val Loss: 4.4871\n",
      "Epoch [1639/2000] - Train Loss: 4.3668 - Val Loss: 4.4759\n",
      "Epoch [1640/2000] - Train Loss: 4.4138 - Val Loss: 4.6235\n",
      "Epoch [1641/2000] - Train Loss: 4.4230 - Val Loss: 4.3615\n",
      "Epoch [1642/2000] - Train Loss: 4.3990 - Val Loss: 4.5232\n",
      "Epoch [1643/2000] - Train Loss: 4.4916 - Val Loss: 4.8634\n",
      "Epoch [1644/2000] - Train Loss: 4.3870 - Val Loss: 4.8416\n",
      "Epoch [1645/2000] - Train Loss: 4.3521 - Val Loss: 4.5389\n",
      "Epoch [1646/2000] - Train Loss: 4.3063 - Val Loss: 4.4861\n",
      "Epoch [1647/2000] - Train Loss: 4.3220 - Val Loss: 4.5177\n",
      "Epoch [1648/2000] - Train Loss: 4.3081 - Val Loss: 4.3417\n",
      "Epoch [1649/2000] - Train Loss: 4.4115 - Val Loss: 4.3667\n",
      "Epoch [1650/2000] - Train Loss: 4.4294 - Val Loss: 4.5580\n",
      "Epoch [1651/2000] - Train Loss: 4.4334 - Val Loss: 4.7205\n",
      "Epoch [1652/2000] - Train Loss: 4.4270 - Val Loss: 4.5777\n",
      "Epoch [1653/2000] - Train Loss: 4.3056 - Val Loss: 4.6563\n",
      "Epoch [1654/2000] - Train Loss: 4.3747 - Val Loss: 4.4697\n",
      "Epoch [1655/2000] - Train Loss: 4.3109 - Val Loss: 4.5925\n",
      "Epoch [1656/2000] - Train Loss: 4.3614 - Val Loss: 4.3258\n",
      "Epoch [1657/2000] - Train Loss: 4.3471 - Val Loss: 4.3993\n",
      "Epoch [1658/2000] - Train Loss: 4.3679 - Val Loss: 4.4446\n",
      "Epoch [1659/2000] - Train Loss: 4.3486 - Val Loss: 4.5489\n",
      "Epoch [1660/2000] - Train Loss: 4.2885 - Val Loss: 4.4265\n",
      "Epoch [1661/2000] - Train Loss: 4.3097 - Val Loss: 4.5278\n",
      "Epoch [1662/2000] - Train Loss: 4.4726 - Val Loss: 4.4821\n",
      "Epoch [1663/2000] - Train Loss: 4.3595 - Val Loss: 4.5262\n",
      "Epoch [1664/2000] - Train Loss: 4.3406 - Val Loss: 4.5835\n",
      "Epoch [1665/2000] - Train Loss: 4.4338 - Val Loss: 4.4343\n",
      "Epoch [1666/2000] - Train Loss: 4.3254 - Val Loss: 4.4535\n",
      "Epoch [1667/2000] - Train Loss: 4.3848 - Val Loss: 4.8694\n",
      "Epoch [1668/2000] - Train Loss: 4.4521 - Val Loss: 4.7679\n",
      "Epoch [1669/2000] - Train Loss: 4.3987 - Val Loss: 4.4866\n",
      "Epoch [1670/2000] - Train Loss: 4.3096 - Val Loss: 4.7734\n",
      "Epoch [1671/2000] - Train Loss: 4.3690 - Val Loss: 4.4816\n",
      "Epoch [1672/2000] - Train Loss: 4.3025 - Val Loss: 4.5327\n",
      "Epoch [1673/2000] - Train Loss: 4.3388 - Val Loss: 4.5761\n",
      "Epoch [1674/2000] - Train Loss: 4.3513 - Val Loss: 4.5016\n",
      "Epoch [1675/2000] - Train Loss: 4.3106 - Val Loss: 4.5357\n",
      "Epoch [1676/2000] - Train Loss: 4.3595 - Val Loss: 4.5974\n",
      "Epoch [1677/2000] - Train Loss: 4.4538 - Val Loss: 4.4710\n",
      "Epoch [1678/2000] - Train Loss: 4.4114 - Val Loss: 4.4812\n",
      "Epoch [1679/2000] - Train Loss: 4.3395 - Val Loss: 4.5191\n",
      "Epoch [1680/2000] - Train Loss: 4.3334 - Val Loss: 4.3954\n",
      "Epoch [1681/2000] - Train Loss: 4.3711 - Val Loss: 4.5113\n",
      "Epoch [1682/2000] - Train Loss: 4.4315 - Val Loss: 4.5238\n",
      "Epoch [1683/2000] - Train Loss: 4.3799 - Val Loss: 4.4603\n",
      "Epoch [1684/2000] - Train Loss: 4.3441 - Val Loss: 4.4468\n",
      "Epoch [1685/2000] - Train Loss: 4.3345 - Val Loss: 4.4978\n",
      "Epoch [1686/2000] - Train Loss: 4.4056 - Val Loss: 4.4953\n",
      "Epoch [1687/2000] - Train Loss: 4.2956 - Val Loss: 4.6206\n",
      "Epoch [1688/2000] - Train Loss: 4.3365 - Val Loss: 4.5052\n",
      "Epoch [1689/2000] - Train Loss: 4.3602 - Val Loss: 4.8267\n",
      "Epoch [1690/2000] - Train Loss: 4.4297 - Val Loss: 4.5075\n",
      "Epoch [1691/2000] - Train Loss: 4.4038 - Val Loss: 4.5235\n",
      "Epoch [1692/2000] - Train Loss: 4.3223 - Val Loss: 4.4023\n",
      "Epoch [1693/2000] - Train Loss: 4.3591 - Val Loss: 4.4290\n",
      "Epoch [1694/2000] - Train Loss: 4.2918 - Val Loss: 4.4835\n",
      "Epoch [1695/2000] - Train Loss: 4.3419 - Val Loss: 4.4139\n",
      "Epoch [1696/2000] - Train Loss: 4.2981 - Val Loss: 4.4370\n",
      "Epoch [1697/2000] - Train Loss: 4.2466 - Val Loss: 4.6043\n",
      "Epoch [1698/2000] - Train Loss: 4.3130 - Val Loss: 4.7057\n",
      "Epoch [1699/2000] - Train Loss: 4.3409 - Val Loss: 4.4084\n",
      "Epoch [1700/2000] - Train Loss: 4.2719 - Val Loss: 4.3867\n",
      "Epoch [1701/2000] - Train Loss: 4.3544 - Val Loss: 4.4472\n",
      "Epoch [1702/2000] - Train Loss: 4.3575 - Val Loss: 4.4207\n",
      "Epoch [1703/2000] - Train Loss: 4.3376 - Val Loss: 4.5838\n",
      "Epoch [1704/2000] - Train Loss: 4.3607 - Val Loss: 4.3867\n",
      "Epoch [1705/2000] - Train Loss: 4.3135 - Val Loss: 4.5156\n",
      "Epoch [1706/2000] - Train Loss: 4.3418 - Val Loss: 4.6565\n",
      "Epoch [1707/2000] - Train Loss: 4.3425 - Val Loss: 4.5442\n",
      "Epoch [1708/2000] - Train Loss: 4.2959 - Val Loss: 4.6103\n",
      "Epoch [1709/2000] - Train Loss: 4.4084 - Val Loss: 4.8337\n",
      "Epoch [1710/2000] - Train Loss: 4.3890 - Val Loss: 4.5202\n",
      "Epoch [1711/2000] - Train Loss: 4.3363 - Val Loss: 4.4808\n",
      "Epoch [1712/2000] - Train Loss: 4.2932 - Val Loss: 4.6515\n",
      "Epoch [1713/2000] - Train Loss: 4.3581 - Val Loss: 4.4751\n",
      "Epoch [1714/2000] - Train Loss: 4.3398 - Val Loss: 4.6057\n",
      "Epoch [1715/2000] - Train Loss: 4.3282 - Val Loss: 4.5536\n",
      "Epoch [1716/2000] - Train Loss: 4.3806 - Val Loss: 4.5865\n",
      "Epoch [1717/2000] - Train Loss: 4.3874 - Val Loss: 4.5028\n",
      "Epoch [1718/2000] - Train Loss: 4.3322 - Val Loss: 4.8519\n",
      "Epoch [1719/2000] - Train Loss: 4.3546 - Val Loss: 4.5870\n",
      "Epoch [1720/2000] - Train Loss: 4.2814 - Val Loss: 4.9199\n",
      "Epoch [1721/2000] - Train Loss: 4.3315 - Val Loss: 4.4808\n",
      "Epoch [1722/2000] - Train Loss: 4.3335 - Val Loss: 4.5961\n",
      "Epoch [1723/2000] - Train Loss: 4.3118 - Val Loss: 4.4035\n",
      "Epoch [1724/2000] - Train Loss: 4.2936 - Val Loss: 4.4623\n",
      "Epoch [1725/2000] - Train Loss: 4.3395 - Val Loss: 4.3848\n",
      "Epoch [1726/2000] - Train Loss: 4.2524 - Val Loss: 4.4243\n",
      "Epoch [1727/2000] - Train Loss: 4.2506 - Val Loss: 4.4937\n",
      "Epoch [1728/2000] - Train Loss: 4.3697 - Val Loss: 4.4400\n",
      "Epoch [1729/2000] - Train Loss: 4.2970 - Val Loss: 4.4032\n",
      "Epoch [1730/2000] - Train Loss: 4.3000 - Val Loss: 4.4735\n",
      "Epoch [1731/2000] - Train Loss: 4.1952 - Val Loss: 4.4955\n",
      "Epoch [1732/2000] - Train Loss: 4.3006 - Val Loss: 4.5366\n",
      "Epoch [1733/2000] - Train Loss: 4.3936 - Val Loss: 4.5304\n",
      "Epoch [1734/2000] - Train Loss: 4.4462 - Val Loss: 4.5032\n",
      "Epoch [1735/2000] - Train Loss: 4.3036 - Val Loss: 4.5381\n",
      "Epoch [1736/2000] - Train Loss: 4.3829 - Val Loss: 4.7375\n",
      "Epoch [1737/2000] - Train Loss: 4.3450 - Val Loss: 4.4217\n",
      "Epoch [1738/2000] - Train Loss: 4.2947 - Val Loss: 4.4190\n",
      "Epoch [1739/2000] - Train Loss: 4.2787 - Val Loss: 4.6109\n",
      "Epoch [1740/2000] - Train Loss: 4.3370 - Val Loss: 4.4297\n",
      "Epoch [1741/2000] - Train Loss: 4.3345 - Val Loss: 4.4771\n",
      "Epoch [1742/2000] - Train Loss: 4.3018 - Val Loss: 4.5076\n",
      "Epoch [1743/2000] - Train Loss: 4.3324 - Val Loss: 4.5157\n",
      "Epoch [1744/2000] - Train Loss: 4.3609 - Val Loss: 4.5361\n",
      "Epoch [1745/2000] - Train Loss: 4.3404 - Val Loss: 4.3299\n",
      "Epoch [1746/2000] - Train Loss: 4.3319 - Val Loss: 4.3287\n",
      "Epoch [1747/2000] - Train Loss: 4.4025 - Val Loss: 4.4576\n",
      "Epoch [1748/2000] - Train Loss: 4.4133 - Val Loss: 4.5606\n",
      "Epoch [1749/2000] - Train Loss: 4.2484 - Val Loss: 4.3614\n",
      "Epoch [1750/2000] - Train Loss: 4.2645 - Val Loss: 4.4303\n",
      "Epoch [1751/2000] - Train Loss: 4.3053 - Val Loss: 4.5613\n",
      "Epoch [1752/2000] - Train Loss: 4.3266 - Val Loss: 4.3568\n",
      "Epoch [1753/2000] - Train Loss: 4.3282 - Val Loss: 4.4536\n",
      "Epoch [1754/2000] - Train Loss: 4.2408 - Val Loss: 4.4124\n",
      "Epoch [1755/2000] - Train Loss: 4.3197 - Val Loss: 4.4677\n",
      "Epoch [1756/2000] - Train Loss: 4.3676 - Val Loss: 4.6401\n",
      "Epoch [1757/2000] - Train Loss: 4.2895 - Val Loss: 4.5009\n",
      "Epoch [1758/2000] - Train Loss: 4.3048 - Val Loss: 4.3232\n",
      "Epoch [1759/2000] - Train Loss: 4.3835 - Val Loss: 4.5157\n",
      "Epoch [1760/2000] - Train Loss: 4.3212 - Val Loss: 4.4797\n",
      "Epoch [1761/2000] - Train Loss: 4.3308 - Val Loss: 4.3246\n",
      "Epoch [1762/2000] - Train Loss: 4.3115 - Val Loss: 4.6785\n",
      "Epoch [1763/2000] - Train Loss: 4.3007 - Val Loss: 4.6264\n",
      "Epoch [1764/2000] - Train Loss: 4.3459 - Val Loss: 4.3814\n",
      "Epoch [1765/2000] - Train Loss: 4.3281 - Val Loss: 4.3431\n",
      "Epoch [1766/2000] - Train Loss: 4.3441 - Val Loss: 4.3974\n",
      "Epoch [1767/2000] - Train Loss: 4.2515 - Val Loss: 4.5621\n",
      "Epoch [1768/2000] - Train Loss: 4.3187 - Val Loss: 4.4087\n",
      "Epoch [1769/2000] - Train Loss: 4.3305 - Val Loss: 4.4478\n",
      "Epoch [1770/2000] - Train Loss: 4.3004 - Val Loss: 4.4025\n",
      "Epoch [1771/2000] - Train Loss: 4.2242 - Val Loss: 4.3555\n",
      "Epoch [1772/2000] - Train Loss: 4.2830 - Val Loss: 4.3621\n",
      "Epoch [1773/2000] - Train Loss: 4.2725 - Val Loss: 4.4260\n",
      "Epoch [1774/2000] - Train Loss: 4.3231 - Val Loss: 4.3524\n",
      "Epoch [1775/2000] - Train Loss: 4.2881 - Val Loss: 4.5784\n",
      "Epoch [1776/2000] - Train Loss: 4.2656 - Val Loss: 4.5322\n",
      "Epoch [1777/2000] - Train Loss: 4.3225 - Val Loss: 4.4192\n",
      "Epoch [1778/2000] - Train Loss: 4.3542 - Val Loss: 4.4567\n",
      "Epoch [1779/2000] - Train Loss: 4.3313 - Val Loss: 4.4112\n",
      "Epoch [1780/2000] - Train Loss: 4.3265 - Val Loss: 4.3660\n",
      "Epoch [1781/2000] - Train Loss: 4.3145 - Val Loss: 4.3865\n",
      "Epoch [1782/2000] - Train Loss: 4.2720 - Val Loss: 4.4515\n",
      "Epoch [1783/2000] - Train Loss: 4.2775 - Val Loss: 4.4405\n",
      "Epoch [1784/2000] - Train Loss: 4.2733 - Val Loss: 4.5382\n",
      "Epoch [1785/2000] - Train Loss: 4.4217 - Val Loss: 4.4557\n",
      "Epoch [1786/2000] - Train Loss: 4.3733 - Val Loss: 4.4318\n",
      "Epoch [1787/2000] - Train Loss: 4.3722 - Val Loss: 4.5006\n",
      "Epoch [1788/2000] - Train Loss: 4.2985 - Val Loss: 4.4282\n",
      "Epoch [1789/2000] - Train Loss: 4.2729 - Val Loss: 4.4467\n",
      "Epoch [1790/2000] - Train Loss: 4.2829 - Val Loss: 4.5937\n",
      "Epoch [1791/2000] - Train Loss: 4.2871 - Val Loss: 4.4059\n",
      "Epoch [1792/2000] - Train Loss: 4.2246 - Val Loss: 4.4202\n",
      "Epoch [1793/2000] - Train Loss: 4.2776 - Val Loss: 4.5219\n",
      "Epoch [1794/2000] - Train Loss: 4.2671 - Val Loss: 4.3930\n",
      "Epoch [1795/2000] - Train Loss: 4.2938 - Val Loss: 4.7870\n",
      "Epoch [1796/2000] - Train Loss: 4.3408 - Val Loss: 4.5565\n",
      "Epoch [1797/2000] - Train Loss: 4.3215 - Val Loss: 4.4377\n",
      "Epoch [1798/2000] - Train Loss: 4.2365 - Val Loss: 4.4306\n",
      "Epoch [1799/2000] - Train Loss: 4.2895 - Val Loss: 4.5298\n",
      "Epoch [1800/2000] - Train Loss: 4.2984 - Val Loss: 4.5723\n",
      "Epoch [1801/2000] - Train Loss: 4.3463 - Val Loss: 4.3925\n",
      "Epoch [1802/2000] - Train Loss: 4.3720 - Val Loss: 4.6038\n",
      "Epoch [1803/2000] - Train Loss: 4.2138 - Val Loss: 4.5479\n",
      "Epoch [1804/2000] - Train Loss: 4.2462 - Val Loss: 4.5906\n",
      "Epoch [1805/2000] - Train Loss: 4.2867 - Val Loss: 4.4785\n",
      "Epoch [1806/2000] - Train Loss: 4.3192 - Val Loss: 4.5211\n",
      "Epoch [1807/2000] - Train Loss: 4.2614 - Val Loss: 4.4233\n",
      "Epoch [1808/2000] - Train Loss: 4.2558 - Val Loss: 4.3819\n",
      "Epoch [1809/2000] - Train Loss: 4.2269 - Val Loss: 4.4658\n",
      "Epoch [1810/2000] - Train Loss: 4.2805 - Val Loss: 4.5947\n",
      "Epoch [1811/2000] - Train Loss: 4.2481 - Val Loss: 4.5951\n",
      "Epoch [1812/2000] - Train Loss: 4.3402 - Val Loss: 4.4600\n",
      "Epoch [1813/2000] - Train Loss: 4.3006 - Val Loss: 4.6802\n",
      "Epoch [1814/2000] - Train Loss: 4.2366 - Val Loss: 4.4819\n",
      "Epoch [1815/2000] - Train Loss: 4.3018 - Val Loss: 4.5418\n",
      "Epoch [1816/2000] - Train Loss: 4.3753 - Val Loss: 4.5029\n",
      "Epoch [1817/2000] - Train Loss: 4.2673 - Val Loss: 4.4484\n",
      "Epoch [1818/2000] - Train Loss: 4.2288 - Val Loss: 4.3576\n",
      "Epoch [1819/2000] - Train Loss: 4.2482 - Val Loss: 4.7712\n",
      "Epoch [1820/2000] - Train Loss: 4.2535 - Val Loss: 4.2949\n",
      "Epoch [1821/2000] - Train Loss: 4.2830 - Val Loss: 4.4299\n",
      "Epoch [1822/2000] - Train Loss: 4.3078 - Val Loss: 4.6830\n",
      "Epoch [1823/2000] - Train Loss: 4.2922 - Val Loss: 4.4173\n",
      "Epoch [1824/2000] - Train Loss: 4.2646 - Val Loss: 4.6333\n",
      "Epoch [1825/2000] - Train Loss: 4.2632 - Val Loss: 4.4985\n",
      "Epoch [1826/2000] - Train Loss: 4.2768 - Val Loss: 4.3970\n",
      "Epoch [1827/2000] - Train Loss: 4.3564 - Val Loss: 4.3077\n",
      "Epoch [1828/2000] - Train Loss: 4.2518 - Val Loss: 4.4625\n",
      "Epoch [1829/2000] - Train Loss: 4.3291 - Val Loss: 4.3295\n",
      "Epoch [1830/2000] - Train Loss: 4.2618 - Val Loss: 4.4597\n",
      "Epoch [1831/2000] - Train Loss: 4.2066 - Val Loss: 4.5382\n",
      "Epoch [1832/2000] - Train Loss: 4.2817 - Val Loss: 4.4908\n",
      "Epoch [1833/2000] - Train Loss: 4.2774 - Val Loss: 4.6629\n",
      "Epoch [1834/2000] - Train Loss: 4.3662 - Val Loss: 4.3801\n",
      "Epoch [1835/2000] - Train Loss: 4.3307 - Val Loss: 4.4029\n",
      "Epoch [1836/2000] - Train Loss: 4.3096 - Val Loss: 4.4915\n",
      "Epoch [1837/2000] - Train Loss: 4.3160 - Val Loss: 4.3179\n",
      "Epoch [1838/2000] - Train Loss: 4.3088 - Val Loss: 4.4444\n",
      "Epoch [1839/2000] - Train Loss: 4.2978 - Val Loss: 4.4992\n",
      "Epoch [1840/2000] - Train Loss: 4.2680 - Val Loss: 4.3650\n",
      "Epoch [1841/2000] - Train Loss: 4.2011 - Val Loss: 4.3759\n",
      "Epoch [1842/2000] - Train Loss: 4.2637 - Val Loss: 4.3980\n",
      "Epoch [1843/2000] - Train Loss: 4.2440 - Val Loss: 4.5055\n",
      "Epoch [1844/2000] - Train Loss: 4.3386 - Val Loss: 4.3575\n",
      "Epoch [1845/2000] - Train Loss: 4.3098 - Val Loss: 4.4868\n",
      "Epoch [1846/2000] - Train Loss: 4.2179 - Val Loss: 4.4157\n",
      "Epoch [1847/2000] - Train Loss: 4.2075 - Val Loss: 4.3490\n",
      "Epoch [1848/2000] - Train Loss: 4.1990 - Val Loss: 4.3258\n",
      "Epoch [1849/2000] - Train Loss: 4.2984 - Val Loss: 4.5394\n",
      "Epoch [1850/2000] - Train Loss: 4.3405 - Val Loss: 4.4550\n",
      "Epoch [1851/2000] - Train Loss: 4.3229 - Val Loss: 4.6233\n",
      "Epoch [1852/2000] - Train Loss: 4.3438 - Val Loss: 4.5521\n",
      "Epoch [1853/2000] - Train Loss: 4.2375 - Val Loss: 4.4804\n",
      "Epoch [1854/2000] - Train Loss: 4.2134 - Val Loss: 4.6011\n",
      "Epoch [1855/2000] - Train Loss: 4.3030 - Val Loss: 4.3975\n",
      "Epoch [1856/2000] - Train Loss: 4.3181 - Val Loss: 4.4258\n",
      "Epoch [1857/2000] - Train Loss: 4.1675 - Val Loss: 4.4685\n",
      "Epoch [1858/2000] - Train Loss: 4.3156 - Val Loss: 4.4264\n",
      "Epoch [1859/2000] - Train Loss: 4.3028 - Val Loss: 4.3539\n",
      "Epoch [1860/2000] - Train Loss: 4.2017 - Val Loss: 4.4444\n",
      "Epoch [1861/2000] - Train Loss: 4.2590 - Val Loss: 4.4190\n",
      "Epoch [1862/2000] - Train Loss: 4.1959 - Val Loss: 4.5584\n",
      "Epoch [1863/2000] - Train Loss: 4.2189 - Val Loss: 4.3871\n",
      "Epoch [1864/2000] - Train Loss: 4.2886 - Val Loss: 4.3696\n",
      "Epoch [1865/2000] - Train Loss: 4.2335 - Val Loss: 4.4201\n",
      "Epoch [1866/2000] - Train Loss: 4.2571 - Val Loss: 4.2799\n",
      "Epoch [1867/2000] - Train Loss: 4.2990 - Val Loss: 4.5527\n",
      "Epoch [1868/2000] - Train Loss: 4.3493 - Val Loss: 4.3685\n",
      "Epoch [1869/2000] - Train Loss: 4.2424 - Val Loss: 4.3822\n",
      "Epoch [1870/2000] - Train Loss: 4.2706 - Val Loss: 4.3974\n",
      "Epoch [1871/2000] - Train Loss: 4.2770 - Val Loss: 4.4608\n",
      "Epoch [1872/2000] - Train Loss: 4.3043 - Val Loss: 4.4112\n",
      "Epoch [1873/2000] - Train Loss: 4.2839 - Val Loss: 4.5188\n",
      "Epoch [1874/2000] - Train Loss: 4.2018 - Val Loss: 4.4586\n",
      "Epoch [1875/2000] - Train Loss: 4.2119 - Val Loss: 4.4854\n",
      "Epoch [1876/2000] - Train Loss: 4.2428 - Val Loss: 4.4489\n",
      "Epoch [1877/2000] - Train Loss: 4.2621 - Val Loss: 4.6295\n",
      "Epoch [1878/2000] - Train Loss: 4.2882 - Val Loss: 4.3843\n",
      "Epoch [1879/2000] - Train Loss: 4.2579 - Val Loss: 4.4263\n",
      "Epoch [1880/2000] - Train Loss: 4.4009 - Val Loss: 4.3379\n",
      "Epoch [1881/2000] - Train Loss: 4.2495 - Val Loss: 4.4045\n",
      "Epoch [1882/2000] - Train Loss: 4.2132 - Val Loss: 4.3657\n",
      "Epoch [1883/2000] - Train Loss: 4.2696 - Val Loss: 4.3825\n",
      "Epoch [1884/2000] - Train Loss: 4.2339 - Val Loss: 4.3295\n",
      "Epoch [1885/2000] - Train Loss: 4.2355 - Val Loss: 4.4221\n",
      "Epoch [1886/2000] - Train Loss: 4.2603 - Val Loss: 4.4124\n",
      "Epoch [1887/2000] - Train Loss: 4.1904 - Val Loss: 4.3878\n",
      "Epoch [1888/2000] - Train Loss: 4.2791 - Val Loss: 4.4166\n",
      "Epoch [1889/2000] - Train Loss: 4.2110 - Val Loss: 4.3632\n",
      "Epoch [1890/2000] - Train Loss: 4.2773 - Val Loss: 4.4874\n",
      "Epoch [1891/2000] - Train Loss: 4.2821 - Val Loss: 4.4674\n",
      "Epoch [1892/2000] - Train Loss: 4.3299 - Val Loss: 4.3553\n",
      "Epoch [1893/2000] - Train Loss: 4.2749 - Val Loss: 4.4886\n",
      "Epoch [1894/2000] - Train Loss: 4.1442 - Val Loss: 4.4248\n",
      "Epoch [1895/2000] - Train Loss: 4.2485 - Val Loss: 4.2956\n",
      "Epoch [1896/2000] - Train Loss: 4.3162 - Val Loss: 4.4247\n",
      "Epoch [1897/2000] - Train Loss: 4.2000 - Val Loss: 4.3412\n",
      "Epoch [1898/2000] - Train Loss: 4.1644 - Val Loss: 4.4138\n",
      "Epoch [1899/2000] - Train Loss: 4.3054 - Val Loss: 4.4880\n",
      "Epoch [1900/2000] - Train Loss: 4.2602 - Val Loss: 4.6368\n",
      "Epoch [1901/2000] - Train Loss: 4.2258 - Val Loss: 4.4161\n",
      "Epoch [1902/2000] - Train Loss: 4.2274 - Val Loss: 4.3117\n",
      "Epoch [1903/2000] - Train Loss: 4.3091 - Val Loss: 4.8517\n",
      "Epoch [1904/2000] - Train Loss: 4.3155 - Val Loss: 4.3519\n",
      "Epoch [1905/2000] - Train Loss: 4.3361 - Val Loss: 4.8412\n",
      "Epoch [1906/2000] - Train Loss: 4.3113 - Val Loss: 4.4307\n",
      "Epoch [1907/2000] - Train Loss: 4.2524 - Val Loss: 4.4113\n",
      "Epoch [1908/2000] - Train Loss: 4.2881 - Val Loss: 4.4549\n",
      "Epoch [1909/2000] - Train Loss: 4.2064 - Val Loss: 4.2560\n",
      "Epoch [1910/2000] - Train Loss: 4.1627 - Val Loss: 4.4254\n",
      "Epoch [1911/2000] - Train Loss: 4.3556 - Val Loss: 4.5700\n",
      "Epoch [1912/2000] - Train Loss: 4.2491 - Val Loss: 4.3856\n",
      "Epoch [1913/2000] - Train Loss: 4.2575 - Val Loss: 4.7159\n",
      "Epoch [1914/2000] - Train Loss: 4.2314 - Val Loss: 4.4547\n",
      "Epoch [1915/2000] - Train Loss: 4.2341 - Val Loss: 4.4323\n",
      "Epoch [1916/2000] - Train Loss: 4.2174 - Val Loss: 4.7240\n",
      "Epoch [1917/2000] - Train Loss: 4.2443 - Val Loss: 4.3828\n",
      "Epoch [1918/2000] - Train Loss: 4.2402 - Val Loss: 4.5083\n",
      "Epoch [1919/2000] - Train Loss: 4.2411 - Val Loss: 4.5613\n",
      "Epoch [1920/2000] - Train Loss: 4.2577 - Val Loss: 4.8029\n",
      "Epoch [1921/2000] - Train Loss: 4.2679 - Val Loss: 4.7079\n",
      "Epoch [1922/2000] - Train Loss: 4.2681 - Val Loss: 4.3616\n",
      "Epoch [1923/2000] - Train Loss: 4.2763 - Val Loss: 4.5295\n",
      "Epoch [1924/2000] - Train Loss: 4.3122 - Val Loss: 4.5152\n",
      "Epoch [1925/2000] - Train Loss: 4.2097 - Val Loss: 4.3841\n",
      "Epoch [1926/2000] - Train Loss: 4.2848 - Val Loss: 4.4768\n",
      "Epoch [1927/2000] - Train Loss: 4.2136 - Val Loss: 4.4557\n",
      "Epoch [1928/2000] - Train Loss: 4.2285 - Val Loss: 4.3883\n",
      "Epoch [1929/2000] - Train Loss: 4.2056 - Val Loss: 4.4372\n",
      "Epoch [1930/2000] - Train Loss: 4.2223 - Val Loss: 4.3829\n",
      "Epoch [1931/2000] - Train Loss: 4.1869 - Val Loss: 4.2484\n",
      "Epoch [1932/2000] - Train Loss: 4.1662 - Val Loss: 4.5174\n",
      "Epoch [1933/2000] - Train Loss: 4.2905 - Val Loss: 4.4887\n",
      "Epoch [1934/2000] - Train Loss: 4.2848 - Val Loss: 4.4645\n",
      "Epoch [1935/2000] - Train Loss: 4.2223 - Val Loss: 4.5878\n",
      "Epoch [1936/2000] - Train Loss: 4.2099 - Val Loss: 4.2770\n",
      "Epoch [1937/2000] - Train Loss: 4.1858 - Val Loss: 4.4695\n",
      "Epoch [1938/2000] - Train Loss: 4.2063 - Val Loss: 4.2619\n",
      "Epoch [1939/2000] - Train Loss: 4.1765 - Val Loss: 4.3687\n",
      "Epoch [1940/2000] - Train Loss: 4.1916 - Val Loss: 4.4472\n",
      "Epoch [1941/2000] - Train Loss: 4.2623 - Val Loss: 4.5346\n",
      "Epoch [1942/2000] - Train Loss: 4.2240 - Val Loss: 4.2229\n",
      "Epoch [1943/2000] - Train Loss: 4.2480 - Val Loss: 4.4089\n",
      "Epoch [1944/2000] - Train Loss: 4.2353 - Val Loss: 4.3899\n",
      "Epoch [1945/2000] - Train Loss: 4.1395 - Val Loss: 4.3826\n",
      "Epoch [1946/2000] - Train Loss: 4.2325 - Val Loss: 4.5099\n",
      "Epoch [1947/2000] - Train Loss: 4.2897 - Val Loss: 4.5120\n",
      "Epoch [1948/2000] - Train Loss: 4.3015 - Val Loss: 4.3673\n",
      "Epoch [1949/2000] - Train Loss: 4.2222 - Val Loss: 4.8223\n",
      "Epoch [1950/2000] - Train Loss: 4.2515 - Val Loss: 4.3788\n",
      "Epoch [1951/2000] - Train Loss: 4.2239 - Val Loss: 4.4301\n",
      "Epoch [1952/2000] - Train Loss: 4.2419 - Val Loss: 4.3926\n",
      "Epoch [1953/2000] - Train Loss: 4.2350 - Val Loss: 4.4799\n",
      "Epoch [1954/2000] - Train Loss: 4.1754 - Val Loss: 4.4432\n",
      "Epoch [1955/2000] - Train Loss: 4.2147 - Val Loss: 4.4584\n",
      "Epoch [1956/2000] - Train Loss: 4.2694 - Val Loss: 4.4718\n",
      "Epoch [1957/2000] - Train Loss: 4.2545 - Val Loss: 4.4844\n",
      "Epoch [1958/2000] - Train Loss: 4.2022 - Val Loss: 4.3371\n",
      "Epoch [1959/2000] - Train Loss: 4.2517 - Val Loss: 4.3728\n",
      "Epoch [1960/2000] - Train Loss: 4.2283 - Val Loss: 4.4587\n",
      "Epoch [1961/2000] - Train Loss: 4.2956 - Val Loss: 4.4641\n",
      "Epoch [1962/2000] - Train Loss: 4.2772 - Val Loss: 4.3971\n",
      "Epoch [1963/2000] - Train Loss: 4.2188 - Val Loss: 4.4961\n",
      "Epoch [1964/2000] - Train Loss: 4.2137 - Val Loss: 4.3938\n",
      "Epoch [1965/2000] - Train Loss: 4.1577 - Val Loss: 4.5596\n",
      "Epoch [1966/2000] - Train Loss: 4.2813 - Val Loss: 4.7108\n",
      "Epoch [1967/2000] - Train Loss: 4.2752 - Val Loss: 4.4084\n",
      "Epoch [1968/2000] - Train Loss: 4.2700 - Val Loss: 4.7421\n",
      "Epoch [1969/2000] - Train Loss: 4.2428 - Val Loss: 4.8196\n",
      "Epoch [1970/2000] - Train Loss: 4.2169 - Val Loss: 4.5353\n",
      "Epoch [1971/2000] - Train Loss: 4.2158 - Val Loss: 4.3608\n",
      "Epoch [1972/2000] - Train Loss: 4.2519 - Val Loss: 4.3939\n",
      "Epoch [1973/2000] - Train Loss: 4.2367 - Val Loss: 4.3794\n",
      "Epoch [1974/2000] - Train Loss: 4.2432 - Val Loss: 4.4032\n",
      "Epoch [1975/2000] - Train Loss: 4.2315 - Val Loss: 4.3533\n",
      "Epoch [1976/2000] - Train Loss: 4.1943 - Val Loss: 4.4664\n",
      "Epoch [1977/2000] - Train Loss: 4.2388 - Val Loss: 4.4839\n",
      "Epoch [1978/2000] - Train Loss: 4.2237 - Val Loss: 4.2444\n",
      "Epoch [1979/2000] - Train Loss: 4.2247 - Val Loss: 4.5713\n",
      "Epoch [1980/2000] - Train Loss: 4.2939 - Val Loss: 4.2765\n",
      "Epoch [1981/2000] - Train Loss: 4.1863 - Val Loss: 4.3717\n",
      "Epoch [1982/2000] - Train Loss: 4.2187 - Val Loss: 4.2961\n",
      "Epoch [1983/2000] - Train Loss: 4.2816 - Val Loss: 4.4596\n",
      "Epoch [1984/2000] - Train Loss: 4.1862 - Val Loss: 4.3809\n",
      "Epoch [1985/2000] - Train Loss: 4.1538 - Val Loss: 4.3873\n",
      "Epoch [1986/2000] - Train Loss: 4.1608 - Val Loss: 4.3374\n",
      "Epoch [1987/2000] - Train Loss: 4.1345 - Val Loss: 4.3243\n",
      "Epoch [1988/2000] - Train Loss: 4.2038 - Val Loss: 4.4378\n",
      "Epoch [1989/2000] - Train Loss: 4.1764 - Val Loss: 4.5381\n",
      "Epoch [1990/2000] - Train Loss: 4.1938 - Val Loss: 4.3357\n",
      "Epoch [1991/2000] - Train Loss: 4.2027 - Val Loss: 4.3428\n",
      "Epoch [1992/2000] - Train Loss: 4.1732 - Val Loss: 4.4506\n",
      "Epoch [1993/2000] - Train Loss: 4.2381 - Val Loss: 4.4216\n",
      "Epoch [1994/2000] - Train Loss: 4.2583 - Val Loss: 4.3900\n",
      "Epoch [1995/2000] - Train Loss: 4.1936 - Val Loss: 4.4837\n",
      "Epoch [1996/2000] - Train Loss: 4.2566 - Val Loss: 4.4206\n",
      "Epoch [1997/2000] - Train Loss: 4.1473 - Val Loss: 4.5077\n",
      "Epoch [1998/2000] - Train Loss: 4.2034 - Val Loss: 4.6257\n",
      "Epoch [1999/2000] - Train Loss: 4.2726 - Val Loss: 4.4704\n",
      "Epoch [2000/2000] - Train Loss: 4.2674 - Val Loss: 4.4102\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RegressionModel(input_size=5, hidden_size=256, output_size=1, num_layers=3, dropout=0.3)\n",
    "model.to(device)  # Move the model to GPU if available\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Move the losses to the CPU before plotting\n",
    "#train_losses = [loss.cpu().detach().numpy() for loss in train_losses]\n",
    "#val_losses = [loss.cpu().detach().numpy() for loss in val_losses]\n",
    "# Plot the losses\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/all_organs_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Percentage Error: -3.2%\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "new_model = RegressionModel(input_size=5, hidden_size=256, output_size=1, num_layers=3, dropout=0.3)\n",
    "new_model.load_state_dict(torch.load('models/all_organs_model.pth'))\n",
    "new_model.to(device)\n",
    "\n",
    "\n",
    "test_data = load('training_arrays/merged_training_arrays/aorta_test_array.npy')\n",
    "\n",
    "X_test = test_data[:,:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_model.eval()\n",
    "    predictions = new_model(X_test).cpu().numpy()\n",
    "\n",
    "predictions = predictions.reshape(-1)\n",
    "print(f'Mean Percentage Error: {100*((np.mean(y_test) - np.mean(predictions))/np.mean(y_test)):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kchan2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 12.7\n",
      "Predicted dose (mGy): 12.3\n",
      "Percent Error:  2.9%\n",
      "----------------------\n",
      "pitsikakis2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 12.2\n",
      "Predicted dose (mGy): 12.2\n",
      "Percent Error:  0.1%\n",
      "----------------------\n",
      "vacheva2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 10.3\n",
      "Predicted dose (mGy): 12.4\n",
      "Percent Error:  -20.3%\n",
      "----------------------\n",
      "pateraki2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 18.2\n",
      "Predicted dose (mGy): 19.0\n",
      "Percent Error:  -4.6%\n",
      "----------------------\n",
      "kandylaki2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 15.9\n",
      "Predicted dose (mGy): 16.8\n",
      "Percent Error:  -5.5%\n",
      "----------------------\n",
      "papoutsakisant2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 17.7\n",
      "Predicted dose (mGy): 17.2\n",
      "Percent Error:  2.8%\n",
      "----------------------\n",
      "georgieva2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 11.1\n",
      "Predicted dose (mGy): 12.4\n",
      "Percent Error:  -11.7%\n",
      "----------------------\n",
      "vailakis2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 23.9\n",
      "Predicted dose (mGy): 21.2\n",
      "Percent Error:  11.3%\n",
      "----------------------\n",
      "damboglou2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 24.7\n",
      "Predicted dose (mGy): 22.7\n",
      "Percent Error:  8.1%\n",
      "----------------------\n",
      "petrakis2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 20.8\n",
      "Predicted dose (mGy): 21.2\n",
      "Percent Error:  -2.0%\n",
      "----------------------\n",
      "koutsouraki2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 11.4\n",
      "Predicted dose (mGy): 12.2\n",
      "Percent Error:  -6.9%\n",
      "----------------------\n",
      "skarpathiotakiel2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 33.6\n",
      "Predicted dose (mGy): 30.1\n",
      "Percent Error:  10.3%\n",
      "----------------------\n",
      "spanakism2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 17.8\n",
      "Predicted dose (mGy): 19.2\n",
      "Percent Error:  -7.7%\n",
      "----------------------\n",
      "bakaloumi2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 20.9\n",
      "Predicted dose (mGy): 18.7\n",
      "Percent Error:  10.9%\n",
      "----------------------\n",
      "poliaki2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 17.1\n",
      "Predicted dose (mGy): 20.0\n",
      "Percent Error:  -17.1%\n",
      "----------------------\n",
      "petrakisof2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 18.6\n",
      "Predicted dose (mGy): 19.2\n",
      "Percent Error:  -2.9%\n",
      "----------------------\n",
      "fragkiadakisi2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 19.7\n",
      "Predicted dose (mGy): 19.3\n",
      "Percent Error:  2.3%\n",
      "----------------------\n",
      "evison2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 23.5\n",
      "Predicted dose (mGy): 23.3\n",
      "Percent Error:  0.9%\n",
      "----------------------\n",
      "lydakis2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 10.2\n",
      "Predicted dose (mGy): 10.2\n",
      "Percent Error:  0.2%\n",
      "----------------------\n",
      "celikja2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 12.8\n",
      "Predicted dose (mGy): 13.0\n",
      "Percent Error:  -1.7%\n",
      "----------------------\n",
      "kornilakis2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 20.7\n",
      "Predicted dose (mGy): 19.5\n",
      "Percent Error:  6.0%\n",
      "----------------------\n",
      "metaxakis2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 16.6\n",
      "Predicted dose (mGy): 18.5\n",
      "Percent Error:  -11.6%\n",
      "----------------------\n",
      "stavrakakis2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 14.7\n",
      "Predicted dose (mGy): 17.9\n",
      "Percent Error:  -22.3%\n",
      "----------------------\n",
      "fanourgaki2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 9.1\n",
      "Predicted dose (mGy): 9.4\n",
      "Percent Error:  -3.8%\n",
      "----------------------\n",
      "stamatakis2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 14.8\n",
      "Predicted dose (mGy): 13.7\n",
      "Percent Error:  7.3%\n",
      "----------------------\n",
      "lazopoulos2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 13.3\n",
      "Predicted dose (mGy): 12.5\n",
      "Percent Error:  5.4%\n",
      "----------------------\n",
      "zheleva2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 8.7\n",
      "Predicted dose (mGy): 9.8\n",
      "Percent Error:  -12.0%\n",
      "----------------------\n",
      "makrydaki2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 23.3\n",
      "Predicted dose (mGy): 22.0\n",
      "Percent Error:  5.8%\n",
      "----------------------\n",
      "agianiotakim2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 14.5\n",
      "Predicted dose (mGy): 17.0\n",
      "Percent Error:  -16.8%\n",
      "----------------------\n",
      "pervolarakis2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 18.4\n",
      "Predicted dose (mGy): 19.4\n",
      "Percent Error:  -5.6%\n",
      "----------------------\n",
      "valatas2.5_heart_myocardium_training_array.npy\n",
      "Ground Truth dose (mGy): 18.2\n",
      "Predicted dose (mGy): 17.9\n",
      "Percent Error:  1.6%\n",
      "----------------------\n",
      "Mean Percentage Error: 7.4%\n"
     ]
    }
   ],
   "source": [
    "#evaluate patient by patient\n",
    "import os\n",
    "\n",
    "files = os.listdir('training_arrays/heart_myocardium')\n",
    "diffs = []\n",
    "for file in files[-31:]:\n",
    "    print(file)\n",
    "    data = load('training_arrays/heart_myocardium/' + file)\n",
    "    X_test = data[:,:-1]\n",
    "    y_test = data[:,-1]\n",
    "\n",
    "    X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        new_model.eval()\n",
    "        predictions = new_model(X_test).cpu().numpy()\n",
    "\n",
    "    diffs.append(abs(100*((np.mean(y_test) - np.mean(predictions))/np.mean(y_test))))\n",
    "\n",
    "    print(f'Ground Truth dose (mGy): {np.mean(y_test):.1f}')\n",
    "    print(f'Predicted dose (mGy): {np.mean(predictions):.1f}')\n",
    "    print(f'Percent Error:  {100*((np.mean(y_test) - np.mean(predictions))/np.mean(y_test)):.1f}%')\n",
    "    print('----------------------')\n",
    "print(f'Mean Percentage Error: {np.nanmean(diffs):.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "totalsegmentator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
